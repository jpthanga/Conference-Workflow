Paper ID,Paper Title,Abstract,Author Names,Author Emails,Subject Areas,Decision
29,Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization,"We consider the minimization of submodular functions subject to ordering constraints. We show that this potentially non-convex optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures, with ordering constraints corresponding to first-order stochastic dominance.  We propose new discretization schemes that lead to simple and efficient algorithms based on zero-th, first, or higher order oracles;  these algorithms also lead to improvements without isotonic constraints. Finally,   our experiments  show that non-convex loss functions can be much more robust to outliers for isotonic regression, while still being solvable in polynomial time.","Francis Bach*,INRIA - Ecole Normale Sup√©rieure",francis.bach@inria.fr,Optimization/Submodular Optimization*; Optimization/Convex Optimization; Optimization/Non-Convex Optimization,Poster
33,Structure-Aware Convolutional Neural Networks,"Convolutional neural networks (CNNs) are inherently subject to invariable filters that can only aggregate local inputs with the same topological structures. It causes that CNNs are allowed to manage data with Euclidean or grid-like structures (e.g., images), not ones with non-Euclidean or graph structures (e.g., traffic networks). To broaden the reach of CNNs, we develop structure-aware convolution to eliminate the invariance, yielding a unified mechanism of dealing with both Euclidean and non-Euclidean structured data. Technically, filters in the structure-aware convolution are generalized to univariate functions, which are capable of aggregating local inputs with diverse topological structures. Since infinite parameters are required to determine a univariate function, we parameterize these filters with numbered learnable parameters in the context of the function approximation theory. By replacing the classical convolution in CNNs with the structure-aware convolution, Structure-Aware Convolutional Neural Networks (SACNNs) are readily established. Extensive experiments on eleven datasets strongly evidence that SACNNs outperform current models on various machine learning tasks, including image classification and clustering, text categorization, skeleton-based action recognition, molecular activity detection, and taxi flow prediction. Code will be available.","Jianlong Chang*,National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences;Jie Gu,National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences;Lingfeng Wang,Institute of Automation, Chinese Academy of Sciences;Gaofeng Meng,Chinese Academy of Sciences;SHIMING XIANG,Chinese Academy of Sciences, China;Chunhong Pan,Institute of Automation, Chinese Academy of Sciences",jianlong.chang@nlpr.ia.ac.cn;jie.gu@nlpr.ia.ac.cn;lfwang@nlpr.ia.ac.cn;gfmeng@nlpr.ia.ac.cn;smxiang@nlpr.ia.ac.cn;chpan@nlpr.ia.ac.cn,Deep Learning*; Deep Learning/CNN Architectures; Deep Learning/Optimization for Deep Networks,Poster
34,Kalman Normalization,"As an indispensable component, Batch Normalization (BN) has successfully improved the training of deep neural networks (DNNs) with mini-batches, by normalizing the distribution of the internal representation for each hidden layer. However, the effectiveness of BN would diminish with the scenario of micro-batch (\eg~less than 4 samples in a mini-batch), since the estimated statistics in a mini-batch are not reliable with insufficient samples. This limits BN's room in training larger models on segmentation, detection, and video-related problems, which require small batches constrained by memory consumption. In this paper, we present a novel normalization method, called Kalman Normalization (KN), for improving and accelerating the training of DNNs, particularly under the context of micro-batches. Specifically, unlike the existing solutions treating each hidden layer as an isolated system, KN treats all the layers in a network as a whole system, and estimates the statistics of a certain layer by considering the distributions of all its preceding layers, mimicking the merits of Kalman Filtering. On ResNet50 trained in ImageNet, KN has 3.4\% lower error than its BN counterpart when using a batch size of 4; Even when using typical batch sizes, KN still maintains an advantage over BN while other BN variants suffers a performance degradation. Moreover, KN can be naturally generalized to many existing normalization variants to obtain gains, \eg equipping Group Normalization \cite{wu2018group} with Group Kalman Normalization (GKN). KN can outperform BN and its variants for large scale object detection and segmentation task in COCO 2017.","Guangrun Wang*,Sun Yat-sen University;Jiefeng Peng,Sun Yat-sen University;Ping Luo,The Chinese University of Hong Kong;Xinjiang Wang, SenseTime Group Ltd.;Liang Lin,Sun Yat-sen University",wanggrun@mail2.sysu.edu.cn;jiefengpeng@gmail.com;pluo@ie.cuhk.edu.hk;swanxinjiang@gmail.com;linliang@ieee.org,Deep Learning/CNN Architectures*; Applications/Computer Vision; Deep Learning,Poster
37,HOGWILD!-Gibbs can be PanAccurate,"Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an accurate method for estimating probabilities of events on a small number of variables of a graphical model satisfying Dobrushin's condition~\cite{DeSaOR16}. We investigate whether it can be used to accurately estimate expectations of functions of {\em all the variables} of the model. Under the same condition, we show that the synchronous and asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between their (multivariate) samples remains bounded by $O(\tau \log n),$ where $n$ is the number of variables in the graphical model, and $\tau$ is a measure of the asynchronicity. A similar bound holds for any constant power of the Hamming distance. Hence, the expectation of any function that is Lipschitz with respect to a power of the Hamming distance, can be estimated with a bias that grows logarithmically in $n$. Going beyond Lipschitz functions, we consider the bias arising from asynchronicity in estimating the expectation of polynomial functions of all variables in the model. Using recent concentration of measure results~\cite{DaskalakisDK17,GheissariLP17,GotzeSS18}, we show that the bias introduced by the asynchronicity is of smaller order than the standard deviation of the function value already present in the true model. We perform experiments on a multi-processor machine to empirically illustrate our theoretical findings.","Constantinos Daskalakis,MIT;Nishanth Dikkala*,MIT;Siddhartha Jayanti,MIT",costis@csail.mit.edu;ndikkala@mit.edu;jayanti@mit.edu,Probabilistic Methods/Distributed Inference*; Probabilistic Methods/Graphical Models; Probabilistic Methods/MCMC; Probabilistic Methods/T,Poster
40,Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language,"This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve text-irrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text is modified. Experimental results show that our method outperforms existing methods on CUB and Oxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs. ","Seonghyeon Nam,Yonsei University;Yunji Kim,Yonsei University;Seon Joo Kim*,Yonsei Univ.",shnnam@yonsei.ac.kr;kim_yunji@yonsei.ac.kr;seonjookim@yonsei.ac.kr,Applications/Computational Photography*; Applications/Computer Vision; Deep Learning/Adversarial Networks,Spotlight
59,IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis,"We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand, the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand, the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs, such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires no extra discriminators, because the inference model itself serves as a discriminator to distinguish between the generated and real samples.  Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g., CELEBA images at \(1024^{2}\)), which are comparable to or better than the state-of-the-art GANs.","Huaibo Huang*,University of Chinese Academy of Sciences;zhihang li,Institute of Automation, Chinese Academy of Science;Ran He,Institute of Automation, Chinese Academy of Sciences;Zhenan Sun,Chinese of Academy of Sciences;Tieniu Tan,NLPR, China",huaibo.huang@cripac.ia.ac.cn;zhihang.li@nlpr.ia.ac.cn;rhe@nlpr.ia.ac.cn;znsun@nlpr.ia.ac.cn;tnt@nlpr.ia.ac.cn,Deep Learning/Generative Models*; Deep Learning/Adversarial Networks; Probabilistic Methods/Variational Inference,Poster
68,Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with $\beta$-Divergences,"We present the very first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with $\beta$-divergences. The resulting inference procedure is doubly robust for both the predictive and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as $\beta \to 0$. Secondly, we give a principled way of choosing the divergence parameter $\beta$ by minimizing expected predictive loss on-line. We offer the state of the art and improve the False Discovery Rate of CPs by more than 80% on real world data.","Jeremias  Knoblauch*,University of Warwick;Jack Jewson,University of Warwick;Theodoros Damoulas,University of Warwick",j.knoblauch@warwick.ac.uk;j.e.jewson@warwick.ac.uk;t.damoulas@warwick.ac.uk,Applications/Time Series Analysis*; Algorithms/Online Learning; Probabilistic Methods/Bayesian Theory; Probabilistic Methods/Va,Poster
75,Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning,"The focus in machine learning has branched beyond training classifiers on a single task to investigating how previously acquired knowledge in a source domain can be leveraged to facilitate learning in a related target domain, known as inductive transfer learning. Three active lines of research have independently explored transfer learning using neural networks. In weight transfer, a model trained on the source domain is used as an initialization point for a network to be trained on the target domain. In deep metric learning, the source domain is used to construct an embedding that captures class structure in both the source and target domains. In few-shot learning, the focus is on generalizing well in the target domain based on a limited number of labeled examples. We compare state-of-the-art methods from these three paradigms and also explore hybrid adapted-embedding methods that use limited target-domain data to fine tune embeddings constructed from source-domain data. We conduct a systematic comparison of methods in a variety of domains, varying the number of labeled instances available in the target domain (k), as well as the number of target-domain classes. The following are the major results: (1) Deep embeddings are far superior, compared to weight transfer, as a starting point for inter-domain transfer or model re-use (2) Our hybrid methods robustly outperform every few-shot learning and every deep metric learning method previously proposed, with a mean error reduction of 30% over state-of-the-art. (3) Among loss functions for discovering embeddings, the histogram loss (Ustinova & Lempitsky, 2016) is most robust. We hope our results will motivate a unification of research in weight transfer, deep metric learning, and few-shot learning.","Tyler Scott*,University of Colorado Boulder;Karl.ridgeway@colora Ridgeway,University of Colorado, Boulde;Michael Mozer,University of Colorado",tysc7237@colorado.edu;karl.ridgeway@colorado.edu;mozer@colorado.edu,Algorithms/Multitask and Transfer Learning*; Algorithms/Metric Learning; Deep Learning/Embedding Approaches; Deep Learning/Few-Shot Lear,Spotlight
77,Generalized Inverse Optimization through Online Learning,"Inverse optimization is a powerful paradigm for learning preferences and restrictions that explain the behavior of a decision maker, based on a set of external signal and the corresponding decision pairs. However, most inverse optimization algorithms are designed specially in batch setting, where all the data is available in advance. As a consequence, there has been little use of these methods in an online setting suitable for real-time applications. In this paper, we propose a general framework for inverse optimization through online learning. Specifically,  we develop an online learning algorithm that uses an implicit update rule which can handle noisy data. Moreover, under additional regularity assumptions in terms of the data and the model, we prove that our algorithm converges at a rate of $\mathcal{O}(1/\sqrt{T})$ and is statistically consistent. In our experiments, we show the online learning approach can learn the parameters with great accuracy and is very robust to noises, and achieves a drastic improvement in computational efficacy over the batch learning approach.","Chaosheng Dong*,University of Pittsburgh;Yiran Chen,Duke University;Bo Zeng,University of Pittsburgh",chaosheng@pitt.edu;yiran.chen@duke.edu;bzeng@pitt.edu,Algorithms/Online Learning*; Applications/Quantitative Finance and Econometrics; Applications/Recommender Systems; Optimiza,Poster
85,An Off-policy Policy Gradient Theorem Using Emphatic Weightings,"Policy gradient methods are widely used for control in reinforcement learning, particularly for the continuous action setting. There have been a host of theoretically sound algorithms proposed for the on-policy setting, due to the existence of the policy gradient theorem which provides a simplified form for the gradient. In off-policy learning, however, where the behaviour policy is not necessarily attempting to learn and follow the optimal policy for the given task, the existence of such a theorem has been elusive. In this work, we solve this open problem by providing the first off-policy policy gradient theorem. The key to the derivation is the use of emphatic weightings. We develop a new actor-critic algorithm---called Actor Critic with Emphatic weightings (ACE)---that approximates the simplified gradients provided by the theorem. We demonstrate in a simple counterexample that previous off-policy policy gradient methods---particularly OffPAC and DPG---converge to the wrong solution whereas ACE finds the optimal solution. ","Ehsan Imani,University of Alberta;Eric Graves,University of Alberta;Martha White*,University of Alberta",imani@ualberta.ca;graves@ualberta.ca;whitem@ualberta.ca,Reinforcement Learning and Planning/Reinforcement Learning,Poster
88,Supervised autoencoders: Improving generalization performance with unsupervised regularizers,"Generalization performance is a central goal in machine learning, particularly when learning representations with large neural networks. A common strategy to improve generalization has been through the use of regularizers, typically as a norm constraining the parameters. Regularizing hidden layers in a neural network architecture, however, is not straightforward. There have been a few effective layer-wise suggestions, but without theoretical guarantees for improved performance. In this work, we theoretically and empirically analyze one such model, called a supervised auto-encoder: a neural network that predicts both inputs (reconstruction error) and targets jointly. We provide a novel generalization result for linear auto-encoders, proving uniform stability based on the inclusion of the reconstruction error---particularly as an improvement on simplistic regularization such as norms or even on more advanced regularizations such as the use of auxiliary tasks. Empirically, we then demonstrate that, across an array of architectures with a different number of hidden units and activation functions, the supervised auto-encoder compared to the corresponding standard neural network never harms performance and can significantly improve generalization. ","Lei Le,Indiana University;Andrew Patterson,Indiana University;Martha White*,University of Alberta",leile@iu.edu;andnpatt@iu.edu;whitem@ualberta.ca,Algorithms/Representation Learning,Poster
92,Visual Object Networks: Image Generation with Disentangled 3D Representations,"Recent progress in deep generative models has led to tremendous breakthroughs in natural image generation. While being able to synthesize photorealistic images, existing models lack an understanding of our underlying 3D world. Different from previous work that is built on 2D datasets and models, we present a new generative model, Visual Object Networks (VON), synthesizing natural images with a 3D disentangled representation. Inspired by classic graphics rendering pipelines, we disentangle the image formation process into three conditionally independent factors---viewpoint, shape, and texture---and present an end-to-end adversarial learning framework that jointly models 3D shape and 2D texture. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It can then render its 2.5D sketch (i.e. object silhouette and depth map) from any viewpoint. Finally, it learns to add realistic texture to its 2.5D projections to generate final, 2D realistic images. Our model not only generates images that are more realistic than state-of-the-art, 2D-only image synthesis methods, but enables many 3D operations such as changing the viewpoint of a generated image, shape and texture editing and interpolation, and example-based texture transfer. ","Jun-Yan Zhu*,MIT;Zhoutong Zhang,MIT;Chengkai Zhang,MIT;Jiajun Wu,MIT;Antonio Torralba,MIT;Joshua Tenenbaum,MIT;Bill Freeman,MIT",junyanz@mit.edu;ztzhang@mit.edu;ckzhang@mit.edu;jiajunwu.cs@gmail.com;torralba@csail.mit.edu;jbt@mit.edu;billf@mit.edu,Deep Learning/Adversarial Networks*; Applications/Computer Vision; Deep Learning/Generative Models,Poster
95,Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units,"This paper presents a general framework for norm-based capacity control for $L_{p,q}$ weight normalized deep neural networks. We establish the upper bound on the Rademacher complexities of this family. With an $L_{p,q}$ normalization where $q\le p^*$ and $1/p+1/p^{*}=1$, we discuss properties of a width-independent capacity control, which only depends on the depth by a square root term. We further analyze the approximation properties of $L_{p,q}$ weight normalized deep neural networks. In particular, for an $L_{1,\infty}$ weight normalized network, the approximation error can be controlled by the $L_1$ norm of the output layer, and the corresponding generalization error only depends on the architecture by the square root of the depth.","Yixi Xu*,Purdue University;Xiao  Wang,Purdue University",xu573@purdue.edu;wangxiao@purdue.edu,Theory/Learning Theory,Poster
99,Learning Pipelines with Limited Data and Domain Knowledge: A Study in Parsing Physics Problems,"As machine learning becomes more widely used in practice, we need new methods to build complex intelligent systems that integrate learning with existing software, and with domain knowledge encoded as rules. As a case study, we present such a system that learns to parse Newtonian physics problems in textbooks. This system, Nuts&Bolts, learns a pipeline process that incorporates existing code, pre-learned machine learning models, and human engineered rules.  It jointly trains the entire pipeline to prevent propagation of errors, using a combination of labelled and unlabelled data.  Our approach achieves a good performance on the parsing task, outperforming the simple pipeline and its variants. We further use Nuts&Bolts to show improvements on the end task of answering these problems.","Mrinmaya Sachan*,CMU;Avinava Dubey,CMU;Tom Mitchell,CMU;Dan Roth,UPenn;Eric Xing,Petuum Inc. and CMU",mrinmayaster@gmail.com;avinava.dubey@gmail.com;tom.mitchell@cs.cmu.edu;danroth@seas.upenn.edu;epxing@cs.cmu.edu,Applications*; Applications/Computer Vision,Poster
102,Learning long-range spatial dependencies with horizontal gated recurrent units,"Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching -- and sometimes even surpassing -- human accuracy on a variety of visual recognition tasks. Here, however, we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce the horizontal gated-recurrent unit (hGRU) to learn intrinsic horizontal connections -- both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures which have orders of magnitude more free parameters. We further discuss the biological plausibility of the hGRU in comparison to anatomical data from the visual cortex as well as human behavioral data on a classic contour detection task.","Drew Linsley*,Brown University ;Junkyung Kim,Brown University;Vijay Veerabadran,Brown University;Charles Windolf,Brown University;Thomas Serre,Brown University",drew_linsley@brown.edu;junkyung_kim@brown.edu;vijay_veerabadran@brown.edu;charles_windolf@alumni.brown.edu;thomas_serre@brown.edu,Deep Learning/Biologically Plausible Deep Networks*; Applications/Computer Vision; Deep Learning/CNN Architectures; Deep Learning/Deep Autoencod,Poster
110,Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution,"Convolutional neural networks (CNNs) have recently achieved great success in single-image super-resolution (SISR).  However, these methods tend to produce over-smoothed outputs and miss some textural details. To solve these problems, we propose the Super-Resolution CliqueNet (SRCliqueNet) to reconstruct the high resolution (HR) image with better textural details in the wavelet domain. The proposed SRCliqueNet firstly extracts a set of feature maps from the low resolution (LR) image by the clique blocks group. Then we send the set of feature maps to the clique up-sampling module to reconstruct the HR image. The clique up-sampling module consists of four sub-nets which predict the high resolution wavelet coefficients of four sub-bands. Since we consider the edge feature properties of four sub-bands, the four sub-nets are connected to the others so that they can learn the coefficients of four sub-bands jointly.  Finally we apply inverse discrete wavelet transform (IDWT) to the output of four sub-nets at the end of the clique up-sampling module to increase the resolution and reconstruct the HR image. Extensive quantitative and qualitative experiments on benchmark datasets show that our method achieves superior performance over the state-of-the-art methods.","Zhisheng Zhong,Peking University;Tiancheng Shen,Peking University;Yibo Yang,Peking University;Zhouchen Lin,Peking University;Chao Zhang*,Peking University",zszhong@pku.edu.cn;tianchengshen@pku.edu.cn;ibo@pku.edu.cn;zlin@pku.edu.cn;chzhang@cis.pku.edu.cn,Applications/Computer Vision*; Deep Learning/CNN Architectures; Deep Learning/Supervised Deep Networks,Poster
142,Fast Similarity Search via Optimal Sparse Lifting,"Similarity search is a fundamental problem in computing science with various applications, and has attracted significant research attention, especially for large-scale search problems in high dimensions. Motivated by the evidence in biological science, we propose a novel approach for similarity search. Fundamentally different from existing methods which mostly try to reduce the dimension of the data during the search, our approach projects the data into an even higher-dimensional space and ensures the data to be sparse and binary in the output space, where the search speed can be significantly improved. Specifically, our approach has two key steps and contributions. Firstly, we seek an {\em optimal sparse lifting} for the input data that increases the dimension of the data while approximately preserving the pairwise similarity through a general matrix factorization method. Secondly, we seek a {\em lifting operator} that maps input samples to their {\em sparse lifting} by solving an optimization model. In empirical studies, our approach reported significantly improved search results over the state-of-the-art solution in information retrieval applications, and exhibited its high potential in solving practical problems.","Wenye Li*,The Chinese University of Hong Kong, Shenzhen;Jingwei Mao,The Chinese University of Hong Kong, Shenzhen;Yin Zhang,The Chinese University of Hong Kong, Shenzhen	;Shuguang Cui,The Chinese University of Hong Kong, Shenzhen	",wyli@cuhk.edu.cn;216019005@link.cuhk.edu.cn;yinzhang@cuhk.edu.cn;shuguangcui@cuhk.edu.cn,Algorithms/Sparse Coding and Dimensionality Expansion*; Algorithms/Unsupervised Learning,Poster
145,Learning Deep Disentangled Embeddings With the F-Statistic Loss,"Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the $F$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@$k$ and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.","Karl.ridgeway@colora Ridgeway*,University of Colorado, Boulde;Michael Mozer,University of Colorado",karl.ridgeway@colorado.edu;mozer@colorado.edu,Algorithms/Representation Learning*; Algorithms/Metric Learning; Deep Learning/Few-Shot Learning Approaches,Poster
153,Geometrically Coupled Monte Carlo Sampling,"Monte Carlo sampling in high-dimensional, low-sample settings is important in many machine learning tasks.  We improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies.  We compare our new strategies against prior methods for improving sample efficiency, including QMC, by studying discrepancy. We explore our findings empirically, and observe benefits of our sampling schemes for reinforcement learning and generative modelling.","Mark Rowland,DeepMind;Krzysztof Choromanski*,Google Brain Robotics;Fran√ßois Chalus,Cambridge University;Aldo Pacchiano,UC Berkeley;Tamas Sarlos,Google Research;Richard Turner,;Adrian Weller,Cambridge University",mr504@cam.ac.uk;kchoro@google.com;chalusf3@gmail.com;pacchiano@berkeley.edu;stamas@google.com;ret26@cam.ac.uk;aw665@cam.ac.uk,Algorithms/Stochastic Methods*; Reinforcement Learning and Planning/Reinforcement Learning,Spotlight
175,Cooperative Holistic 3D Scene Understanding from a Single RGB Image,"Holistic 3D indoor scene understanding involves jointly recovering the room layout, camera pose, and object bounding boxes, all in 3D. Most current methods either are inefficient or only tackle part of the problem. In this paper, we propose an end-to-end model that simultaneously solves all three tasks in real-time given a single RGB image. The key idea is to improve the prediction by i) parametrizing the targets (e.g., 3D boxes) instead of directly estimating the targets, and ii) cooperative training among different modules. Specifically, we parametrize the 3D object bounding boxes by the predictions from several modules, i.e., 3D camera pose, depth, and object poses and sizes. The proposed method brings up three major advantages. i) The parametrization helps maintain the consistency between 2D images and 3D world. ii) It largely reduces the prediction variances of the 3D coordinates. iii) Constraints can be imposed on the parametrizations to train different modules simultaneously. We call these constraints ``cooperative losses"". In this paper, we employ three cooperative losses for 3D bounding boxes, 2D projection, and physical constraints to estimate a geometrically consistent and physically plausible 3D scene. Experiments on the SUN RGB-D dataset shows that our method significantly outperforms prior approaches on 3D layout estimation, 3D object detection, and holistic scene understanding.","Siyuan Huang*,UCLA;Siyuan Qi,UCLA;Yinxue Xiao,University of California, Los Angeles;Yixin Zhu,UCLA;Ying Nian Wu,University of California, Los Angeles;Song-Chun Zhu,UCLA",huangsiyuan@ucla.edu;syqi@cs.ucla.edu;yinxuex@g.ucla.edu;yixin.zhu@ucla.edu;ywu@stat.ucla.edu;sczhu@stat.ucla.edu,Applications/Visual Scene Analysis and Interpretation*; Applications/Robotics; Neuroscience and Cognitive Science/Cognitive Science,Poster
177,An Efficient Pruning Algorithm for Robust Isotonic Regression,"We study a generalization of the classic isotonic regression problem  where we allow separable nonconvex objective functions, focusing on the case of estimators used in robust regression. A simple dynamic programming approach allows us to solve this problem to within Œµ-accuracy (of the global minimum) in time linear in 1/Œµ and the dimension. We can combine techniques from the convex case with branch-and-bound ideas to form a new algorithm for this problem that naturally exploits the shape of the objective function. Our algorithm achieves the best bounds for both the general nonconvex and convex case (linear in log (1/Œµ)), while performing much faster in practice than a straightforward dynamic programming approach, especially as the desired accuracy increases. ","Cong Han Lim*,University of Wisconsin-Madison",clim9@wisc.edu,Algorithms/Regression,Poster
178,PAC-learning in the presence of adversaries,"The existence of evasion attacks during the test phase of machine learning algorithms represents a significant challenge to both their deployment and understanding. These attacks can be carried out by adding imperceptible perturbations to inputs to generate adversarial examples and finding effective defenses and detectors has proven to be difficult. In this paper, we step away from the attack-defense arms race and seek to understand the limits of what can be learned in the presence of a test-time adversary. In particular, we extend the Probably Approximately Correct (PAC)-learning framework to account for the presence of an adversary. We first define corrupted hypothesis classes which arise from standard binary hypothesis classes in the presence of an evasion adversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted as the Adversarial VC-dimension. We then show that a corresponding Fundamental Theorem of Statistical learning can be proved for evasion adversaries, where the sample complexity is controlled by the Adversarial VC-dimension. We then explicitly derive the Adversarial VC-dimension for halfspace classifiers in the presence of a sample-wise norm-constrained adversary of the type commonly studied for evasion attacks and show that it is the same as the standard VC-dimensiont, closing an open question. Finally, we prove that the Adversarial VC-dimension can be either larger or smaller than the standard VC-dimension depending on the hypothesis class and adversary, making it an interesting object of study in its own right.","Daniel Cullina*,Princeton University;Arjun Nitin Bhagoji,Princeton University;Prateek Mittal,Princeton University",dcullina@princeton.edu;abhagoji@princeton.edu;pmittal@princeton.edu,Theory/Learning Theory,Poster
182,Sparse DNNs with Improved Adversarial Robustness,"Deep neural networks (DNNs) are computationally/memory-intensive and vulnerable to adversarial attacks, making them prohibitive in some real-world applications. By converting dense models into sparse ones, pruning appears to be a promising solution to reducing the computation/memory cost. This paper studies classification models, especially DNN-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness. Our analyses reveal, both theoretically and empirically, that nonlinear DNN-based classifiers behave differently under $l_2$ attacks from some linear ones. We further demonstrate that an appropriately higher model sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples.","Yiwen Guo*,Intel Labs China;Chao Zhang,Intel Labs China;Changshui Zhang,Tsinghua University;Yurong Chen,Intel Labs China",yiwen.guo@intel.com;pkuzc@pku.edu.cn;zcs@mail.tsinghua.edu.cn;yurong.chen@intel.com,Deep Learning/Adversarial Networks*; Algorithms/Sparsity and Compressed Sensing; Deep Learning/CNN Architectures; Deep Learning ->,Poster
185,Snap ML: A Hierarchical Framework for Machine Learning,"We describe a new software framework for fast training of generalized linear models. The framework, named Snap Machine Learning (Snap ML), combines recent advances in machine learning systems and algorithms in a nested manner to reflect the hierarchical architecture of modern computing systems. We prove theoretically that such a hierarchical system can accelerate training in distributed environments where intra-node communication is cheaper than inter-node communication. Additionally, we provide a review of the implementation of Snap ML in terms of GPU acceleration, pipelining, communication patterns and software architecture, highlighting aspects that were critical for achieving high performance. We evaluate the performance of Snap ML in both single-node and multi-node environments, quantifying the benefit of the hierarchical scheme and the data streaming functionality, and comparing with other widely-used machine learning software frameworks. Finally, we present a logistic regression benchmark on the Criteo Terabyte Click Logs dataset and show that Snap ML achieves the same test loss an order of magnitude faster than any of the previously reported results. ","Celestine Duenner*,IBM Research;Thomas Parnell,IBM Research;Dimitrios Sarigiannis,IBM Research;Nikolas Ioannou,IBM Research;Andreea Anghel,IBM Research;Gummadi Ravi,IBM Systems;Madhusudanan Kandasamy ,IBM Systems;Charalampos Pozidis,IBM Research Zurich",cdu@zurich.ibm.com;tpa@zurich.ibm.com;rig@zurich.ibm.com;nio@zurich.ibm.com;aan@zurich.ibm.com;ravigumm@in.ibm.com;madhusudanan@in.ibm.com;hap@zurich.ibm.com,"Applications/Hardware and Systems*; Data, Competitions, Implementations, and Software/Software Toolkits",Poster
186,See and Think: Disentangling Semantic Scene Completion,"Semantic scene completion predicts volumetric occupancy and object category of a 3D scene, which helps intelligent agents to understand and interact with the surroundings. In this work, we propose a disentangled framework, sequentially carrying out 2D semantic segmentation, 2D-3D reprojection and 3D semantic scene completion. This three-stage framework has three advantages: (1) explicit semantic segmentation significantly boosts performance; (2) flexible fusion ways of sensor data bring good extensibility; (3) progress in any subtask will promote the holistic performance. Experimental results show that regardless of inputing a single depth or RGB-D, our framework can generate high-quality semantic scene completion, and outperforms state-of-the-art approaches on both synthetic and real datasets.","Shice Liu,Institute of Computing Technology, Chinese Academy of Sciences;Yu Hu*,Institute of Computing Technology, Chinese Academy of Sciences;Yiming Zeng,Institute of Computing Technology Chinese Academy of Sciences;Qiankun Tang,Institute of Computing Technology, Chinese Academy of Sciences;Beibei Jin,Institute of Computing Technology, Chinese Academy of Sciences;Yinhe Han,Institute of Computing Technology, Chinese Academy of Sciences;Xiaowei Li,Institute of Computing Technology, Chinese Academy of Sciences",liushice@ict.ac.cn;huyu@ict.ac.cn;zengyiming@ict.ac.cn;tangqiankun@ict.ac.cn;jinbeibei@ict.ac.cn;huyu.cas@gmail.com;meridian_cn@hotmail.com,Applications/Computer Vision*; Deep Learning/CNN Architectures,Poster
187,Chain of Reasoning for Visual Question Answering,"Reasoning plays an essential role in Visual Question Answering (VQA). Multi-step and dynamic reasoning is often necessary for answering complex questions. For example, a question ""What is placed next to the bus on the right of the picture?"" talks about a compound object ""bus on the right,"" which is generated by the relation <bus, on the right of, picture>. Furthermore, a new relation including this compound object <sign, next to, bus on the right> is then required to infer the answer. However, previous methods support either one-step or static reasoning, without updating relations or generating compound objects. This paper proposes a novel reasoning model for addressing these problems. A chain of reasoning (CoR) is constructed for supporting multi-step and dynamic reasoning on changed relations and objects. In detail, iteratively, the relational reasoning operations form new relations between objects, and the object refining operations generate new compound objects from relations. We achieve new state-of-the-art results on four publicly available datasets. The visualization of the chain of reasoning illustrates the progress that the CoR generates new compound objects that lead to the answer of the question step by step.","Chenfei Wu*,Beijing University of Posts and Telecommunications;Jinlai Liu,Beijing University of Posts and Telecommunications;Xiaojie Wang,Beijing University of Posts and Telecommunications;Xuan Dong,Beijing University of Posts and Telecommunications",wuchenfei@bupt.edu.cn;liujinlai@bupt.edu.cn;xjwang@bupt.edu.cn;dongxuan8811@gmail.com,Applications/Visual Question Answering*; Neuroscience and Cognitive Science/Reasoning,Poster
188,Sigsoftmax: Reanalysis of the Softmax Bottleneck,"Softmax is an output activation function for modeling categorical probability distributions in many applications of deep learning. However, a recent study revealed that softmax can be a bottleneck of representational capacity of neural networks in language modeling (the softmax bottleneck). In this paper, we propose an output activation function for breaking the softmax bottleneck without additional parameters. We re-analyze the softmax bottleneck from the perspective of the output set of log-softmax and identify the cause of the softmax bottleneck. On the basis of this analysis, we propose sigsoftmax, which is composed of a multiplication of an exponential function and sigmoid function. Sigsoftmax can break the softmax bottleneck. The experiments on language modeling demonstrate that sigsoftmax and mixture of sigsoftmax outperform softmax and mixture of softmax, respectively.","Sekitoshi Kanai*,NTT Software Innovation Center;Yasuhiro Fujiwara,NTT Software Innovation Center;Yuki Yamanaka,NTT Secure Platform Laboratories;Shuichi Adachi,Keio University",kanai.sekitoshi@lab.ntt.co.jp;fujiwara.yasuhiro@lab.ntt.co.jp;yamanaka.yuki@lab.ntt.co.jp;adachi@appi.keio.ac.jp,Deep Learning*; Deep Learning/Recurrent Networks; Deep Learning/Supervised Deep Networks,Poster
191,Deep Non-Blind Deconvolution via Generalized Low-Rank Approximation,"In this paper, we present a deep convolutional neural network to capture the inherent properties of image degradation,  which can handle different kernels and saturated pixels in a unified framework. The proposed neural network is motivated by the low-rank property of pseudo-inverse kernels. We first compute a generalized low-rank approximation for a large number of blur kernels, and then use separable filters to initialize the convolutional parameters in the network. Our analysis shows that the estimated decomposed matrices contain the most essential information of the input kernel,  which ensures the proposed network to handle various blurs in a unified framework and generate high-quality deblurring results. Experimental results on benchmark datasets with noise and saturated pixels demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.","Wenqi Ren*,Institute of Information Engineering,  Chinese Academy of Sciences;Jiawei Zhang,Sensetime Research;Lin Ma,Tencent AI Lab;Jinshan Pan,Nanjing University of Science and Technology;Xiaochun Cao,Chinese Academy of Sciences;Wangmeng Zuo,Harbin Institute of Technology;Wei Liu,Tencent AI Lab;Ming-Hsuan Yang,University of California at Merced",rwq.renwenqi@gmail.com;zhjw1988@gmail.com;forest.linma@gmail.com;sdluran@gmail.com;caoxiaochun@iie.ac.cn;cswmzuo@gmail.com;wl2223@columbia.edu;mhyang@ucmerced.edu,Applications/Computer Vision*; Applications/Denoising,Poster
192,Probabilistic Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC,"We introduce Tempered Geodesic MCMC (TG-MCMC) algorithm for initializing pose graph optimization problems, arising in various scenarios such as SFM (structure from motion) or SLAM (simultaneous localization and mapping). TG-MCMC is first of its kind as it unites asymptotically global non-convex optimization on the spherical manifold of quaternions  with posterior sampling, in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of individual solutions. We devise rigorous theoretical convergence guarantees for our method and extensively evaluate it on synthetic and real benchmark datasets. Besides its elegance in formulation and theory, we show that our method is robust to missing data, noise and the estimated uncertainties capture intuitive properties of the data.","Tolga Birdal*,TU Munich;Umut Simsekli,Telecom ParisTech;Mustafa Onur Eken,Technical University of Munich;Slobodan Ilic,Siemens AG",tolga.birdal@tum.de;umut.simsekli@telecom-paristech.fr;onur.eken@tum.de;slobodan.ilic@siemens.com,Applications/Computer Vision*; Applications/Robotics; Probabilistic Methods/MCMC,Poster
207,MetaAnchor: Learning to Detect Objects with Customized Anchors,"We propose a novel and flexible anchor mechanism named MetaAnchor for object detection frameworks. Unlike many previous detectors model anchors via a predefined manner, in MetaAnchor anchor functions could be dynamically generated from the arbitrary customized prior boxes. Taking advantage of weight prediction, MetaAnchor is able to work with most of the anchor-based object detection systems such as RetinaNet. Compared with the predefined anchor scheme, we empirically find that MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on the transfer task. Our experiment on COCO detection task shows MetaAnchor consistently outperforms the counterparts in various scenarios.","Tong Yang*,Megvii Inc (Face++);Xiangyu Zhang,Megvii Inc;Zeming Li,Megvii(Face++) Inc;Wenqiang Zhang,Fudan University;Jian Sun,Megvii, Face++",yangt15@fudan.edu.cn;zhangxiangyu@megvii.com;lizeming@megvii.com;wqzhang@fudan.edu.cn;sunjian@megvii.com,Applications/Object Detection,Poster
210,Image Inpainting via Generative Multi-column Convolutional Neural Networks,"In this paper, we propose a generative multi-column network for image inpainting. This network can synthesize different image components in a parallel manner within one stage. To better characterize global structures, we design a confidence-driven reconstruction loss while an implicit diversified MRF regularization is adopted to enhance local details. The multi-column network combined with the reconstruction and MRF loss propagates local and global information derived from context to the target inpainting regions. Extensive experiments on challenging street view, face, natural objects and scenes manifest that our method can produce visual compelling results even without previously common post-processing.","Yi Wang*,Chinese University of Hong Kong;Xin Tao,CUHK;Xiaojuan Qi,CUHK;Xiaoyong Shen,CUHK;Jiaya Jia,Chinese University of Hong Kong",yiwang@cse.cuhk.edu.hk;xtao@cse.cuhk.edu.hk;xjqi@cse.cuhk.edu.hk;goodshenxy@gmail.com;leojia@cse.cuhk.edu.hk,Applications/Computer Vision*; Applications/Computational Photography,Poster
231,On Misinformation Containment in Online Social Networks,"The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. This paper makes four contributions. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of $\Omega(2^{\log^{1-\epsilon}n^4})$ in polynomial time unless $NP \subseteq DTIME(n^{\polylog{n}})$. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.","Guangmo Tong*,University of Delaware;Ding-Zhu Du,University of Texas at Dallas;Weili Wu,University of Texas at Dallas",gxt140030@utdallas.edu;dzdu@utdallas.edu;weiliwu@utdallas.edu,Applications/Computational Social Science*; Algorithms; Applications/Network Analysis; Optimization/Combinatorial Optimization; Optimizati,Poster
233,A^2-Nets: Double Attention Networks,"Learning to capture long-range relations is fundamental to image/video recognition. Existing CNN models generally rely on increasing depth to model such relations which is highly inefficient. In this work, we propose the ``double attention block'', a novel component that aggregates and propagates informative global features from the entire spatio-temporal space of input images/videos, enabling subsequent convolution layers to access features from the entire space efficiently. The component is designed with a double attention mechanism in two steps, where the first step gathers features from the entire space into a compact set through second-order attention pooling and the second step adaptively selects and distributes features to each location via another attention. The proposed double attention block is easy to adopt and can be plugged into existing deep neural networks conveniently. We conduct extensive ablation studies and experiments on both image and video recognition tasks for evaluating its performance. On the image recognition task, a ResNet-50 equipped with our double attention blocks outperforms a much larger ResNet-152 architecture on ImageNet-1k dataset with over 40% less the number of parameters and less FLOPs. On the action recognition task, our proposed model achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with significantly higher efficiency than recent works.","Yunpeng Chen*,National University of Singapore;Yannis Kalantidis,Facebook Research, USA;Jianshu Li,NUS;Yan Shuicheng,National University of Singapore;Jiashi Feng,NUS",chenyunpeng@u.nus.edu;ykalant@image.ntua.gr;jianshu@u.nus.edu;eleyans@nus.edu.sg;elefjia@nus.edu.sg,Deep Learning/CNN Architectures*; Applications/Computer Vision; Applications/Object Recognition; Deep Learning; Deep Learning ->,Poster
236,Self-Supervised Generation of Spatial Audio for 360-degree Video,"We introduce an approach to convert mono audio recorded by a 360-degree video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere.
Spatial audio is an important component of immersive 360-degree video viewing, but spatial audio microphones are still rare in current 360-degree video production.
Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis from the audio and 360-degree video frames.
We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360-degree videos uploaded with spatial audio.
During training, ground truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network.
Using our approach we show that it is possible to infer the spatial localization of sounds based only on a synchronized 360-degree video and the mono audio track.","Pedro Morgado*,University of California, San Diego;Nuno Vasconcelos,UC San Diego;Timothy Langlois,Adobe Systems Inc;Oliver Wang,Adobe Systems Inc",pmaravil@eng.ucsd.edu;nvasconcelos@ucsd.edu;tlangloi@adobe.com;owang@adobe.com,Applications,Poster
252,How Many Samples are Needed to Learn a Convolutional Neural Network?,"A widespread folklore for explaining the success of convolutional neural network (CNN) is that CNN is a more compact representation than the fully connected neural network (FNN) and thus requires fewer samples for learning. We initiate the study of rigorously characterizing the sample complexity of learning convolutional neural networks. We show that for learning an $m$-dimensional convolutional filter with linear activation acting on a $d$-dimensional input, the sample complexity of achieving population prediction error of $\epsilon$ is $\widetilde{O}(m/\epsilon^2)$ whereas its FNN counterpart needs at least $\Omega(d/\epsilon^2)$ samples. Since $m \ll d$, this result demonstrates the advantage of using CNN. We further consider the sample complexity of learning a one-hidden-layer CNN with linear activation where both the $m$-dimensional convolutional filter and the $r$-dimensional output weights are unknown. For this model, we show the sample complexity is $\widetilde{O}\left((m+r)/\epsilon^2\right)$ when the ratio between the stride size and the filter size is a constant. For both models, we also present lower bounds showing our sample complexities are tight up to logarithmic factors. Our main tools for deriving these results are localized empirical process and a new lemma characterizing the convolutional structure. We believe these tools may inspire further developments in understanding CNN.","Simon Du*,Carnegie Mellon University;Yining Wang,CMU;Xiyu Zhai,Massachusetts Institute of Technology;Sivaraman Balakrishnan,Carnegie Mellon University;Ruslan Salakhutdinov,Carnegie Mellon University;Aarti Singh,Carnegie Mellon University",ssdu@cs.cmu.edu;ynwang.yining@gmail.com;xiyuzhai@mit.edu;siva@stat.cmu.edu;rsalakhu@cs.cmu.edu;aartisingh@cmu.edu,Deep Learning,Poster
255,Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced,"We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes $\eta_t=O(t^{‚àí(1/2+\delta)}) (0<\delta\le1/2)$ automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.","Simon Du,Carnegie Mellon University;Wei Hu*,Princeton University;Jason Lee,USC",ssdu@cs.cmu.edu;huwei@cs.princeton.edu;jasonlee@marshall.usc.edu,Optimization/Non-Convex Optimization*; Deep Learning/Optimization for Deep Networks,Poster
259,Optimization for Approximate Submodularity,"We consider the problem of maximizing a submodular function when given access to its approximate version. Submodular functions are heavily studied in a wide variety of disciplines, since they are used to model many real world phenomena, and are amenable to optimization. However, there are many cases in which the phenomena we observe is only approximately submodular and the approximation guarantees cease to hold. We describe a technique which we call the sampled
mean approximation that yields strong guarantees for maximization of submodular functions from approximate surrogates under cardinality and intersection of matroid constraints. In particular, we show tight guarantees for maximization under a cardinality constraint and 1/(1+P) approximation
under intersection of P matroids.","Yaron Singer*,Harvard University;Avinatan Hassidim,Bar Ilan University",yaron@seas.harvard.edu;avinatan@cs.biu.ac.il,Optimization/Submodular Optimization,Poster
267,(Probably) Concave Graph Matching,"In this paper we address the graph matching problem. Following the recent works of \cite{zaslavskiy2009path,Vestner2017} we analyze and generalize the idea of concave relaxations. We introduce the concepts of \emph{conditionally concave} and \emph{probably conditionally concave} energies on polytopes and show that they encapsulate many instances of the graph matching problem, including matching Euclidean graphs and graphs on surfaces. We further prove that local minima of probably conditionally concave energies on general matching polytopes (\eg, doubly stochastic) are with high probability extreme points of the matching polytope (\eg, permutations). ","Haggai Maron*,Weizmann;Yaron Lipman,Weizmann Institute of Science",haggai.maron@weizmann.ac.il;yaron.lipman@weizmann.ac.il,Optimization/Convex Optimization*; Optimization/Non-Convex Optimization,Spotlight
268,Deep Defense: Training DNNs with Improved Adversarial Robustness,"Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named ""deep defense"". Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results are available at https://github.com/ZiangYan/deepdefense.pytorch.","Ziang Yan*,Tsinghua University;Yiwen Guo,Intel Labs China;Changshui Zhang,Tsinghua University",yza15@mails.tsinghua.edu.cn;yiwen.guo@intel.com;zcs@mail.tsinghua.edu.cn,Deep Learning/Adversarial Networks*; Deep Learning/Supervised Deep Networks,Poster
270,Rest-Katyusha: Exploiting the Solution's Structure via Scheduled Restart Schemes,"We propose a structure-adaptive variant of the state-of-the-art stochastic variance-reduced gradient algorithm Katyusha for  regularized empirical risk minimization. The proposed method is able to exploit the intrinsic low-dimensional structure of the solution, such as sparsity or low rank which is enforced by a non-smooth regularization, to achieve even faster convergence rate. This provable algorithmic improvement is done by restarting the Katyusha algorithm according to restricted strong-convexity constants. We demonstrate the effectiveness of our approach via numerical experiments.","Junqi Tang*,University of Edinburgh;Mohammad Golbabaee,University of Bath;Francis Bach,INRIA - Ecole Normale Sup√©rieure;Mike Davies,University of Edinburgh",J.Tang@ed.ac.uk;M.Golbabaee@bath.ac.uk;francis.bach@inria.fr;mike.davies@ed.ac.uk,Optimization/Convex Optimization,Poster
280,Implicit Reparameterization Gradients,"By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions.  We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.","Michael Figurnov*,DeepMind;Shakir Mohamed,DeepMind;Andriy Mnih,DeepMind",mfigurnov@google.com;shakir@google.com;amnih@google.com,Probabilistic Methods/Variational Inference*; Probabilistic Methods/Latent Variable Models,Spotlight
282,Training DNNs with Hybrid Block Floating Point,"The wide adoption of DNNs has given birth to unrelenting computing requirements, forcing datacenter operators to adopt domain-specific accelerators to train them. These accelerators typically employ densely packed full precision floating-point arithmetic to maximize performance per area. Ongoing research efforts seek to further increase that performance density by replacing floating-point with fixed-point arithmetic. However, a significant roadblock for these attempts has been fixed point's narrow dynamic range, which is insufficient for DNN training convergence. We identify block floating point~(BFP) as a promising alternative representation since it exhibits wide dynamic range and enables the majority of DNN operations to be performed with fixed-point logic. Unfortunately, BFP alone introduces several limitations that preclude its direct applicability. In this work, we introduce HBFP, a hybrid BFP-FP approach, which performs all dot products in BFP and other operations in floating point. HBFP delivers the best of both worlds: the high accuracy of floating point at the superior hardware density of fixed point. For a wide variety of models we show that HBFP matches floating point's accuracy while enabling hardware implementations that deliver up to 8.5x higher throughput.","Mario Drumond*,EPFL;Tao Lin,EPFL;Martin Jaggi,EPFL;Babak Falsafi,EPFL",mario.drumond@epfl.ch;tao.lin@epfl.ch;martin.jaggi@epfl.ch;babak.falsafi@epfl.ch,Deep Learning*; Deep Learning/Optimization for Deep Networks; Optimization,Poster
283,A Model for Learned Bloom Filters and Optimizing by Sandwiching,"Recent work has suggested enhancing Bloom filters by using a pre-filter, based on applying machine learning to determine a function that models the data set the Bloom filter is meant to represent.  Here we model such learned Bloom filters, with the following outcomes: (1) we clarify what guarantees can and cannot be associated with such a structure; (2) we show how to estimate what size the learning function must obtain in order to obtain improved performance;  (3) we provide a simple method, sandwiching, for optimizing learned Bloom filters;  and (4) we propose a design and analysis approach for a learned Bloomier filter, based on our modeling approach.","Michael Mitzenmacher*,Harvard",michaelm@eecs.harvard.edu,Algorithms*; Algorithms/Classification; Algorithms/Stochastic Methods; Applications/Hardware and Systems,Poster
292,Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis,"Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is light-weight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our Warping-GAN that significantly outperforms all existing methods on two large datasets.","Haoye Dong,Sun Yat-sen University;Xiaodan Liang*,Carnegie Mellon University;Ke Gong,Sun Yat-sen University			 ;Hanjiang Lai,Sun Yat-Sen university;Jia Zhu,South China Normal University;Jian Yin,Sun Yat-Sen University",donghy7@mail2.sysu.edu.cn;xdliang328@gmail.com;kegong936@gmail.com;laihanj3@mail.sysu.edu.cn;jzhu@m.scnu.edu.cn;issjyin@mail.sysu.edu.cn,Applications/Computer Vision*; Deep Learning/Adversarial Networks,Poster
295,Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions,"Various 3D semantic attributes such as segmentation masks, geometric features, keypoints, and materials can be encoded as per-point probe functions on 3D geometries. Given a collection of related 3D shapes, we consider how to jointly analyze such probe functions over different shapes, and how to discover common latent structures using a neural network --- even in the absence of any correspondence information. Our network is trained on point cloud representations of shape geometry and associated semantic functions on that point cloud. These functions express a shared semantic understanding of the shapes but are not coordinated in any way. For example, in a segmentation task, the functions can be indicator functions of arbitrary sets of shape parts, with the particular combination involved not known to the network. Our network is able to produce a small dictionary of basis functions for each shape, a dictionary whose span includes the semantic functions provided for that shape.  Even though our shapes have independent discretizations and no functional correspondences are provided, the network is able to generate latent bases, in a consistent order, that reflect the shared semantic structure among the shapes. We demonstrate the effectiveness of our technique in various segmentation and keypoint selection applications.","Minhyuk Sung*,Stanford University;Hao Su,UCSD;Ronald Yu,UCSD;Leonidas Guibas,Stanford University",mhsung@cs.stanford.edu;haosu@eng.ucsd.edu;ronaldiscool@gmail.com;guibas@cs.stanford.edu,"Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA)*; Algorithms/Collaborative Filtering; Algorithms/Sparsity and Compressed Sensing; Deep Learning",Poster
296,"Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling","Nonlocal neural networks have been proposed and shown to be effective in several computer vision tasks, where the nonlocal operations can directly capture long-range dependencies in the feature space. In this paper, we study the nature of diffusion and damping effect of nonlocal networks by doing the spectrum analysis on the weight matrices of the well-trained networks, and propose a new formulation of the nonlocal block. The new block not only learns the nonlocal interactions but also has stable dynamics and thus allows deeper nonlocal structures. Moreover, we interpret our formulation from the general nonlocal modeling perspective, where we make connections between the proposed nonlocal network and other nonlocal models, such as nonlocal diffusion processes and nonlocal Markov jump processes.","Yunzhe Tao*,Columbia University;Qi Sun,CSRC & USTC & CU;Qiang Du,Columbia University;Wei Liu,Tencent AI Lab",y.tao@columbia.edu;sunqi@csrc.ac.cn;qd2125@columbia.edu;wl2223@columbia.edu,Deep Learning/CNN Architectures*; Deep Learning/Supervised Deep Networks,Poster
298,Are ResNets Provably Better than Linear Predictors?,"A residual network (or ResNet) is a standard deep neural net architecture, with state-of-the-art performance across numerous applications. The main premise of ResNets is that they allow the training of each layer to focus on fitting just the residual of the previous layer's output and the target output. Thus, we should expect that the trained network is no worse than what we can obtain if we remove the residual layers and train a shallower network instead. However, due to the non-convexity of the optimization problem, it is not at all clear that ResNets indeed achieve this behavior, rather than getting stuck at some arbitrarily poor local minimum. In this paper, we rigorously prove that arbitrarily deep, nonlinear residual units indeed exhibit this behavior, in the sense that the optimization landscape contains no local minima with value above what can be obtained with a linear predictor (namely a 1-layer network). Notably, we show this under minimal or no assumptions on the precise network architecture, data distribution, or loss function used. We also provide a quantitative analysis of approximate stationary points for this problem. Finally, we show that with a certain tweak to the architecture, training the network with standard stochastic gradient descent achieves an objective value close or better than any linear predictor.","Ohad Shamir*,Weizmann Institute of Science",ohad.shamir@weizmann.ac.il,Optimization/Non-Convex Optimization*; Deep Learning/Optimization for Deep Networks; Theory,Poster
305,Learning to Decompose and Disentangle Representations for Video Prediction,"Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the high-dimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we would intuitively do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision.","Jun-Ting Hsieh*,Stanford University;Bingbin Liu,Stanford University;De-An Huang,Stanford University;Li Fei-Fei,Stanford University;Juan Carlos Niebles,Stanford University",junting@stanford.edu;bingbin@stanford.edu;dahuang@stanford.edu;feifeili@cs.stanford.edu;jniebles@cs.stanford.edu,Applications/Video Analysis,Poster
318,Multi-Task Learning as Multi-Objective Optimization,"In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem since different tasks may conflict, necessitating a trade-off between them. A common approach to this trade-off is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this proxy is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. Although these algorithms have desirable theoretical guarantees, they are not directly applicable to large-scale learning problems. We therefore propose efficient and accurate approximations. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method yields higher-performing models than recent multi-task learning formulations or per-task training.","Ozan Sener*,Intel Labs;Vladlen Koltun,Intel Labs",ozan.sener@intel.com;vkoltun@gmail.com,Deep Learning*; Algorithms/Multitask and Transfer Learning; Deep Learning/Optimization for Deep Networks,Poster
319,Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search,"We present a learning-based approach to computing solutions for certain NP-hard problems. Our approach combines deep learning techniques with useful algorithmic elements from classic heuristics. The central component is a graph convolutional network that is trained to estimate the likelihood, for each vertex in a graph, of whether this vertex is part of the optimal solution. The network is designed and trained to synthesize a diverse set of solutions, which enables rapid exploration of the solution space via tree search. The presented approach is evaluated on four canonical NP-hard problems and five datasets, which include benchmark satisfiability problems and real social network graphs with up to a hundred thousand nodes. Experimental results demonstrate that the presented approach substantially outperforms recent work, generalizes across datasets, and scales to graphs that are orders of magnitude larger than those used during training.","Zhuwen Li*,Intel Labs;Qifeng Chen,HKUST;Vladlen Koltun,Intel Labs",lzhuwen@gmail.com;chenqifeng22@gmail.com;vkoltun@gmail.com,Deep Learning,Poster
332,Self-Handicapping Network for Integral Object Attention,"Recently, adversarial erasing for weakly-supervised object attention has been deeply studied due to its capability in localizing integral object regions. However, such a strategy raises one key problem that attention regions will gradually expand to non-object regions as training iterations continue, which significantly decreases the quality of the produced attention maps. To tackle such an issue as well as promote the quality of object attention, we introduce a simple yet effective Self-Handicapping Network (ShaNet) to prohibit attentions from spreading to unexpected background regions. In particular, ShaNet leverages two self-handicapping strategies to encourage networks to use reliable object and background cues for learning to attention. In this way, integral object regions can be effectively highlighted without including much more background regions. To test the quality of the generated attention maps, we employ the mined object regions as heuristic cues for learning semantic segmentation models. Experiments on Pascal VOC well demonstrate the superiority of the ShaNet over other state-of-the-art methods.","Qibin Hou*,Nankai University;Peng-Tao Jiang,Nankai University;Yunchao Wei,UIUC;Ming-Ming Cheng,Nankai University",andrewhoux@gmail.com;pt.jiang@mail.nankai.edu.cn;wychao1987@gmail.com;cmm@nankai.edu.cn,Applications/Image Segmentation*; Deep Learning/Attention Models,Poster
342,LinkNet: Relational Embedding for Scene Graph,"Objects and their relationships are critical contents for image understanding. A scene graph provides a structured description that captures these properties of an image. However, reasoning about the relationships between objects is very challenging and only a few recent works have attempted to solve the problem of generating a scene graph from an image. In this paper, we present a novel method that improves scene graph generation by explicitly modeling inter-dependency among the entire object instances. We design a simple and effective relational embedding module that enables our model to jointly represent connections among all related objects, rather than focus on an object in isolation. Our novel method significantly benefits two main parts of the scene graph generation task: object classification and relationship classification. Using it on top of a basic Faster R-CNN, our model achieves state-of-the-art results on the Visual Genome benchmark. We further push the performance by introducing global context encoding module and geometrical layout encoding module. We validate our final model, LinkNet, through extensive ablation studies, demonstrating its efficacy in scene graph generation.","Sanghyun Woo*,KAIST;Dahun Kim,KAIST;Donghyeon Cho,KAIST;In So Kweon,KAIST",shwoo93@kaist.ac.kr;mcahny@kaist.ac.kr;cdh12242@gmail.com;iskweon77@kaist.ac.kr,Applications/Computer Vision*; Applications/Object Recognition,Poster
345,How to Start Training: The Effect of Initialization and Architecture,"We identify and study two common failure modes for early training in deep ReLU nets. For each, we give a rigorous proof of when it occurs and how to avoid it, for fully connected, convolutional, and residual architectures. We show that the first failure mode, exploding or vanishing mean activation length, can be avoided by initializing weights from a symmetric distribution with variance 2/fan-in and, for ResNets, by correctly scaling the residual modules. We prove that the second failure mode, exponentially large variance of activation length, never occurs in residual nets once the first failure mode is avoided. In contrast, for fully connected nets, we prove that this failure mode can happen and is avoided by keeping constant the sum of the reciprocals of layer widths. We demonstrate empirically the effectiveness of our theoretical results in predicting when networks are able to start training. In particular, we note that many popular initializations fail our criteria, whereas correct initialization and architecture allows much deeper networks to be trained.","Boris Hanin*,Texas A&M;David Rolnick,MIT",bhanin@tamu.edu;drolnick@mit.edu,Theory/Learning Theory*; Deep Learning/Optimization for Deep Networks,Poster
346,Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?,"We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in the sum of the reciprocals of the hidden layer widths. Our approach complements the mean field theory analysis of random neural nets. From this point of view, we rigorously compute the finite width corrections to gradients at the edge of chaos. ","Boris Hanin*,Texas A&M",bhanin@tamu.edu,Deep Learning/Optimization for Deep Networks*; Theory/Learning Theory,Poster
347,Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives,"In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be  minimally and necessarily \emph{absent} (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically \emph{absent} is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.","Amit Dhurandhar*,IBM Research;Pin-Yu Chen,IBM Research;Ronny Luss,IBM Research;Chun-Chen Tu,University of Michigan;Pai-Shun Ting,UMICH;Karthikeyan Shanmugam,IBM Research;Payel Das,IBM Research",adhuran@us.ibm.com;pin-yu.chen@ibm.com;rluss@us.ibm.com;timtu@umich.edu;paishun@umich.edu;karthikeyan.shanmugam2@ibm.com;daspa@us.ibm.com,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Classification; Neuroscience and Cognitive Science/Neuroscience",Poster
355,HitNet: Hybrid Ternary Recurrent Neural Network,"Quantization is a promising technique to reduce the model size, memory footprint, and massive computation operations of recurrent neural networks (RNNs) for embedded devices with limited resources. Although extreme low-bit quantization has achieved impressive success on convolutional neural networks, it still suffers from huge accuracy degradation on RNNs with the same low-bit precision. In this paper, we first investigate the accuracy degradation on RNN models under different quantization schemes, and the distribution of tensor values in the full precision model. Our observation reveals that due to the difference between the distributions of weights and activations, different quantization methods are suitable for different parts of models. Based on our observation, we propose HitNet, a hybrid ternary recurrent neural network, which bridges the accuracy gap between the full precision model and the quantized model. In HitNet, we develop a hybrid quantization method to quantize weights and activations. Moreover, we introduce a sloping factor motivated by prior work on Boltzmann machine to activation functions, further closing the accuracy gap between the full precision model and the quantized model. Overall, our HitNet can quantize RNN models into ternary values, {-1, 0, 1}, outperforming the state-of-the-art quantization methods on RNN models significantly. We test it on typical RNN models, such as Long-Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), on which the results outperform previous work significantly. For example, we improve the perplexity per word (PPW) of a ternary LSTM on Penn Tree Bank (PTB) corpus from 126 (the state-of-the-art result to the best of our knowledge) to 110.3 with a full precision model in 97.2, and a ternary GRU from 142 to 113.5 with a full precision model in 102.7.   ","Peiqi Wang*,Tsinghua University;Xinfeng Xie,UCSB;Lei Deng,University of California, Santa Barbara;Guoqi Li,Tsinghua University;Dongsheng Wang,Tsinghua University;Yuan Xie,University of California, Santa Barbara",wpq14@mails.tsinghua.edu.cn;xinfeng@umail.ucsb.edu;leideng@ucsb.edu;liguoqi@mail.tsinghua.edu.cn;wds@mail.tsinghua.edu.cn;yuanxie@ucsb.edu,Deep Learning/Efficient Inference Methods*; Deep Learning/Recurrent Networks,Poster
358,A Unified Framework for Extensive-Form Game Abstraction with Bounds,"Abstraction has long been a key component in the practical solving of
large-scale extensive-form games. Despite this, abstraction remains poorly
understood. There have been some recent theoretical results but they have been
confined to specific assumptions on abstraction structure and are specific to
various specific disjoint types of abstraction, and specific solution concepts,
for example, exact Nash equilibria or strategies with bounded immediate regret.
In this paper we present a unified framework for analyzing abstractions that can
express all types of abstractions and solution concepts used in prior papers
with performance guarantees---while maintaining comparable bounds on abstraction
quality. Moreover, our framework extends well beyond prior work. We present the
first exact decomposition of abstraction error for a broad class of abstractions
that encompasses abstractions used in practice. Because it is significantly more
general, this decomposition has a stronger dependence on the specific strategy
computed in the abstraction. We show that this dependence can be removed by
making similar, though slightly weaker, assumptions than in prior work. We also
show, via counterexample, that such assumptions are necessary for some games.
Finally, we prove the first bounds for how $\epsilon$-Nash equilibria computed
in abstractions perform in the original game. This is important because often
one cannot afford to compute an exact Nash equilibrium in the abstraction.
All our results apply to general-sum n-player games.","Christian Kroer*,Carnegie Mellon University;Tuomas Sandholm,Carnegie Mellon University",ckroer@cs.cmu.edu;sandholm@cs.cmu.edu,Theory/Game Theory and Computational Economics,Poster
360,Removing the Feature Correlation Effect of Multiplicative Noise,"Multiplicative noise, including dropout, is widely used to regularize deep neural networks (DNNs), and is shown to be effective in a wide range of architectures and tasks. From an information perspective, we consider injecting multiplicative noise into a DNN as training the network to solve the task with noisy information pathways, which leads to the observation that multiplicative noise tends to increase the correlation between features, so as to increase the signal-to-noise ratio of information pathways. However, high feature correlation is undesirable, as it increases redundancy in representations. In this work, we propose feature-decorrelating multiplicative noise (FDMN), which exploits batch normalization to remove the correlation effect in a simple yet effective way. We show that FDMN significantly improves the performance of standard multiplicative noise on image classification tasks, providing a better alternative to dropout for batch-normalized networks. Additionally, we present a unified view of FDMN and shake-shake regularization, which explains the performance gain of the latter.","Zijun Zhang*,University of Calgary;Yining Zhang,University of Calgary;Zongpeng Li,Wuhan University",zijun.zhang@ucalgary.ca;yining.zhang1@ucalgary.ca;zongpeng@whu.edu.cn,Theory/Regularization*; Theory/Information Theory,Spotlight
369,Maximum-Entropy Fine Grained Classification,"Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes, and often requires expert annotators to collect data. Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve state-of-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values, amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.","Abhimanyu Dubey*,Massachusetts Institute of Technology;Otkrist Gupta,MIT;Ramesh Raskar,Massachusetts Institute of Technology;Nikhil Naik,MIT",dubeya@mit.edu;otkrist@mit.edu;raskar@mit.edu;naik@mit.edu,Algorithms/Classification*; Applications/Object Recognition,Poster
377,On Learning Markov Chains,"Estimating an unknown discrete distribution from its samples is a fundamental problem in statistical learning. Over the past decade, this problem has attracted a significant amount of research effort and has been solved for different divergence measures. Surprisingly, an equally important problem, estimating an unknown Markov chain from its samples, is still far from understood. We consider the problem of determining the minimax risk (expected loss) of estimating an unknown $k$-state Markov chain from its $n$ sequential sample points. We present two related but different formulations: (1) predicting the conditional distribution of the next sample point with respect to the KL-divergence. Under this formulation, we show that the minimax prediction risk is both $\Omega(k\log\log n/n)$ and $\mathcal{O}(k^2\log\log n/n)$. (2) estimating the transition matrix with respect to a natural loss induced by some $f$-divergence measure. Under this formulation, if we allow the transition probabilities to be arbitrarily small, then certain states may not even be observable. Therefore, we consider the case when the transition probabilities are bounded away from zero. We completely resolve the latter problem for essentially all sufficiently smooth $f$-divergences, including but not limited to $L_2$-, Chi-squared, KL-, Hellinger, and Alpha-divergences. Additionally, we show that for the KL-divergence, if one allows the transition probabilities to be as small as $1/n$, then the minimax risk would be roughly $\log\log n$ times larger.  The agreement between our theory and experimental results is excellent.","Yi HAO*,University of California, San Diego;Alon Orlitsky,UC San Diego;Venkatadheeraj Pichapati,UCSD",yih179@eng.ucsd.edu;alon@ucsd.edu;dheerajpv7@ucsd.edu,Theory*; Theory/Information Theory,Poster
378,A Neural Compositional Paradigm for Image Captioning,"Mainstream captioning models often follow a sequential structure to generate cap-
tions, leading to issues such as introduction of irrelevant semantics, lack of diversity in the generated captions, and inadequate generalization performance. In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic representation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom-up manner. Compared to conventional ones, our paradigm better preserves the semantic content through an explicit factorization of semantics and syntax. By using the compositional generation procedure, caption construction follows a recursive structure, which naturally fits the properties of human language. Moreover, the proposed compositional procedure requires less data to train, generalizes better, and yields more diverse captions.","Bo Dai*,the Chinese University of Hong Kong;Sanja Fidler,University of Toronto;Dahua Lin,The Chinese University of Hong Kong",doubledaibo@gmail.com;fidler@cs.toronto.edu;dhlin@ie.cuhk.edu.hk,Applications/Computer Vision*; Applications/Natural Language Processing; Deep Learning/Generative Models,Poster
386,Quantifying Learning Guarantees for Convex but Inconsistent Surrogates,"We study the consistency properties of machine learning methods based on minimizing convex surrogates. We extend the recent framework of Osokin et al. (2017) for quantitative analysis of the consistency properties to the case of inconsistent surrogates. Our key technical contribution consists in the new lower bound on the calibration function for the quadratic surrogate, which is non-trivial (not always zero) for inconsistent cases. The new bound allows to quantify the level of inconsistency of the setting and shows how learning with inconsistent surrogates can have guarantees on sample complexity and optimization difficulty. We apply our theory in two concrete cases: hierarchical classification with the tree-structured loss and ranking with the mean average precision loss. The results show the approximation-computation trade-offs caused by inconsistent surrogates and their potential benefits.","Kirill Struminsky,NRU HSE;Simon Lacoste-Julien,University of Montreal;Anton Osokin*,NRU HSE",k.struminsky@gmail.com;slacoste@iro.umontreal.ca;anton.osokin@gmail.com,Theory/Learning Theory*; Algorithms/Structured Prediction,Poster
390,Dialog-based Interactive Image Retrieval,"Inspired by the enormous growth of huge online media collections of many types (e.g. images, audio, video, e-books, etc.), and the paucity of intelligent retrieval systems, this paper introduces a novel approach to interactive visual content retrieval. The proposed retrieval framework is guided by free-form natural language feedback from users, allowing for more natural and effective communication. Such a system constitutes a multi-modal dialog protocol where in each dialog turn, a user submits a natural language request to a retrieval agent, which then attempts to retrieve the optimal object. We formulate the retrieval task as a reinforcement learning problem, and reward the dialog system for improving the rank of the target object during each dialog turn. This framework can be applied to a variety of visual media types (images, videos, graphics, etc.), and in this paper, we study in-depth its application on the task of interactive image retrieval. To avoid the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train the dialog system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear image retrieval application. Extensive experiments on both simulated and real-world data show that: 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines; and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface. ","Xiaoxiao Guo,IBM Research;Hui Wu*,IBM Research;Yu Cheng,Microsoft AI & Research;Steven Rennie,Fusemachines;Gerald Tesauro,IBM Research;Rogerio Feris,IBM Research",xiaoxiao.guo@ibm.com;wuhu@us.ibm.com;chengyu05@gmail.com;srennie@gmail.com;gtesauro@us.ibm.com;rsferis@us.ibm.com,Applications/Computer Vision*; Applications/Information Retrieval; Applications/Natural Language Processing; Reinforcement Le,Poster
395,Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator,"In this paper, we propose a new technique named \textit{Stochastic Path-Integrated Differential EstimatoR} (SPIDER), which can be used to track many deterministic quantities of interests with significantly reduced gradient computation. Combining SPIDER with the method of normalized gradient descent, we propose two new algorithms, namely SPIDER-SFO and SPIDER-SSO, that solve non-convex stochastic optimization problems using stochastic gradients only. We prove that both SPIDER-SFO and SPIDER-SSO algorithms achieve a record-breaking $\tilde{\mathcal{O}}(\epsilon^{-3} )$ gradient computation cost to find an $\epsilon$-approximate first-order and second-order stationary point, respectively. In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting.","Cong Fang*,Peking University;Chris Junchi Li,Tencent;Zhouchen Lin,Peking University;Tong Zhang,Tecent AI Lab",fangcong@pku.edu.cn;junchi.li.duke@gmail.com;zlin@pku.edu.cn;tongzhang@tongzhang-ml.org,Optimization/Non-Convex Optimization*; Algorithms/Stochastic Methods,Spotlight
397,Are GANs Created Equal? A Large-Scale Study,"  Generative adversarial networks (GAN) are a powerful subclass of
generative models. Despite a very rich research activity leading to
numerous interesting GAN algorithms, it is still very hard to assess
which algorithm(s) perform better than others.  We conduct a neutral,
multi-faceted large-scale empirical study on state-of-the art models
and evaluation measures. We find that most models can reach similar
scores with enough hyperparameter optimization and random restarts.
This suggests that improvements can arise from a higher computational
budget and tuning more than fundamental algorithmic changes.  To
overcome some limitations of the current metrics, we also propose
several data sets on which precision and recall can be computed.  Our
experimental results suggest that future GAN research should be based
on more systematic and objective evaluation procedures. Finally, we
did not find evidence that any of the tested algorithms consistently
outperforms the non-saturating GAN introduced in \cite{goodfellow2014generative}.","Mario Lucic*,Google Brain;Karol Kurach,Google;Marcin Michalski,Google;Sylvain Gelly,Google;Olivier Bousquet,Google",lucic@google.com;kkurach@google.com;michalski@google.com;sylvaingelly@google.com;obousquet@google.com,Deep Learning/Adversarial Networks*; Applications/Computer Vision; Deep Learning/Generative Models,Poster
412,Learning Disentangled Joint Continuous and Discrete Representations,"We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.","Emilien Dupont*,Schlumberger",emil.dupo@gmail.com,Algorithms/Representation Learning*; Algorithms/Unsupervised Learning; Deep Learning/Deep Autoencoders; Deep Learning/Generative,Poster
414,TADAM: Task dependent adaptive metric for improved few-shot learning,"Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100.","Boris Oreshkin*,Element AI;Pau Rodriguez,Element AI;Alexandre Lacoste,",boris@elementai.com;pau.rodriguez@elementai.com;allac@elementai.com,Deep Learning/Few-Shot Learning Approaches*; Algorithms/Classification; Algorithms/Metric Learning; Algorithms/Representation Learning;,Poster
418,"Do Less, Get More: Streaming Submodular Maximization with Subsampling","In this paper, we develop the first one-pass streaming algorithm for submodular maximization  that does not evaluate the entire stream even once. By carefully subsampling each element of data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a $p$-matchoid constraint, our randomized algorithm achieves a $4p$ approximation ratio (in expectation) with $O(k)$ memory and $O(km/p)$ queries per element ($k$ is the size of the largest feasible solution and $m$ is the number of matroids used to define the constraint). For the non-monotone case, our approximation ratio increases only slightly to $4p+2-o(1)$.  To the best or our knowledge, our algorithm is the first that combines the benefits of streaming and subsampling in a novel way in order to truly scale submodular maximization to massive machine learning problems. To showcase its practicality, we empirically evaluated the performance of our algorithm on a video summarization application and observed that it outperforms the state-of-the-art algorithm by up to fifty fold, while maintaining practically the same utility.
We also evaluated the scalability of our algorithm on a large dataset of Uber pick up locations. ","Moran Feldman,Open University of Israel;Amin Karbasi,Yale;Ehsan Kazemi*,Yale Institute for Network Science, Yale",moranfe@openu.ac.il;amin.karbasi@yale.edu;ehsan.kazemi66@gmail.com,Optimization/Submodular Optimization*; Optimization/Combinatorial Optimization,Spotlight
425,Sparse Covariance Modeling in High Dimensions with Gaussian Processes,"This paper studies statistical relationships among elements of high-dimensional observations varying across non-random covariates. We propose to model the observation elements' changing covariances as sparse multivariate stochastic processes. In particular, our novel covariance modeling method reduces dimensionality by relating the observation vectors to a lower dimensional subspace. To characterize the changing correlations, we jointly model the latent factors and the factor loadings as collections of basis functions that vary with the covariates as Gaussian processes. Automatic relevance determination (ARD) encodes basis sparsity through their coefficients to account for the inherent redundancy. Experiments conducted across domains show superior performances to the state-of-the-art methods.","Rui Li*,Rochester Institute of Technology;Kishan KC,Rochester Institute of Technology;Feng Cui,Rochester Institute of Technology;Justin Domke,UMass Amherst;Anne Haake,Rochester Institute of Technology",lr8032@gmail.com;kk3671@rit.edu;fxcsbi@rit.edu;domke@cs.umass.edu;arhics@rit.edu,Probabilistic Methods/Gaussian Processes*; Algorithms/Stochastic Methods; Applications/Computational Biology and Bioinformatics; Applicat,Spotlight
426,Deep Neural Nets with Interpolating Function as Output Activation,"We replace the output layer of deep neural nets, typically the softmax function, by a novel interpolating function. And we propose end-to-end training and testing algorithms for this new architecture. Compared to classical neural nets with softmax function as output activation, the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. The new framework demonstrates the following major advantages: First, it is better applicable to the case with insufficient training data. Second, it significantly improves the generalization accuracy on a wide variety of networks. The algorithm is implemented in PyTorch, and code will  be made publicly available.","Bao Wang*,UCLA;Xiyang Luo,UCLA;Zhen Li,Hong Kong University of Science & Technology;Wei Zhu,Duke University;Zuoqiang  Shi,zqshi@mail.tsinghua.edu.cn;Stanley Osher,UCLA",wangbaonj@gmail.com;xylmath@gmail.com;mazli@ust.hk;zhu@math.duke.edu;zqshi@mail.tsinghua.edu.cn;sjo@math.ucla.edu,Deep Learning*; Deep Learning/CNN Architectures,Poster
429,FishNet: the Beauty of Feature Preservation and Refinement,"The basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels, e.g., image-level, region-level, and pixel-level, are diverging. Generally, network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation, but there is seldom network structure designed under the consideration of unifying the advantages of networks designed for pixel-level or region-level predicting tasks, which may require features with high-resolution to be preserved and more semantically descriptive. Towards this goal, we design a fish-like network, called FishNet. In FishNet, the information of all resolutions is preserved and refined for the final task. Besides, we observe that existing works still cannot \emph{directly} propagate the gradient information from deep layer to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate that the remarkable performance of the FishNet. In particular, on ImageNet-1k, the  accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. When used as the backbone network for state-of-the-art approaches on the challenging dataset MS COCO, the FishNet improves the absolute AP by 2.8% for object detection and 2.3% for instance segmentation without bells or whistles. The code of our network for both training and testing will be available online upon acceptance.","Shuyang Sun*,The University of Sydney;Jiangmiao Pang,Zhejiang University;Jianping Shi,Sensetime Group Limited;Shuai Yi,SenseTime Group Limited;Wanli Ouyang,The University of Sydney",shuyang.sun@sydney.edu.au;pjm@zju.edu.cn;shijianping@sensetime.com;yishuai@sensetime.com;wanli.ouyang@sydney.edu.au,Deep Learning/CNN Architectures*; Deep Learning; Deep Learning/Supervised Deep Networks,Poster
431,Visual Memory for Robust Path Following,"Humans routinely retrace a path in a novel environment both forwards and backwards despite uncertainty in their motion. In this paper, we present an approach for doing so. Given a demonstration of a path, a first network generates an abstraction of the path. Equipped with this abstraction, a second network then observes the world and decides how to act in order to retrace the path under noisy actuation and a changing environment. The two networks are optimized end-to-end at training time. We evaluate the method in two realistic simulators, performing path following both forwards and backwards. Our experiments show that our approach outperforms both a classical approach to solving this task as well as a number of other baselines.","Ashish Kumar,UC Berkeley;Saurabh Gupta*,UC Berkeley;David Fouhey,UC Berkeley;Sergey Levine,UC Berkeley;Jitendra Malik,University of California at Berkley",ashish_kumar@berkeley.edu;sgupta@eecs.berkeley.edu;dfouhey@berkeley.edu;svlevine@eecs.berkeley.edu;malik@eecs.berkeley.edu,Applications/Computer Vision,Oral
433,KDGAN: Distilling Knowledge with Adversarial Networks,"Knowledge distillation (KD) aims to train a lightweight classifier suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs, the classifier is trained by a teacher, i.e., a high-capacity model whose training may be resource-hungry. The accuracy of the classifier trained this way is usually suboptimal because it is difficult to learn the true data distribution from the teacher. An alternative method is to adversarially train the classifier against a discriminator in a two-player game akin to generative adversarial networks (GAN), which can ensure the classifier to learn the true data distribution at the equilibrium of this game. However, it may take excessively long time for such a two-player game to reach equilibrium due to high-variance gradient updates. To address these limitations, we propose a three-player game named KDGAN consisting of a classifier, a teacher, and a discriminator. The classifier and the teacher learn from each other via distillation losses and are adversarially trained against the discriminator via adversarial losses. By simultaneously optimizing the distillation and adversarial losses, the classifier will learn the true data distribution at the equilibrium. We approximate the discrete distribution learned by the classifier (or the teacher) with a concrete distribution. From the concrete distribution, we generate continuous samples to obtain low-variance gradient updates, which speed up the training. Extensive experiments using real datasets confirm the superiority of KDGAN in both accuracy and training speed.","Xiaojie Wang*,The University of Melbourne;Rui Zhang,"" University of Melbourne, Australia"";Yu Sun,Twitter Inc.;Jianzhong Qi,The University of Melbourne",xiaojiew1@student.unimelb.edu.au;rui.zhang@unimelb.edu.au;ysun@twitter.com;jianzhong.qi@unimelb.edu.au,Applications/Recommender Systems*; Algorithms/Classification,Poster
442,Long short-term memory and Learning-to-learn in networks of spiking neurons,"The brain carries out demanding computations and learning processes with recurrent networks of spiking neurons (RSNNs). But computing and learning capabilities of currently available RSNN models have remained poor, especially in comparison with the performance of recurrent networks of artificial neurons, such as Long Short-Term Memory (LSTM) networks. In this article, we investigate whether deep learning can improve RSNN performance. We applied backpropagation through time (BPTT), augmented by biologically inspired heuristics for synaptic rewiring, to RSNNs whose inherent time constants were enriched through simple models for adapting spiking neurons. We found that the resulting RSNNs approximate, for the first time, the computational power of LSTM networks on two common benchmark tasks. Furthermore, our results show that recent successes with applications of Learning-to-Learn (L2L) to LSTM networks can be ported to RSNNs. This opens the door to the investigation of L2L in data-based models for neural networks of the brain, whose activity can -- unlike that of LSTM networks -- be compared directly with recordings from neurons in the brain. In particular, L2L shows that RSNNs can learn large families of non-linear transformations from very few examples, using previously unknown network learning mechanisms. Furthermore, meta-reinforcement learning (meta-RL) shows that LSNNs can learn and execute complex exploration and exploitation strategies.","Guillaume Bellec*,Graz University of Technology;Darjan Salaj,Graz University of Technology;Anand Subramoney,Graz University of Technology;Robert  Legenstein,Graz University of Technology;Wolfgang Maass,Graz University of Technology",bellec@igi.tugraz.at;salaj@igi.tugraz.at;subramoney@igi.tugraz.at;legi@igi.tugraz.at;maass@igi.tugraz.at,Neuroscience and Cognitive Science*; Deep Learning/Meta-Learning; Deep Learning/Recurrent Networks; Neuroscience and Cognitive Scie,Poster
443,Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in CNN,"To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.","Shupeng Su,Peking University;Chao Zhang*,Peking University;Kai Han,Huawei Noah's Ark Lab;Yonghong Tian,PKU",sushupeng18@gmail.com;c.zhang@pku.edu.cn;hankai@pku.edu.cn;yhtian@pku.edu.cn,Applications/Computer Vision*; Applications/Information Retrieval; Deep Learning/Supervised Deep Networks,Poster
444,Informative Features for Model Comparison,"Given two candidate models, and a set of target observations, we address the problem of measuring the relative goodness of fit of the two models. We propose two new statistical tests which are nonparametric, computationally efficient (runtime complexity is linear in the sample size), and interpretable. As a unique advantage, our tests can produce a set of examples (informative features) indicating the regions in the data domain where one model fits significantly better than the other. In a real-world problem of comparing GAN models, the test power of our new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.","Wittawat Jitkrittum*,Max Planck Institute for Intelligent Systems;Heishiro Kanagawa,Gatsby Computational Neuroscience Unit, University College London;Patsorn Sangkloy,Georgia Institute of Technology;James Hays,Georgia Institute of Technology, USA;Bernhard Sch√∂lkopf,Max Planck Institute for Intelligent Systems;Arthur Gretton,Gatsby Computational Neuroscience Unit",wittawatj@gmail.com;heishiro.kanagawa@gmail.com;patsorn_sangkloy@gatech.edu;hays@gatech.edu;bernhard.schoelkopf@tuebingen.mpg.de;arthur.gretton@gmail.com,Algorithms/Kernel Methods*; Algorithms/Model Selection and Structure Learning; Deep Learning/Generative Models; Theory ->,Poster
450,PointCNN,"We present a simple and general framework for feature learning from point cloud. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point cloud are irregular and unordered, thus a direct convolving of kernels against the features associated with the points will result in deserting the shape information while being variant to the orders. To address these problems, we propose to learn a X-transformation from the input points, which is used for simultaneously weighting the input features associated with the points and permuting them into latent potentially canonical order. Then element-wise product and sum operations of typical convolution operator are applied on the X-transformed features. The proposed method is a generalization of typical CNNs into learning features from point cloud, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.","Yangyan Li*,Shandong University;Rui Bu,Shandong University;Mingchao Sun,Shandong University;Wei Wu,Shandong University;XINHAN DI,Deepearthgo;Baoquan Chen,Shandong University",yangyan.lee@gmail.com;burui11087@gmail.com;sun.mc@outlook.com;weiwucg@gmail.com;deepearthgo@gmail.com;baoquan.chen@gmail.com,Deep Learning/CNN Architectures*; Applications/Computer Vision; Applications/Object Recognition,Poster
454,Connectionist Temporal Classification with Maximum Entropy Regularization,"Connectionist Temporal Classification (CTC) is an objective function for end-to-end sequence learning, which adopts dynamic programming algorithms to directly learn the mapping between sequences. CTC has shown promising results in many sequence learning applications including speech recognition and scene text recognition. However, CTC tends to produce highly peaky and overconfident distributions, which is a symptom of overfitting. To remedy this, we propose a regularization method based on maximum conditional entropy which penalizes peaky distributions and encourages exploration. We also introduce an entropy-based pruning method to dramatically reduce the number of CTC feasible paths by ruling out unreasonable alignments. Experiments on scene text recognition show that our proposed methods consistently improve over the CTC baseline without the need to adjust training settings. The code will be made publicly available upon the acceptance of the paper.","Hu Liu,Tsinghua University;Sheng Jin,Tsinghua University;Changshui Zhang*,Tsinghua University",liuhu15@mails.tsinghua.edu.cn;js17@mails.tsinghua.edu.cn;zcs@mail.tsinghua.edu.cn,Theory/Regularization*; Deep Learning/Recurrent Networks,Spotlight
457,Large Margin Deep Networks for Classification,"We present a formulation of deep learning that aims at  producing a large margin classifier. The notion of \emc{margin}, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classification and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer. Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any $l_p$ norm ($p \geq 1$) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classification loss functions. Specifically, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks: generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques (weight decay, dropout, and batch norm).","Gamaleldin Elsayed,Google Brain;Dilip Krishnan*,-;Hossein Mobahi,Google Research;Kevin Regan,Google;Samy Bengio,Google Brain",gamaleldin.elsayed@gmail.com;dilipkay@google.com;hmobahi@google.com;kevinregan@google.com;bengio@google.com,Algorithms/Large Margin Methods*; Algorithms/Classification,Poster
462,Generalizing Graph Matching beyond Quadratic Assignment Model,"Graph matching has received persistent attention over decades, which can be formulated as a quadratic assignment problem (QAP). We show that a large family of functions, which we define as Separable Functions, can approximate discrete graph matching in the continuous domain asymptotically by varying the approximation controlling parameters. We also study the properties of global optimality and devise convex/concave-preserving extensions to the widely used Lawler's QAP form. Our theoretical findings show the potential for deriving new algorithms and techniques for graph matching. We deliver solvers based on two specific instances of Separable Functions, and the state-of-the-art performance of our method is verified on popular benchmarks.","Tianshu Yu*,Arizona State University;Junchi Yan,Shanghai Jiao Tong University;Yilin Wang,Adobe;Wei Liu,Tencent AI Lab;baoxin Li,Arizona State University",tianshuy@asu.edu;yanjunchi@sjtu.edu.cn;yilwang@adobe.com;wl2223@columbia.edu;baoxin.li@asu.edu,Applications/Computer Vision*; Optimization/Combinatorial Optimization,Poster
466,Solving Large Sequential Games with the Excessive Gap Technique,"There has been tremendous recent progress on equilibrium-finding algorithms for zero-sum imperfect-information extensive-form games, but there has been a puzzling gap between theory and practice. \emph{First-order methods} have significantly better theoretical convergence rates than any \emph{counterfactual-regret minimization (CFR)} variant. Despite this, CFR variants have been favored in practice. Experiments with first-order methods have only been conducted on small- and medium-sized games because those methods are complicated to implement in this setting, and because CFR variants have been enhanced extensively for over a decade they perform well in practice. In this paper we show that a particular first-order method, a state-of-the-art variant of the \emph{excessive gap technique}---instantiated with the \emph{dilated   entropy distance function}---can efficiently solve large real-world problems competitively with CFR and its variants. We show this on large endgames encountered by the \emph{Libratus} poker AI, which recently beat top human poker specialist professionals at no-limit Texas hold'em. We show experimental results on our variant of the excessive gap technique as well as a prior version. We introduce a numerically friendly implementation of the smoothed best response computation associated with first-order methods for extensive-form game solving. We present, to our knowledge, the first GPU implementation of a first-order method for extensive-form games. We present comparisons of several excessive gap technique and CFR variants.","Christian Kroer*,Carnegie Mellon University;Gabriele Farina,Carnegie Mellon University;Tuomas Sandholm,Carnegie Mellon University",ckroer@cs.cmu.edu;gfarina@cs.cmu.edu;sandholm@cs.cmu.edu,Theory/Game Theory and Computational Economics,Spotlight
492,Discrimination-aware Channel Pruning for Deep Neural Networks,"Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either (i) train from scratch with sparsity constraints on channels, or (ii) minimize the reconstruction error between the pre-trained feature maps and the compressed ones. Both strategies suffer from limitations: the former kind is computationally expensive and difficult to converge, whilst the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. To overcome these drawbacks, we investigate a simple-yet-effective method, named discrimination-aware channel pruning (DCP), which seeks to select those channels that really contribute to discriminative power. To this end, we introduce additional losses into the network to increase the discriminative power of intermediate layers. We then propose to select the most discriminative channels for each layer, where both an additional loss and the reconstruction error are considered. Last, we propose a greedy algorithm to make channel selection and parameter optimization in an iterative way. Extensive experiments demonstrate the effectiveness of our method. For example, on ILSVRC-12, our pruned ResNet-50 with 30% reduction of channels even outperforms the original model by 0.39% in top-1 accuracy.","Zhuangwei Zhuang,South China University of Technology;Mingkui Tan*,South China University of Technology;Bohan Zhuang,The University of Adelaide;Jing Liu,South China University of Technology;Yong  Guo,South China University of Technology;Qingyao Wu,South China University of Technology;Junzhou Huang,University of Texas at Arlington;Jinhui Zhu,South China University of Technology",z.zhuangwei@mail.scut.edu.cn;mingkuitan@scut.edu.cn;bohan.zhuang@adelaide.edu.au;seliujing@mail.scut.edu.cn;guo.yong@mail.scut.edu.cn;qyw@scut.edu.cn;jzhuang@uta.edu;csjhzhu@scut.edu.cn,Deep Learning/Efficient Inference Methods*; Applications/Computer Vision; Deep Learning/CNN Architectures,Poster
496,On Word Embedding Dimensionality,"In this paper, we analyze the effect of dimensionality on word embeddings. Motivated by the unitary-invariance observation of word embedding, we propose the Pairwise Inner Product (PIP) loss, a novel metric for word embedding similarity. By connecting the embedding training process to noisy matrix factorization, we reveal a fundamental bias-variance trade-off in dimensionality selection. This bias-variance trade-off sheds light on many empirical observations which were not previously explained, for example the existence of an optimal dimensionality. Moreover, new insights and discoveries, like when and how word embeddings are robust to over-parametrization, are revealed. By optimizing over the bias-variance trade-off of the PIP loss, we can explicitly answer the open question of dimensionality selection for word embedding.","Zi Yin*,n.a.;Yuanyuan Shen,Stanford University",s0960974@gmail.com;yyshen@stanford.edu,Deep Learning/Embedding Approaches*; Applications/Matrix and Tensor Factorization; Applications/Natural Language Processing,Oral
500,Reinforced Continual Learning,"Most artificial intelligence models   have  limitation to solve new tasks without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed,  which  searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies.  We name it as Reinforced Continual Learning (RCL). Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives with deep networks.","Ju Xu,Peking University;Zhanxing Zhu*,Peking University",xuju@pku.edu.cn;zhanxing.zhu@pku.edu.cn,Deep Learning*; Reinforcement Learning and Planning/Reinforcement Learning,Poster
503,Uncertainty-Aware Attention for Reliable Interpretation and Prediction,"Attention mechanism is effective in both focusing the deep learning models on relevant features and interpreting them. However, attentions may be unreliable since the networks that generate them are often trained in a weakly-supervised manner. To overcome this limitation, we introduce the notion of input-dependent uncertainty to the attention mechanism, such that it generates attention for each feature with varying degrees of noise based on the given input, to learn larger variance on instances it is uncertain about. We learn this Uncertainty-aware Attention (UA) mechanism using variational inference, and validate it on various risk prediction tasks from electronic health records on which our model significantly outperforms existing attention models. The analysis of the learned attentions shows that our model generates attentions that comply with clinicians' interpretation, and provide richer interpretation via learned variance. Further evaluation of both the accuracy of the uncertainty calibration and the prediction performance with ""I don't know'' decision show that UA yields networks with high reliability as well.","Jay Heo,UNIST, AItrics;Hae Beom Lee,UNIST;Saehoon Kim,AITRICS;Juho Lee,University of Oxford;Kwang Joon Kim,Yonsei University College of Medicine;Eunho Yang,KAIST;Sung Ju Hwang*,KAIST",jayheo7@gmail.com;hblee@unist.ac.kr;shkim@aitrics.com;juho.lee@stats.ox.ac.uk;preppie@yuhs.ac;yangeh@gmail.com;sjhwang82@kaist.ac.kr,Deep Learning/Attention Models*; Applications/Time Series Analysis; Deep Learning/Recurrent Networks; Probabilistic Methods ->,Poster
508,DropMax: Adaptive Variational Softmax,"We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are input-adaptively learned via variational inference. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries. Moreover, the learning of dropout rates for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains significantly improved accuracy over the regular softmax classifier and other baselines. Further analysis of the learned dropout probabilities shows that our model indeed selects confusing classes more often when it performs classification.","Hae Beom Lee,UNIST;Juho Lee,University of Oxford;Saehoon Kim,AITRICS;Eunho Yang,KAIST;Sung Ju Hwang*,KAIST",hblee@unist.ac.kr;juho.lee@stats.ox.ac.uk;shkim@aitrics.com;yangeh@gmail.com;sjhwang82@kaist.ac.kr,Deep Learning/Supervised Deep Networks*; Applications/Object Recognition; Deep Learning/CNN Architectures; Probabilistic Methods/Var,Poster
514,Posterior Concentration for Sparse Deep Learning," We introduce Spike-and-Slab Deep Learning (SS-DL), a fully Bayesian  alternative to dropout for improving generalizability of deep ReLU networks. This new type of regularization enables  provable recovery of smooth input-output maps with {\sl unknown} levels of smoothness. Indeed, we  show that  the posterior distribution concentrates at the near minimax rate for alpha-Holder smooth maps, performing as well as if we knew the smoothness level alpha ahead of time. Our result sheds light on architecture design for deep neural networks, namely the choice of depth, width and sparsity level. These network attributes typically depend on  unknown smoothness  in order to be optimal. We obviate this constraint with the fully Bayes construction. As an aside, we show that SS-DL does not overfit in the sense that the posterior concentrates on smaller networks with fewer (up to the  optimal number of) nodes and links. Our results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view.","Veronika Rockova*,University of Chicago;nicholas polson,Chicago Booth",Veronika.Rockova@chicagobooth.edu;ngp@chicagobooth.edu,Probabilistic Methods/Bayesian Theory*; Deep Learning/Supervised Deep Networks; Probabilistic Methods/Hierarchical Models,Poster
522,A flexible model for training action localization with varying levels of supervision,"Spatio-temporal action detection in videos is typically addressed in a fully-supervised setup with manual annotation of training videos required at every frame.  Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels over temporal points or sparse action bounding boxes to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos.","Guilhem Cheron*,INRIA;Jean-Baptiste ALAYRAC,Inria;Ivan Laptev,INRIA Paris;Cordelia Schmid,INRIA",guilhem.cheron@inria.fr;jean-baptiste.alayrac@inria.fr;ivan.laptev@inria.fr;cordelia.schmid@inria.fr,Applications/Activity and Event Recognition*; Algorithms/Semi-Supervised Learning; Applications/Computer Vision; Applications/Video Analy,Poster
526,A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents,"In multiagent domains, coping with non-stationary agents that change behaviors constantly is a challenging problem, where an agent is usually required to be able to quickly detect the other agent's policy during online interaction, and then adapt its own policy accordingly. This paper studies efficient policy detecting and reusing techniques when playing against non-stationary agents in Markov games. We propose a new deep BPR+ algorithm by extending the recent BPR+ algorithm with a neural network as the value-function approximator. To detect policy accurately, we propose the \textit{rectified belief model} taking advantage of the \textit{opponent model} to infer the other agent's policy from reward signals and its behaviors. Instead of directly storing individual policies as BPR+, we introduce \textit{distilled policy network} that serves as the policy library in BPR+, using policy distillation to achieve efficient online policy learning and reuse. Deep BPR+ inherits all the advantages of BPR+ and empirically shows better performance in terms of detection accuracy, cumulative rewards and speed of convergence compared to existing algorithms in complex Markov games with raw visual inputs.","yan zheng*,tianjin university;Zhaopeng Meng,School of Computer Software, Tianjin University;Jianye Hao,Tianjin University;Zongzhang Zhang,Soochow University;Tianpei Yang,Tianjin University;Changjie Fan,Netease",yanzheng@tju.edu.cn;mengzp@tju.edu.cn;jianye.hao@tju.edu.cn;zzzhang@suda.edu.cn;tpyang@tju.edu.cn;fanchangjie@netease.com,Reinforcement Learning and Planning/Multi-Agent RL,Poster
527,Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited,"In this paper, we revisit the Empirical Risk Minimization problem in the non-interactive local model of differential privacy. In the case of constant or low dimensions ($p\ll n$), we first show that if the  loss function is $(\infty, T)$-smooth,  we can avoid a dependence of the  sample complexity, to achieve error $\alpha$, on the exponential of the dimensionality $p$ with base $1/\alpha$  ({\em i.e.,} $\alpha^{-p}$),
 which answers a question in \cite{smith2017interaction}.  Our approach is based on polynomial approximation. Then, we propose player-efficient algorithms with $1$-bit communication complexity and $O(1)$ computation cost for each player. The error bound is asymptotically the same as the original one. With some additional assumptions, we also give an efficient algorithm for the server. 
 In the case of high dimensions ($n\ll p$),

 we show that if the loss function is a convex generalized linear function,  the error  can be bounded by using the Gaussian width of the constrained set, instead of $p$, which improves the one in    
  \cite{smith2017interaction}.
  Our techniques can be extended to some related problems, such as $k$-way marginal queries and smooth queries. ","Di Wang*,SUNY at Buffalo;Marco Gaboardi,Univeristy at Buffalo;Jinhui Xu,SUNY Buffalo",dwang45@buffalo.edu;gaboardi@buffalo.edu;jinhui@buffalo.edu,"Applications/Privacy, Anonymity, and Security",Poster
543,Low-shot Learning via Covariance-Preserving Adversarial Augmentation Network,"Feature augmentation attacks low-shot learning problem by easing the catastrophic forgetting and over-fitting issues in low data regime. However, previous works either erroneously assume that any intra-class variances of base classes are sharable with any novel classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adversarial Augmentation Network for low-shot learning to overcome existing limits. We regulate generation within base-novel class pairs that are semantically adaptable. For better feature generation quality, a new Generative Adversarial Network is designed to model the latent distribution of each novel class given its base counterparts. Since direct estimation on novel classes can be inductively biased, we explicitly preserve covariance information as the ``variability'' of base examples during generation process. Empirical evidences show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state-of-the-art.","Hang Gao*,Columbia University;Zheng Shou,Columbia University;Alireza Zareian,Columbia University;Hanwang Zhang,Nanyang Technological University;Shih-Fu Chang,Columbia University",hang.gao@columbia.edu;zs2262@columbia.edu;alireza@cs.columbia.edu;hanwangzhang@ntu.edu.sg;sc250@columbia.edu,Deep Learning/Few-Shot Learning Approaches*; Deep Learning/Adversarial Networks; Deep Learning/Generative Models,Poster
544,Learning semantic similarity in a continuous space,"We address the problem of learning semantic representation of questions to measure similarity between pairs as a continuous distance metric. Our work naturally extends Word Mover‚Äôs Distance (WMD) [1] by representing text documents as normal distributions instead of bags of embedded words. Our learned metric measures the dissimilarity between two questions as the minimum amount of distance the intent (hidden representation) of one question needs to ""travel"" to match the intent of another question. We first learn to repeat, reformulate questions to infer intents as normal distributions with a deep generative model [2] (variational auto encoder). Semantic similarity between pairs is then learned discriminatively as an optimal transport distance metric (Wasserstein 2) with our novel variational siamese framework. Among known models that can read sentences individually, our proposed framework achieves competitive results on Quora duplicate questions dataset. Our work sheds light on how deep generative models can approximate distributions (semantic representations) to effectively measure semantic similarity with meaningful distance metrics from Information Theory.","Michel Deudon*,Ecole Polytechnique",michel.deudon@polytechnique.edu,Algorithms/Similarity and Distance Learning*; Algorithms/Representation Learning; Algorithms/Semi-Supervised Learning; Applications/Natur,Poster
545,MetaReg: Towards Domain Generalization using Meta-Regularization,"Training models that generalize to unseen domains at test time is a problem of
fundamental importance in machine learning. In this work, we propose using
regularization to capture this notion of domain generalization. We pose the problem
of finding such a regularization function in a Learning to Learn (or) Meta Learning
framework. The notion of domain generalization is explicitly captured by learning a
regularizer that makes the model trained on one domain to perform well on another
domain. Experimental validations on computer vision and natural language datasets
indicate that our method can learn regularizers that achieve good cross-domain
generalization.","Yogesh Balaji*,University of Maryland, College Park;Swami Sankaranarayanan,University of Maryland, College Park;Rama Chellappa,University of Maryland",yogesh@cs.umd.edu;swamiviv1@gmail.com;Rama@umiacs.umd.edu,Algorithms/Multitask and Transfer Learning*; Algorithms/Representation Learning; Applications/Computer Vision; Deep Learning; Deep Learning,Poster
548,Boosted Sparse and Low-Rank Tensor Regression,"We propose a sparse and low-rank tensor regression model to relate a univariate outcome to a feature tensor, in which each unit-rank tensor from the CP decomposition of the coefficient tensor is assumed to be sparse. This structure is both parsimonious and highly interpretable, as it implies that the outcome is related to the features through a few distinct pathways, each of which may only involve subsets of feature dimensions. We take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor regression problems. To make the computation efficient and scalable, for the unit-rank tensor regression, we propose a stagewise estimation procedure to efficiently trace out its entire solution path. We show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression. The superior performance of our approach is demonstrated on various real-world and synthetic examples.","Lifang He*,Cornell University;Kun Chen,University of Connecticut;Wanwan Xu,University of Connecticut;Jiayu Zhou,Michigan State University;Fei Wang,Cornell University",lifanghescut@gmail.com;kun.chen@uconn.edu;wanwan.xu@uconn.edu;dearjiayu@gmail.com;few2001@med.cornell.edu,Algorithms/Regression*; Applications/Matrix and Tensor Factorization; Optimization/Non-Convex Optimization,Poster
549,Domain-Invariant Projection Learning for Zero-Shot Recognition,"Zero-shot learning (ZSL) aims to recognize unseen object classes without any training samples, which can be regarded as a form of transfer learning from seen classes to unseen ones. This is made possible by learning a projection between a feature space and a semantic space (e.g. attribute space). Key to ZSL is thus to learn a projection function that is robust against the often large domain gap between the seen and unseen classes. In this paper, we propose a novel ZSL model termed domain-invariant projection learning (DIPL). Our model has two novel components: (1) A domain-invariant feature self-reconstruction task is introduced to the seen/unseen class data, resulting in a simple linear formulation that casts ZSL into a min-min optimization problem. Solving the problem is non-trivial, and a novel iterative algorithm is formulated as the solver, with rigorous theoretic algorithm analysis provided. (2) To further align the two domains via the learned projection, shared semantic structure among seen and unseen classes is explored via forming superclasses in the semantic space. Extensive experiments show that our model outperforms the state-of-the-art alternatives by significant margins.","An Zhao,Renmin University of China;Mingyu Ding,Renmin University of China;Jiechao Guan,Renmin University of China;Zhiwu Lu*,Renmin University of China;Tao Xiang,Queen Mary, University of London, UK;Ji-Rong Wen,Renmin University of China",zhaoan_ruc@163.com;d130143597@163.com;guanjiechao0660@163.com;luzhiwu@ruc.edu.cn;t.xiang@qmul.ac.uk;jrwen@ruc.edu.cn,Algorithms/Unsupervised Learning*; Algorithms/Regression,Poster
554,Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding,"We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our visual question answering (VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three advantages. First, executing programs on a symbolic space is more robust to long program traces. Our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it learns to perform well on a small number of training data; it can also encode an image into a compact representation and answer questions offline, using only 1% of the storage needed by the best competing methods.
Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step. Our model recovers the ground truth programs precisely.","Kexin Yi,Harvard University;Jiajun Wu*,MIT;Chuang Gan,MIT;Antonio Torralba,MIT;Pushmeet Kohli,DeepMind;Joshua Tenenbaum,MIT",kyi@g.harvard.edu;jiajunwu.cs@gmail.com;ganchuang1990@gmail.com;torralba@mit.edu;pushmeet@google.com;jbt@mit.edu,Neuroscience and Cognitive Science/Reasoning*; Applications/Visual Scene Analysis and Interpretation; Deep Learning/Program Induction; Neuros,Spotlight
560,Frequency-Domain Dynamic Pruning for Convolutional Neural Networks,"Deep convolutional neural networks have demonstrated their powerfulness in a variety of applications. However, the storage and computational requirements have largely restricted their further extensions on mobile devices. Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin. Specifically, it can achieve a compression ratio of 8x and an inference speed-up of 8.9x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-10.","Zhenhua Liu*,Peking University;Jizheng Xu,Bytedance Inc.;Xiulian Peng,Microsoft Research;Ruiqin Xiong,Peking University",liu-zh@pku.edu.cn;jzxu@live.cn;xipe@microsoft.com;rqxiong@pku.edu.cn,Deep Learning/Efficient Inference Methods*; Deep Learning/CNN Architectures,Poster
561,Quadratic Decomposable Submodular Function Minimization,"We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization. The problem is closely related to decomposable submodular function minimization and arises in many learning on graphs and hypergraphs settings, such as graph-based semi-supervised learning and PageRank. We approach the problem via a new dual strategy and describe an objective that may be optimized via random coordinate descent (RCD) methods and projections onto cones. We also establish the linear convergence rate of the RCD algorithm and develop efficient projection algorithms with provable performance guarantees. Numerical experiments in semi-supervised learning on hypergraphs confirm the efficiency of the proposed algorithm and demonstrate the significant improvements in prediction accuracy with respect to state-of-the-art methods.","Pan Li*,UIUC;Niao He,University of Illinois at Urbana-Champaign;Olgica Milenkovic,University of Illinois UC",panli2@illinois.edu;niaohe@illinois.edu;milenkov@illinois.edu,Optimization/Submodular Optimization*; Algorithms/Semi-Supervised Learning; Optimization/Convex Optimization,Poster
562,A Block Coordinate Ascent Algorithm for Mean-Variance Optimization,"Risk management in dynamic decision problems is a primary concern in many fields, including financial investment, autonomous driving, and healthcare. The mean-variance function is one of the most widely used objective functions in risk management due to its simplicity and interpretability. Existing algorithms for mean-variance optimization are based on multi-time-scale stochastic approximation, whose learning rate schedules are often hard to tune, and have only asymptotic convergence proof. In this paper, we develop a model-free policy search framework for mean-variance optimization with finite-sample error bound analysis (to local optima). Our starting point is a reformulation of the original mean-variance function with its Fenchel dual, from which we propose a stochastic block coordinate ascent policy search algorithm. Both the asymptotic convergence guarantee of the last iteration's solution and the convergence rate of the randomly picked solution are provided, and their applicability is demonstrated on several benchmark domains.","Tengyang Xie,University of Massachusetts Amherst;Bo Liu*,Auburn University;Yangyang Xu,;Mohammad Ghavamzadeh,FAIR;Yinlam Chow,DeepMind;Daoming Lyu,Auburn University;Daesub Yoon,ETRI",txie@cs.umass.edu;boliu@auburn.edu;xuy21@rpi.edu;mohammad.ghavamzadeh@inria.fr;yinlamchow@google.com;dzl0053@tigermail.auburn.edu;eyetracker@etri.re.kr,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
569,$\ell_1$-regression with Heavy-tailed Distributions,"In this paper, we consider the problem of linear regression with heavy-tailed distributions. Different from previous studies that use the squared loss to measure the performance, we choose the absolute loss, which is more robust in the presence of large prediction errors. To address the challenge that both the input and output could be heavy-tailed, we propose a truncated minimization problem, and demonstrate that it enjoys an $O(\sqrt{d/n})$ excess risk, where $d$ is the dimensionality and $n$ is the number of samples. Compared with traditional work on $\ell_1$-regression, the main advantage of our result is that we achieve a high-probability risk bound without exponential moment conditions on the input and output. Furthermore, if the input is bounded, we show that the classical empirical risk minimization is competent for $\ell_1$-regression even when the output is heavy-tailed.","Lijun Zhang*,Nanjing University;Zhi-Hua  Zhou,Nanjing University",zljzju@gmail.com;zhouzh@nju.edu.cn,Theory/Learning Theory*; Algorithms/Regression; Algorithms/Stochastic Methods,Poster
574,Neural Nearest Neighbors Networks,"Non-local methods exploiting the self-similarity of natural signals have been well studied, for example in image analysis and restoration. Existing approaches, however, rely on k-nearest neighbor (KNN) matching in a fixed feature space. The main hurdle in optimizing this feature space w.r.t. application performance is the non-differentiability of the KNN selection rule. To overcome this, we propose a continuous deterministic relaxation of the KNN selection that maintains differentiability w.r.t. pairwise distances, but retains the original KNN as the limit of a temperature parameter approaching zero. To exploit our relaxation, we propose the neural nearest neighbors block (N3 block), a novel non-local processing layer that leverages the principle of self-similarity and can be used as building block in modern neural network architectures. We show its effectiveness for the set reasoning task of correspondence classification and for image restoration tasks of image denoising and single image super-resolution, where we outperform strong convolutional neural network (CNN) baselines and recent non-local models that rely on KNN selection in hand-chosen features spaces.","Tobias Ploetz*,TU Darmstadt;Stefan Roth,TU Darmstadt",tobias.ploetz@visinf.tu-darmstadt.de;stefan.roth@visinf.tu-darmstadt.de,Applications/Denoising*; Applications/Computer Vision; Deep Learning/Attention Models; Deep Learning/CNN Architectur,Poster
576,Efficient nonmyopic batch active search,"Active search is a learning paradigm for actively identifying as many members of a given class as possible. Important applications include drug discovery, fraud detection, and product recommendation. All existing work focuses on sequential policies, i.e., selecting one point to query at a time. However, in many real applications, it is possible to evaluate multiple points simultaneously. We investigate batch active search, the first such study we know of in the literature. We first derive the Bayesian optimal policy for batch active search, and prove a lower bound on the performance gap between sequential and batch optimal policies. Then we propose novel batch policies inspired by state-of-the-art sequential policies, and develop an aggressive pruning technique that can speed up computation by up-to nearly 50 times. We conduct thorough experiments on three application domains: a citation network, material science, and drug discovery, testing all proposed policies (14 total) for a wide range of batch sizes. Results show that the empirical gap matches our theoretical bound; nonmyopic policies usually beat myopic ones significantly; we also find diversity to be an important consideration for batch policy design.","Shali Jiang*,Washington University in St. Louis;Gustavo Malkomes,Washington University in St. Louis;Matthew Abbott,Washington University in St. Louis;Benjamin Moseley,Carnegie Mellon University;Roman Garnett,-",jiang.s@wustl.edu;luizgustavo@wustl.edu;mbabbott@wustl.edu;moseleyb@andrew.cmu.edu;garnett@wustl.edu,Algorithms/Active Learning,Spotlight
585,A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers,"We introduce a game-theoretic approach to the study of recommendation systems with strategic content providers. Such systems should be fair and stable. Showing that traditional approaches fail to satisfy these requirements, we propose the Shapley mediator. We show that the Shapley mediator satisfies the fairness and stability requirements, runs in linear time, and is the only economically efficient mechanism satisfying these properties.","Omer Ben-Porat*,Technion ‚Äì Israel Institute of Technology;Moshe Tennenholtz,Technion ‚Äì Israel Institute of Technology",omerbp@gmail.com;moshet@ie.technion.ac.il,"Theory/Game Theory and Computational Economics*; Applications/Fairness, Accountability, and Transparency",Poster
597,Interactive Structure Learning with Structural Query-by-Committee,"In this work, we introduce interactive structure learning, a framework that unifies many different interactive learning tasks. We present a generalization of the query-by-committee active learning algorithm for this setting, and we study its consistency and rate of convergence, both theoretically and empirically, with and without noise.","Christopher Tosh*,Columbia University;Sanjoy Dasgupta,University of California, San Diego",ctosh@cs.ucsd.edu;dasgupta@cs.ucsd.edu,Algorithms/Active Learning*; Algorithms/Semi-Supervised Learning; Algorithms/Unsupervised Learning,Spotlight
602,Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere,"Multichannel blind deconvolution is the problem of recovering an unknown signal $f$ and multiple unknown channels $x_i$ from convolutional measurements $y_i=x_i \circledast f$ ($i=1,2,\dots,N$). We consider the case where $x_i$'s are sparse, and convolution with $f$ is invertible. Our nonconvex optimization formulation solves for a filter $h$ on the unit sphere that produces sparse output $y_i\circledast h$. Under some technical assumptions, we show that all local minima of the objective function correspond to the inverse filter of $f$ up to an inherent sign and shift ambiguity, and all saddle points have strictly negative curvatures. This geometric structure allows successful recovery of $f$ and $x_i$ using a simple manifold gradient descent algorithm with random initialization. Our theoretical findings are complemented by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods. ","Yanjun Li*,UIUC;Yoram Bresler,UIUC",yli145@illinois.edu;ybresler@illinois.edu,Algorithms/Sparsity and Compressed Sensing*; Applications/Signal Processing,Spotlight
603,Video-to-Video Synthesis,"We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel approach for the video-to-video synthesis problem under adversarial learning framework. Through the introduction of new generator and discriminator architectures, coupled with a spatial-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines.  In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, not possible before our work. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems. (Note: using Adobe Reader is highly recommended to view the paper.)","Ting-Chun Wang*,NVIDIA;Ming-Yu Liu,NVIDIA;Jun-Yan Zhu,MIT;Guilin Liu,NVIDIA;Andrew Tao,NVIDIA;Jan Kautz,NVIDIA;Bryan Catanzaro,NVIDIA",tingchunw@nvidia.com;sean.mingyu.liu@gmail.com;junyanz@mit.edu;guilinl@nvidia.com;atao@nvidia.com;jkautz@nvidia.com;bcatanzaro@nvidia.com,Deep Learning/Adversarial Networks*; Applications/Computer Vision,Poster
608,How To Make the Gradients Small Stochastically,"(This is a theory paper.)

In convex stochastic optimization, convergence rates in terms of minimizing the objective have been well-established. However, in terms of making the gradients small, the best known convergence rate was eps^{-8/3} and it was left open how to improve it.

In this paper, we improve this rate to eps^{-2}, which is optimal up to log factors.","Zeyuan Allen-Zhu*,Microsoft Research AI",zeyuan@csail.mit.edu,Algorithms/Stochastic Methods*; Algorithms/Online Learning,Poster
610,Synthesize Policies for Transfer and Adaptation across Environments and Tasks,"The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments (\scene) and tasks, probably more importantly, by learning from only sparse (\scene, \task) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from the environment and task embeddings. Notably, one of the main challenges is that the embeddings have to be learned jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on \gridworld and \thor, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (\scene, \task) pairs after learning from only 40\% of them.","Hexiang Hu*,University of Southern California;Liyu Chen,USC;Boqing Gong,Tencent AI Lab;Fei Sha,USC",frank.hexiang@gmail.com;liyuc@usc.edu;boqinggo@outlook.com;feisha@usc.edu,Algorithms/Multitask and Transfer Learning*; Algorithms/Similarity and Distance Learning; Applications/Robotics; Reinforcement Learning and,Spotlight
614,Adversarial vulnerability for any classifier,"Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations.  This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.","Alhussein Fawzi*,Google Deepmind;Hamza Fawzi,University of Cambridge;Omar Fawzi,ENS Lyon",afawzi@google.com;h.fawzi@damtp.cam.ac.uk;omar.fawzi@ens-lyon.fr,Deep Learning/Supervised Deep Networks*; Deep Learning/Generative Models,Poster
621,Evolutionary Reinforcement Learning,"Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer with high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmark tasks demonstrate that ERL significantly outperforms prior DRL and EA methods, achieving state-of-the-art performances. ","Shauharda Khadka`*,Oregon State University;Kagan Tumer,Oregon State University US",khadkas@oregonstate.edu;kagan.tumer@oregonstate.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Applications/Motor Control; Applications/Robotics; Deep Learning/Biologically Plausible Dee,Poster
624,Toddler-Inspired Visual Object Learning,"Real-world learning systems have practical limitations on the size of the training datasets that they can collect and consider. Given a fixed training size budget,
how should a system go about choosing a subset of the possible training examples
that still allows for learning accurate, generalizable models? To help address this question, we draw inspiration from a highly efficient practical learning system: the human child. Using head-mounted cameras, eye gaze trackers, and a model of foveated vision, we collected first-person (egocentric) imagery that represents a highly accurate approximation of the ""training data"" that toddlers' visual systems collect in everyday, naturalistic learning contexts. We used state-of-the-art computer vision learning models (convolutional neural networks) to help characterize the structure of this data, and found that it produces significantly better object models than egocentric data experienced by adults in exactly the same environment. By using the CNNs as a modeling tool to investigate the properties of the child data that may enable this rapid learning, we found that the data exhibits a unique combination of quality and diversity, with not only many similar large, high-quality object views but also  a greater number and diversity of rare views. This novel methodology of analyzing the visual ""training data"" used by children may not only reveal insights to improve machine learning, but also may suggest new experimental tools to better understand infant learning in developmental psychology.","Sven Bambach*,Indiana University;David Crandall,Indiana University;Linda Smith,Indiana University;Chen Yu,Indiana University",sbambach@indiana.edu;djcran@indiana.edu;smith4@indiana.edu;chenyu@indiana.edu,Neuroscience and Cognitive Science*; Deep Learning; Neuroscience and Cognitive Science/Cognitive Science; Neuroscience and Cognitive S,Poster
627,"Alternating optimization of decision trees, with application to learning sparse oblique trees","Learning a decision tree from data is a difficult optimization problem. The most widespread algorithm in practice, dating to the 1980s, is based on a greedy growth of the tree structure by recursively splitting nodes, and possibly pruning back the final tree. The parameters (decision function) of an internal node are approximately estimated by minimizing an impurity measure. We give an algorithm that, given an input tree (structure and nodes' parameters), produces a new tree with the same structure but new parameter values that provably lower or leave unchanged the misclassification error. It is based on 1) a separability condition that allows us to optimize in parallel over the nodes at the same depth in the tree, and alternate over depths; and 2) the fact that optimizing a single node becomes a smaller binary classification problem over the node's decision function. This can be applied to both axis-aligned and oblique trees and our experiments show it consistently outperforms various other algorithms while being highly scalable to large datasets and trees. Further, the same algorithm can handle a sparsity penalty, so it can learn ""sparse oblique trees"", having a structure that is a subset of the original tree and few nonzero parameters. This combines the best of axis-aligned and oblique trees: flexibility to model correlated data, low generalization error, fast inference and interpretable nodes that involve only a few features in their decision.","Miguel Carreira-Perpinan*,UC Merced;Pooya Tavallali,UC Merced",mcarreira-perpinan@ucmerced.edu;ptavallali@ucmerced.edu,Optimization/Non-Convex Optimization*; Algorithms/Classification; Algorithms/Sparsity and Compressed Sensing; Optimization/Combina,Poster
636,FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification,"Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.","Yixiao Ge*,The Chinese University of Hong Kong;Zhuowan Li,Tsinghua University;Haiyu Zhao,The Chinese University of Hong Kong;Guojun Yin,University of Science and Technology of China;Shuai Yi,The Chinese University of Hong Kong;Xiaogang Wang,Chinese University of Hong Kong, Hong Kong;Hongsheng Li,Chinese University of Hong Kong",yxge@link.cuhk.edu.hk;lizhuowan14@gmail.com;hyzhao@cuhk.edu.hk;gjyin@mail.ustc.edu.cn;syi@ee.cuhk.edu.hk;xgwang@ee.cuhk.edu.hk;hsli@ee.cuhk.edu.hk,Applications/Object Recognition*; Deep Learning/Adversarial Networks,Poster
641,New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity,"As an incremental-gradient algorithm, the hybrid stochastic gradient descent (HSGD)  enjoys  merits of both stochastic and full gradient methods for finite-sum minimization problem. However, the existing rate-of-convergence analysis for HSGD is made under with-replacement sampling (WRS) and is restricted to convex problems. It is not clear whether HSGD still carries these advantages under the common practice of without-replacement sampling (WoRS) for non-convex problems. In this paper, we affirmatively answer this open question by showing that under WoRS and for both convex and non-convex problems, it is still possible for HSGD (with constant step-size) to match full gradient descent in rate of convergence, while maintaining comparable sample-size-independent incremental first-order oracle  complexity to stochastic gradient descent. For a special class of finite-sum problems with linear prediction models, our convergence results can be further improved in some cases. Extensive numerical results confirm our theoretical affirmation and demonstrate the favorable efficiency of WoRS-based HSGD.","Pan Zhou*,NUS;Xiaotong Yuan,Nanjing University of Information Science and Technology;Jiashi Feng,NUS",pzhou@u.nus.edu;xtyuan1980@gmail.com;elefjia@nus.edu.sg,Algorithms*; Algorithms/Stochastic Methods,Poster
657,The Lingering of Gradients: How to Reuse Gradients Over Time,"Classically, the time complexity of a first-order method is estimated by its number of gradient computations. In this paper, we study a more refined complexity by taking into account the ``lingering'' of gradients: once a gradient is computed at $x_k$, the additional time to compute gradients at $x_{k+1},x_{k+2},\dots$ may be reduced.

We show how this improves the running time of gradient descent and SVRG. For instance, if the ``additional time'' scales linearly with respect to the traveled distance, then the ``convergence rate'' of gradient descent can be improved from $1/T$ to $\exp(-T^{1/3})$. 

On the empirical side, we solve a hypothetical revenue management (a.k.a. resource allocation) LP problem on the Yahoo! Front Page Today Module application with 4.6m users to $10^{-6}$ error (or $10^{-12}$ dual error) using 6 passes of the dataset.","Zeyuan Allen-Zhu*,Microsoft Research AI;Xinshang Wang,MIT;David Simchi-Levi,MIT",zeyuan@csail.mit.edu;xinshang@mit.edu;dslevi@mit.edu,Optimization/Convex Optimization*; Optimization/Combinatorial Optimization,Poster
658,Unsupervised Learning of View-invariant Action Representations,"The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.","Junnan Li*,National University of Singapore;Wong Yongkang,National University of Singapore;Qi Zhao,University of Minnesota;Mohan Kankanhalli,National University of Singapore,",e0008178@u.nus.edu;yongkang.wong@nus.edu.sg;qzhao@cs.umn.edu;mohan@comp.nus.edu.sg,Algorithms/Unsupervised Learning*; Applications/Activity and Event Recognition; Applications/Tracking and Motion in Video,Poster
662,Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making,"We draw attention to an important, yet largely overlooked aspect of evaluating fairness for automated decision making systems---namely risk and welfare considerations. Our proposed family of measures corresponds to the long-established formulations of cardinal social welfare in economics. We come to this proposal by taking the perspective of a rational, risk-averse individual who is going to be subject to algorithmic decision making and is faced with the task of choosing between several algorithmic alternatives behind a Rawlsian veil of ignorance. The convex formulation of our measures allows us to integrate them as a constraint into any convex loss minimization pipeline. Our empirical analysis reveals interesting trade-offs between our proposal and (a) prediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of individual fairness. Furthermore and perhaps most importantly, our work provides both theoretical and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes; hence presenting the first computationally feasible mechanism for bounding individual-level (un)fairness.","Hoda Heidari*,ETH Zurich;Claudio Ferrari,ETH Z√ºrich;Krishna Gummadi,MPI-SWS;Andreas Krause,ETH Z√ºrich",hheidari@inf.ethz.ch;ferraric@ethz.ch;gummadi@mpi-sws.org;krausea@ethz.ch,"Applications/Fairness, Accountability, and Transparency",Poster
670,Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks,"In most of existing deep convolutional neural networks (CNNs) for classification, global average (first-order) pooling (GAP) has become a standard module to summarize activations of the last convolution layer as final representation for prediction. Recent researches show integration of higher-order pooling (HOP) methods clearly improves performance of deep CNNs. However, both GAP and existing HOP methods assume unimodal distributions, which cannot fully capture statistics of convolutional activations, limiting representation ability of deep CNNs, especially for samples with complex contents. To overcome the above limitation, this paper proposes a global Gated Mixture of Second-order Pooling (GM-SOP) method to further improve representation ability of deep CNNs. To this end, we introduce a sparsity-constrained gating mechanism and propose a novel parametric SOP as component of mixture model. Given a bank of SOP candidates, our method can adaptively choose Top-$K (K>1)$ candidates for each input sample through the sparsity-constrained gating module, and performs weighted sum of outputs of $K$ selected candidates as representation of the sample. The proposed GM-SOP can flexibly accommodate a large number of personalized SOP candidates in an efficient way, leading to richer representations. The deep networks with our GM-SOP can be end-to-end trained, having potential to characterize complex, multi-modal distributions. The proposed methods are evaluated on two large scale image benchmarks (i.e., downsampled ImageNet-1K and Places365), and experimental results show our GM-SOP is superior to its counterparts and achieves very competitive performance.","Qilong Wang,Tianjin University; Dalian University of Technology;Zilin Gao,Dalian University of Technology;Jiangtao Xie,Dalian University of Technology;Wangmeng Zuo,Harbin Institute of Technology, China;Peihua Li*,Dalian University of Technology",qlwang@tju.edu.cn;gzl@mail.dlut.edu.cn;jiangtaoxie@mail.dlut.edu.cn;wmzuo@hit.edu.cn;Peihuali@dlut.edu.cn,Deep Learning*; Applications/Object Recognition; Deep Learning/CNN Architectures,Poster
676, Image-to-image translation for cross-domain disentanglement,"Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. The obtained model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains. We can perform cross-domain retrieval without the need of labeled data. Finally, we can perform domain-specific image transfer and interpolation. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results.","Abel Gonzalez-Garcia*,Computer Vision Center;Joost van de Weijer,Computer Vision Center;Yoshua Bengio,Universite de Montreal",agonzalez@cvc.uab.es;joost@cvc.uab.es;yoshua.umontreal@gmail.com,Algorithms/Representation Learning*; Applications/Computer Vision; Deep Learning/Adversarial Networks; Deep Learning/CNN Archite,Poster
685,Gradient Sparsification for Communication-Efficient Distributed Optimization,"Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed computational architectures. A key bottleneck is the communication overhead for exchanging information such as stochastic gradients among different workers. In this paper,  to reduce the communication cost, we propose a convex optimization formulation to minimize the coding length of stochastic gradients. The key idea is to randomly drop out coordinates of the stochastic gradient vectors and amplify the remaining coordinates appropriately to ensure the sparsified gradient to be unbiased. To solve the optimal sparsification efficiently, several simple and fast algorithms are proposed for an approximate solution, with a theoretical guarantee for sparseness.  Experiments on $\ell_2$ regularized logistic regression, support vector machines, and convolutional neural networks validate our sparsification approaches.","Jianqiao Wangni*,University of Pennsylvania, Tencent AI Lab;Jialei Wang,U Chicago;Ji  Liu,University of Rochester;Tong Zhang,Tecent AI Lab",zjnqha@gmail.com;jialei@uchicago.edu;ji.liu.uwisc@gmail.com;tongzhang@tongzhang-ml.org,Applications/Communication- or Memory-Bounded Learning*; Optimization/Convex Optimization,Poster
687,Revisiting Multi-Task Learning with ROCK: a Deep Residual Auxiliary Block for Visual Detection,"Multi-Task Learning (MTL) is appealing for deep learning regularization. In this paper, we tackle a specific MTL context denoted as primary MTL, where the ultimate goal is to improve the performance of a given primary task by leveraging several other auxiliary tasks. Our main methodological contribution is to introduce ROCK, a new generic multi-modal fusion block for deep learning tailored to the primary MTL context. ROCK architecture is based on a residual connection, which makes forward prediction explicitly impacted by the intermediate auxiliary representations. The auxiliary predictor's architecture is also specifically designed to our primary MTL context, by incorporating intensive pooling operators for maximizing complementarity of intermediate representations. Extensive experiments on NYUv2 dataset (object detection with scene classification, depth prediction, and surface normal estimation as auxiliary tasks) validate the relevance of the approach and its superiority to flat MTL approaches. Our method outperforms state-of-the-art object detection models on NYUv2 by a large margin, and is also able to handle large-scale heterogeneous inputs (real and synthetic images) and missing annotation modalities.","Taylor Mordan*,Sorbonne Universit√©, LIP6;Nicolas Thome,CNAM, Paris;Gilles Henaff,Thales Optronique S.A.S.;Matthieu Cord,Sorbonne University",taylor.mordan@gmail.com;nicolas.thome@cnam.fr;gilles.henaff@fr.thalesgroup.com;matthieu.cord@lip6.fr,Applications/Object Detection*; Algorithms/Multitask and Transfer Learning; Applications/Computer Vision; Deep Learning/CNN,Poster
691,Adaptive Online Learning in Dynamic Environments,"In this paper, we study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of comparators. Existing work have shown that online gradient descent enjoys an $O(\sqrt{T}(1+P_T))$ dynamic regret, where $T$ is the number of iterations and $P_T$ is the path-length of the comparator sequence.  However, this result is unsatisfactory, as there exists a large gap from the $\Omega(\sqrt{T(1+P_T)})$ lower bound established in our paper. To address this limitation, we develop a novel online method, namely adaptive learning for dynamic environment (Ader), which achieves an $O(\sqrt{T(\log \log T+P_T)})$ dynamic regret, matching the lower bound up to a double logarithmic factor. The basic idea is to maintain a set of experts, each attaining an optimal dynamic regret for a specific path-length, and combines them with an expert-tracking algorithm.  Furthermore, we propose an improved Ader based on the surrogate loss, and in this way the number of gradient evaluations per round is reduced from $O(\log T)$ to $1$. Finally, we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators.","Lijun Zhang*,Nanjing University;Shiyin Lu,Nanjing University;Zhi-Hua  Zhou,Nanjing University",zljzju@gmail.com;lsy1116@qq.com;zhouzh@nju.edu.cn,Algorithms/Online Learning*; Optimization/Convex Optimization; Theory/Learning Theory,Poster
692,FRAGE: Frequency-Agnostic Word Representation,"Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In order to mitigate the issue, in this paper, we propose a neat, simple yet effective adversarial training method to blur the boundary between the embeddings of high-frequency words and low-frequency words. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that we achieve higher performance than the baselines in all tasks.","Chengyue Gong*,Peking Univeristy;Di He,Peking University;Xu Tan,;Tao Qin,Microsoft Research Asia;Liwei Wang,Peking University;Tieyan Liu,",1400016601@pku.edu.cn;dihe@microsoft.com;xu.tan@microsoft.com;taoqin@microsoft.com;wanglw@cis.pku.edu.cn;tyliu@microsoft.com,Applications/Natural Language Processing*; Algorithms/Representation Learning,Poster
695,Generative Neural Machine Translation,"We introduce Generative Neural Machine Translation (GNMT), a latent variable architecture which is designed to model the semantics of the source and target sentences. We modify an encoder-decoder translation model by adding a latent variable as a language agnostic representation which is encouraged to learn the meaning of the sentence. GNMT achieves competitive BLEU scores on pure translation tasks, and is superior when there are missing words in the source sentence. We augment the model to facilitate multilingual translation and semi-supervised learning without adding parameters. This framework significantly reduces overfitting when there is limited paired data available, and is effective for translating between pairs of languages not seen during training.","Harshil Shah*,UCL;David Barber,UCL",ucabhah@ucl.ac.uk;david.barber@ucl.ac.uk,Probabilistic Methods/Latent Variable Models*; Algorithms/Representation Learning; Algorithms/Semi-Supervised Learning; Applications/Natur,Poster
699,Found Graph Data and Planted Vertex Covers,"A typical way in which network data is recorded is to measure all interactions involving a specified set of core nodes, which produces a graph containing this core together with a potentially larger set of fringe nodes that link to the core. Interactions between nodes in the fringe, however, are not present in the resulting graph data.
For example, a phone service provider may only record calls in which at least one of the participants is a customer; this can include calls between a customer and a non-customer, but not between pairs of non-customers. Knowledge of which nodes belong to the core is crucial for interpreting the dataset, but this metadata is unavailable in many cases, either because it has been lost due to difficulties in data provenance, or because the network consists of ``found data'' obtained in settings
such as counter-surveillance. This leads to an algorithmic problem of recovering the core set. Since the core is a vertex cover, we essentially have a planted vertex cover problem, but with an arbitrary underlying graph. We develop a framework for analyzing this planted vertex cover problem, based on the theory of fixed-parameter tractability, together with algorithms for recovering the core. Our algorithms are fast, simple to implement, and out-perform several baselines based on core-periphery structure on various real-world datasets.","Austin Benson*,Cornell University;Jon Kleinberg,Cornell",arb@cs.cornell.edu;kleinber@cs.cornell.edu,Applications/Network Analysis*; Optimization/Combinatorial Optimization,Poster
700,Joint Active Feature Acquisition and Classification with Variable-Size Set Encoding,"We consider the problem of active feature acquisition where the goal is to sequentially select the subset of features in order to achieve the maximum prediction performance in the most cost-effective way at test time. In this work, we formulate this active feature acquisition as a jointly learning problem of training both the classifier (environment) and the RL agent that decides either to `stop and predict' or `collect a new feature' at test time, in a cost-sensitive manner. We also introduce a novel encoding scheme to represent acquired subsets of features by proposing an order-invariant set encoding at the feature level, which also significantly reduces the search space for our agent. We evaluate our model on a carefully designed synthetic dataset for the active feature acquisition as well as several medical datasets. Our framework shows meaningful feature acquisition process for diagnosis that complies with human knowledge, and outperforms all baselines in terms of prediction performance as well as feature acquisition cost. ","Hajin Shim,KAIST;Sung Ju Hwang,KAIST;Eunho Yang*,KAIST",shj9444@gmail.com;sjhwang82@kaist.ac.kr;yangeh@gmail.com,Deep Learning/Efficient Inference Methods*; Algorithms/Classification; Deep Learning/Efficient Training Methods; Reinforcement Learning an,Poster
707,Regularization Learning Networks: Deep Learning for Tabular Datasets,"Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at https://github.com/irashavitt/regularization_ learning_networks.","Ira Shavitt*,Weizmann Institute of Science;Eran Segal,Weizmann Institute of Science",irashavitt@gmail.com;eran.segal@weizmann.ac.il,Applications/Computational Biology and Bioinformatics*; Applications/Systems Biology; Deep Learning/Efficient Training Methods,Poster
719,Multitask Boosting for Survival Analysis with Competing Risks,"The co-occurrence of multiple diseases among the general population is an important problem as those patients have more risk of complications and represent a large share of health care expenditure. Learning to predict time-to-event probabilities for these patients is a challenging problem because the risks of events are correlated (there are competing risks) with often only few patients experiencing individual events of interest, and of those only a fraction are actually observed in the data. We introduce in this paper a survival model with the flexibility to leverage a common representation of related events that is designed to correct for the strong imbalance in observed outcomes. The procedure is sequential: outcome-specific survival distributions form the components of nonparametric multivariate estimators which we combine into an ensemble in such a way as to ensure accurate predictions on all outcome types simultaneously. Our algorithm is general and represents the first boosting-like method for time-to-event data with multiple outcomes. We demonstrate the performance of our algorithm on synthetic and real data.","Alexis Bellot*,University of Oxford;Mihaela van der Schaar,University of Oxford",alexis.b11@hotmail.com;mihaela.vanderschaar@eng.ox.ac.uk,Algorithms/Boosting and Ensemble Methods,Poster
722,Geometry Based Data Generation,"We propose a new type of generative model of high-dimensional data that learns a manifold geometry of the data, rather than density, and can generate points evenly along this manifold. This is in contrast to existing generative models that represent data density, and are strongly affected by noise and other artifacts of data collection. We demonstrate how this approach corrects sampling biases and artifacts, and improves several downstream data analysis tasks such as clustering and classification. Finally, we demonstrate that this approach is especially useful in biology where, despite the advent of single-cell technologies, rare subpopulations and gene-interaction relationships are affected by biased sampling. We show that SUGAR can generate hypothetical populations and reveal intrinsic patterns and mutual-information relationships between genes on a single cell dataset of hematopoiesis. ","Ofir Lindenbaum,Tel Aviv University;Jay Stanley,;Guy  Wolf,Yale University;Smita Krishnaswamy*,",ofirlin@gmail.com;Jay.stanley@yale.edu;guy.wolf@yale.edu;smita.krishnaswamy@yale.edu,Applications/Computational Biology and Bioinformatics*; Algorithms/Classification; Algorithms/Clustering; Algorithms/Kernel Methods; Algorithms ->,Spotlight
725,SLAYER: Spike Layer Error Reassignment in Time,"Configuring deep Spiking Neural Networks (SNNs) is an exciting research avenue for low power spike event based computation. However, the spike generation function is non-differentiable and therefore not directly compatible with the standard error backpropagation algorithm. In this paper, we introduce a new general backpropagation mechanism for learning synaptic weights and axonal delays which overcomes the problem of non-differentiability of the spike function and uses a temporal credit assignment policy for backpropagating error to preceding layers. We describe and release a GPU accelerated software implementation of our method which allows training both fully connected and convolutional neural network (CNN) architectures. Using our software, we compare our method against existing SNN based learning approaches and standard ANN to SNN conversion techniques and show that our method achieves state of the art performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS datasets.","Sumit Shrestha*,Temasek Laboratories @ National University of Singapore;Garrick Orchard,NUS",tslsbs@nus.edu.sg;tslgmo@nus.edu.sg,Deep Learning/Biologically Plausible Deep Networks*; Algorithms/Classification; Deep Learning,Poster
745,On Oracle-Efficient PAC RL with Rich Observations,"We study the computational tractability of provably sample-efficient (PAC) reinforcement learning in episodic environments with rich observations. We present new sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation‚Äîaccessing policy and value function classes exclusively through standard optimization primitives‚Äîand therefore represent computationally efficient alternatives to prior algorithms that require enumeration. In the more general stochastic transition setting, we prove that the only known sample-efficient algorithm, OLIVE [1], cannot be implemented in our oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.","Christoph Dann*,Carnegie Mellon University;Nan Jiang,University of Illinois at Urbana-Champaign;Akshay Krishnamurthy,Microsoft;Alekh Agarwal,Microsoft;John Langford,Microsoft;Robert Schapire,Microsoft",cdann@cdann.net;nanjiang@umich.edu;akshay@cs.umass.edu;alekha@microsoft.com;jcl@microsoft.com;schapire@microsoft.com,Theory/Learning Theory*; Reinforcement Learning and Planning/Exploration,Spotlight
746,Gradient Descent for Spiking Neural Networks,"Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (‚âà millisecond) spike-based interactions for efficient encoding of information, and a delayed-memory XOR task over extended duration (‚âà second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as the behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.","Dongsung Huh*,University of California, San Diego;Terry Sejnowski,",dongsunghuh@gmail.com;terry@salk.edu,Neuroscience and Cognitive Science/Spike Train Generation*; Deep Learning/Biologically Plausible Deep Networks; Deep Learning/Recurrent Networks; Deep Lea,Poster
750,Generalizing Tree Probability Estimation via Bayesian Networks,"Probability estimation is one of the fundamental tasks in statistics and machine learning. However, standard methods for probability estimation on discrete objects do not handle object structure in a satisfactory manner. In this paper, we derive a general Bayesian network formulation for probability estimation on leaf-labeled trees that enables flexible approximations which can generalize beyond observations. We show that efficient algorithms for learning Bayesian networks can be easily extended to probability estimation on this challenging structured space. Experiments on both synthetic and real data show that our methods greatly outperform the current practice of using the empirical distribution, as well as a previous effort for probability estimation on trees.","Cheng Zhang*,Fred Hutch;Frederick A Matsen IV,Fred Hutchinson Cancer Research Center",zc.rabbit@gmail.com;ematsen@gmail.com,Applications/Computational Biology and Bioinformatics*; Algorithms/Density Estimation; Algorithms/Representation Learning; Algorithms/Structured Pr,Spotlight
753,Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,"Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.","Siddharth Reddy*,UC Berkeley;Anca Dragan,EECS Department, University of California, Berkeley;Sergey Levine,UC Berkeley",sgr@berkeley.edu;anca@berkeley.edu;svlevine@eecs.berkeley.edu,Reinforcement Learning and Planning/Reinforcement Learning,Poster
757,Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution,"The high-dimensional convolution is widely used in various disciplines but has a serious performance problem due to its high computational complexity. Over the decades, people took a handmade approach to design fast algorithms for the Gaussian convolution. Recently, requirements for various non-Gaussian convolutions have emerged and are contentiously getting higher. However, the handmade acceleration approach is no longer feasible for so many different convolutions since it is a time-consuming and painstaking job. Instead, we propose an Acceleration Network (AccNet) which turns the work of designing new fast algorithms to training the AccNet. This is done by: 1, interpreting splatting, blurring, slicing operations as convolutions; 2, turning these convolutions to $g$CP layers to build AccNet. After training,  the activation function $g$ together with AccNet weights automatically define the new splatting, blurring and slicing operations. Experiments demonstrate AccNet is able to design acceleration algorithms for a ton of convolutions including Gaussian/non-Gaussian convolutions and produce state-of-the-art results.","Longquan Dai*,Nanjing University of Science and Technology ;Liang Tang, CASA Environmental Technology Co., Ltd and CASA EM&EW IOT Research Center;Yuan Xie,Chinese Academy of Sciences;Jinhui Tang,Nanjing University of Science and Technology",lqdai@foxmail.com;tangl@casaet.com;yuan.xie@ia.ac.cn;jinhuitang@njust.edu.cn,Applications/Computer Vision*; Applications/Signal Processing,Poster
758,Understanding the Role of Adaptivity in Machine Teaching: The Case of Version Space Learners,"In real-world applications of education, an effective teacher adaptively chooses the next example to teach based on the learner's current state. However, most existing work in algorithmic machine teaching focus on the batch setting, where adaptivity plays no role. In this paper, we study the case of teaching consistent, version space learners in an interactive setting. At any time step, the teacher provides an example, the learner performs an update, and the teacher observes the learner's new state. We highlight that adaptivity does not speed up the teaching process when considering existing models of version space learners, such as ""worst-case"" (the learner picks the next hypothesis randomly from the version space) and ""preference-based"" (the learner picks hypothesis according to some global preference). Inspired by human teaching, we propose a new model where the learner picks hypotheses according to some local preference defined by the current hypothesis. We show that our model exhibits several desirable properties, e.g., adaptivity plays a key role, and the learner‚Äôs transitions over hypotheses are smooth/interpretable. We develop efficient teaching algorithms, and demonstrate our results via simulation and user studies.","Yuxin Chen*,Caltech;Adish Singla,MPI-SWS;Oisin Mac Aodha,Caltech;Pietro Perona,CALTECH, USA;Yisong Yue,Caltech",chenyux@caltech.edu;adishs@mpi-sws.org;macaodha@caltech.edu;perona@caltech.edu;yyue@caltech.edu,Algorithms/Active Learning*; Neuroscience and Cognitive Science/Human or Animal Learning,Poster
766,A loss framework for calibrated anomaly detection,"Given samples from a probability distribution, anomaly detection is the problem of determining if a given point lies in a low-density region. This paper concerns calibrated anomaly detection, which is the practically relevant extension where we additionally wish to produce a confidence score for a point being anomalous. Building on a classification framework for anomaly detection, we show how minimisation of a suitably modified proper loss produces density estimates only for anomalous instances. We then show how to incorporate quantile control by relating our objective to a generalised version of the pinball loss. Finally, we show how to efficiently optimise the objective with kernelised scorer, by leveraging a recent result from the point process literature. The resulting objective captures a close relative of the one-class SVM as a special case. ","Aditya Menon*,Australian National University;Robert Williamson,Australian National University",aditya.menon@anu.edu.au;Bob.Williamson@anu.edu.au,Theory/Learning Theory*; Algorithms/Unsupervised Learning,Spotlight
769,PacGAN: The power of two samples in generative adversarial networks,"Generative adversarial networks (GANs) are a technique for learning generative models of complex data distributions from samples. Despite remarkable advances in generating realistic images, a major shortcoming of GANs is the fact that they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the focus of much recent work. We study a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We draw analysis tools from binary hypothesis testing---in particular the seminal result of Blackwell---to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggest that packing provides significant improvements. ","Zinan Lin,CMU;Ashish Khetan,UIUC;Giulia Fanti,CMU;Sewoong Oh*,UIUC",zinanl@andrew.cmu.edu;khetan2@illinois.edu;gfanti@andrew.cmu.edu;sewoong79@gmail.com,Deep Learning/Adversarial Networks*; Theory/Information Theory,Poster
772,Variational Memory Encoder-Decoder,"Introducing variability while maintaining coherence is a core task in learning to generate utterances in conversation. Standard neural encoder-decoder models and their extensions using conditional variational autoencoder often result in either trivial or digressive responses. To overcome this, we explore a novel approach that injects variability into neural encoder-decoder via the use of external memory as a mixture model, namely Variational Memory Encoder-Decoder (VMED). By associating each memory read with a mode in the latent mixture distribution at each timestep, our model can capture the variability observed in sequential data such as natural conversations. We empirically compare the proposed model against other recent approaches on various conversational datasets. The results show that VMED consistently achieves significant improvement over others in both metric-based and qualitative evaluations. ","Hung Le*,Deakin University;Truyen Tran,Deakin University;Thin Nguyen,Deakin University;Svetha Venkatesh,Deakin University",lethai@deakin.edu.au;truyen.tran@deakin.edu.au;thin.nguyen@deakin.edu.au;svetha.venkatesh@deakin.edu.au,Deep Learning/Memory-Augmented Neural Networks*; Applications/Dialog- or Communication-Based Learning; Deep Learning/Deep Autoencoders; Deep Le,Poster
776,Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities,"We study stochastic composite mirror descent, a class of scalable algorithms able to exploit the geometry and composite structure of a problem. We consider both convex and strongly convex objectives, for each of which we establish high-probability convergence rates optimal up to a logarithmic factor. We apply the derived computational error bounds to investigate generalization bounds of stochastic gradient descent (SGD) in a non-parametric setting, which refine the existing bounds in expectation by either removing smoothness assumptions on loss functions or improving the specific learning rates. We show that the involved estimation errors scale logarithmically with respect to the number of iterations provided that the step size sequence is square-summable, which justifies the ability of SGD to overcome overfitting. Our analysis removes boundedness assumptions on subgradients often imposed in the literature. Numerical results are reported to support our findings.","Yunwen Lei*,Southern University of Science and Technology;Ke Tang,Southern University of Science and Technology",yunwen.lei@hotmail.com;tangk3@sustc.edu.cn,Theory/Learning Theory*; Algorithms/Kernel Methods; Algorithms/Stochastic Methods; Optimization/Convex Optimization,Poster
778,Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation,"Generating long and topic-coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, and modern learning-based approaches to achieve structured, robust and diverse report generation. HRGR-Agent employs a retrieval policy module to generate a sequence of actions to decide between retrieving appropriate template sentences from an off-the-shelf template database, and automatic generating sentences by a generation module. Thus multiple sentences are sequentially generated via hierarchical decision-making. Our HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that HRGR-Agent achieves the state-of-the-art results on two medical report datasets, generating a well balanced structured and complicated sentences with robust coverage of heterogeneous medical report contents. In addition to automatic evaluations, we demonstrate that our model achieves the highest detection accuracy of medical terminologies, and best human evaluation performance.","Yuan Li*,Duke University;Xiaodan Liang,Carnegie Mellon University;Zhiting Hu,Carnegie Mellon University;Eric Xing,Petuum Inc. and CMU",liyuanchristy@gmail.com;xdliang328@gmail.com;zhitingh@cs.cmu.edu;epxing@cs.cmu.edu,Applications/Computer Vision*; Applications/Natural Language Processing,Poster
779,Overcoming Language Priors in Visual Question Answering with Adversarial Regularization,"Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training -- \eg overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings. 

In this work, we present a novel regularization scheme for VQA that reduces this effect. We introduce a question-only model that takes as input the question encoding from the VQA model and must leverage language biases in order to succeed. We then pose training as an adversarial game between the VQA model and this question-only adversary -- discouraging the VQA model from capturing language biases in its question encoding.Further, we leverage this question-only model to estimate the mutual information between the image and answer given the question, which we maximize explicitly to encourage visual grounding. Our approach is a model agnostic training procedure and simple to implement. We show empirically that it can improve performance significantly on a bias-sensitive split of the VQA dataset for multiple base models -- achieving state-of-the-art on this task. Further, on standard VQA tasks, our approach shows significantly less drop in accuracy compared to existing bias-reducing VQA models.","Sainandan Ramakrishnan*,Georgia Institute of Technology;Aishwarya Agrawal,Georgia Institute of Technology;Stefan Lee,Georgia Institute of Technology",sainandancv@gatech.edu;aishwarya@gatech.edu;steflee@gatech.edu,Applications/Visual Question Answering*; Deep Learning/Adversarial Networks,Poster
782,Hybrid Knowledge Routed Modules for Large-scale Object Detection,"The dominant object detection approaches treat the recognition of each region separately and overlook crucial semantic correlations between objects in one scene. This paradigm leads to substantial performance drop when facing heavy long-tail problems, where very few samples are available for rare classes and plenty of confusing categories exists. We exploit diverse human commonsense knowledge for reasoning over large-scale object categories and reaching semantic coherency within one image. Particularly, we present Hybrid Knowledge Routed Modules (HKRM) that incorporates the reasoning routed by two kinds of knowledge forms: an explicit knowledge module for structured constraints that are summarized with linguistic knowledge (e.g. shared attributes, relationships) about concepts; and an implicit knowledge module that depicts some implicit constraints (e.g. common spatial layouts). By functioning over a region-to-region graph, both modules can be individualized and adapted to coordinate with visual patterns in each image, guided by specific knowledge forms. HKRM are light-weight, general-purpose and extensible by easily incorporating multiple knowledge to endow any detection networks the ability of global semantic reasoning. Experiments on large-scale object detection benchmarks show HKRM obtains around 34.5% improvement on VisualGenome (1000 categories) and 30.4% on ADE in terms of mAP.","ChenHan Jiang*,Sun Yat-sen University;Hang Xu,Huawei Noah's Ark Lab;Xiaodan Liang,Carnegie Mellon University;Liang Lin,Sun Yat-sen University",jchcyan@gmail.com;xbjxh@live.com;xdliang328@gmail.com;linliang@ieee.org,Applications/Object Detection*; Applications/Object Recognition,Poster
792,Bilinear Attention Networks,"Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively.  However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.","Jin-Hwa Kim*,SK T-Brain;Jaehyun Jun,Seoul National University;Byoung-Tak Zhang,Seoul National University",jnhwkim@snu.ac.kr;jhjun@bi.snu.ac.kr;btzhang@bi.snu.ac.kr,Applications/Visual Question Answering*; Deep Learning/Attention Models,Poster
798,Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning,"We propose a parsimonious quantile regression framework to learn the dynamic tail behavior of financial asset returns. Our method captures well both the time-varying characteristic and the asymmetrical heavy-tail property of financial time series. It combines the merits of the popular sequential neural network model, i.e., LSTM, with a novel parametric quantile function that we construct to represent conditional distribution of asset returns. Our method also captures individually the serial dependences of higher moments, rather than just the volatility. Across a wide range of asset classes, the out-of-sample forecasts of conditional quantiles or VaR of our model outperform the GARCH family. Further, the approach does not suffer from the issue of quantile crossing nor does it expose to the ill-posedness comparing to the parametric probability density function approach.","Xing Yan,The Chinese University of Hong Kong;Weizhong Zhang,Tencent AI Lab;Lin Ma,Tencent AI Lab;Wei Liu,Tencent AI Lab;Qi Wu*,City University of Hong Kong",xyan@se.cuhk.edu.hk;zhangweizhongzju@gmail.com;forest.linma@gmail.com;wl2223@columbia.edu;qiwu55@cityu.edu.hk,Applications/Quantitative Finance and Econometrics,Poster
802,Multi-Class Learning: From Theory to Algorithm,"In this paper, we study the generalization performance of multi-class classification and obtain a shaper data-dependent generalization error bound with fast convergence rate, substantially improving the state-of-art bounds in the existing data-dependent generalization analysis. The theoretical analysis motivates us to devise two effective multi-class kernel learning algorithms with statistical guarantees. Experimental results show that our proposed methods can significantly outperform the existing multi-class classification methods.","Jian Li,Institute of Information Engineering, CAS;Yong Liu*,Institute of Information Engineering, CAS;Rong Yin,School of Cyber Security, University of Chinese Academy of Sciences;Hua Zhang,Institute of Information Engineering,Chinese Academy of Sciences;Lizhong Ding,KAUST;Weiping Wang,Institute of Information Engineering, CAS, China",lijian9026@iie.ac.cn;liuyong@iie.ac.cn;yinrong@iie.ac.cn;zhanghua@iie.ac.cn;lizhong.ding@kaust.edu.sa;wangweiping@iie.ac.cn,Algorithms/Classification*; Algorithms/Large Margin Methods; Theory/Learning Theory,Poster
814,Multivariate Time Series Imputation with Generative Adversarial Networks,"Multivariate time series are characterized by a variety of missing values and the advanced analysis suffers. Conventional methods for imputing missing values, such as mean and zero imputation, deletion and matrix factorization, are not capable to model the temporal dependencies and the complex distribution of the multivariate time series. In this paper, we regard missing value imputation as a data generation problem. Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we adopt GAN to learn the overall distribution of a multivariate time series dataset and to generate the missing values for each sample. Different from image completion where GAN is trained on complete images, we cannot obtain complete time series due to the nature of data recording process. Therefore, a modified Gate Recurrent Unit (GRU) model is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that our method outperforms the baselines in terms of accuracy of missing value imputation. Additionally, a simple model on the imputed data achieves state-of-the-art results on the two prediction tasks, which demonstrates the benefit of our imputation method for downstream applications.","Yonghong Luo,Nankai University;Xiangrui Cai,Nankai University;Ying ZHANG*,Nankai Univeristy;Jun Xu,Renmin University of China;Yuan xiaojie,Nankai Univeristy",luoyonghong@dbis.nankai.edu.cn;caixiangrui@dbis.nankai.edu.cn;yingzhang@nankai.edu.cn;junxu@ruc.edu.cn;yuanxj@nankai.edu.cn,Algorithms/Missing Data*; Deep Learning/Adversarial Networks; Deep Learning/Recurrent Networks,Poster
818,Learning Versatile Filters for Efficient Convolutional Neural Networks,"This paper introduces versatile filters to construct efficient convolutional neural network. Considering the demands of efficient deep learning techniques running on cost-effective hardware, a number of methods have been developed to learn compact neural networks. Most of these works aim to slim down filters in different ways, e.g., investigating small, sparse or binarized filters. In contrast, we treat filters from an additive perspective. A series of secondary filters can be derived from a primary filter. These secondary filters all inherit in the primary filter without occupying more storage, but once been unfolded in computation they could significantly enhance the capability of the filter by integrating information extracted from different receptive fields. Besides spatial versatile filters, we additionally investigate versatile filters from the channel perspective. The new techniques are general to upgrade filters in existing CNNs. Experimental results on benchmark datasets and neural networks demonstrate that CNNs constructed with our versatile filters are able to achieve comparable accuracy as that of original filters, but require less memory and FLOPs.","Yunhe Wang*,Huawei Noah‚Äôs Ark Lab;Chang Xu,University of Sydney;Chunjing Xu,Huawei;Chao Xu,Peking University;Dacheng Tao,The University of Sydney",wangyunhe@pku.edu.cn;changxu1989@gmail.com;xuchunjing@huawei.com;xuchao@cis.pku.edu.cn;dacheng.tao@gmail.com,Deep Learning*; Deep Learning/CNN Architectures,Poster
825,Accelerated Stochastic Matrix Inversion:  General Theory and  Speeding up BFGS Rules for Faster Second-Order Optimization,"We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces. One essential problem of this type is the matrix inversion problem. In particular, our algorithm can be specialized to invert positive definite matrices in such a way that all iterates (approximate solutions) generated by the algorithm are positive definite matrices themselves. This opens the way for many applications in the field of optimization and machine learning.  As an application of our general theory, we develop the first  accelerated (deterministic and stochastic) quasi-Newton updates. Our updates lead to provably more aggressive approximations of the inverse Hessian, and lead to speed-ups over classical non-accelerated rules in numerical experiments. Experiments with empirical risk minimization show that our rules can accelerate training of machine learning models.","Robert Gower,ParisTech;Filip Hanzely,KAUST;Peter Richtarik,KAUST;Sebastian Stich*,EPFL",gower.robert@gmail.com;filip.hanzely@kaust.edu.sa;richtarik@gmail.com;sebastian.stich@epfl.ch,Optimization/Convex Optimization,Poster
828,DifNet: Semantic Segmentation by Diffusion Networks,"Deep Neural Networks (DNNs) have recently shown state of the art performance on semantic segmentation tasks, however they still suffer from the problem of poor boundary localization and spatial fragmented predictions. The difficulties lie in the requirement of making dense predictions from a long path model all at once, since details are hard to keep when data goes through deeper layers. Instead, in this work, we decompose this difficult task into two relative simple sub-tasks: seed detection which is required to predict initial predictions without need of wholeness and preciseness, and similarity estimation which estimates the possibility of any two nodes that belong to the same class without need of knowing which class they are. We use one branch network for one sub-task each, and apply a cascade of random walk operations base on hierarchical semantics to approximate a complex diffusion process which propagates seed information to the whole image according to the estimated similarities.
The proposed DifNet consistently produces an improvement over the baseline models with same depth and equivalent number of parameters, and also get promising performance on Pascal VOC 2012 and Pascal Context dataset. Our DifNet is trained end-to-end without complex loss functions.","Peng Jiang*,Shandong University;Fanglin Gu,Shandong University;Yunhai Wang,Shandong University;Changhe Tu,Shandong University;Baoquan Chen,Shandong University",sdujump@gmail.com;fanglin.gu@gmail.com;cloudseawang@gmail.com;chtu@sdu.edu.cn;baoquan.chen@gmail.com,Applications/Image Segmentation*; Algorithms/Similarity and Distance Learning; Deep Learning/CNN Architectures,Poster
836,Conditional Adversarial Domain Adaptation,"Adversarial learning has been embedded into deep networks to learn transferable representations for domain adaptation. Existing adversarial domain adaptation methods may struggle to align different domains of multimode distributions that are native in classification problems. In this paper, we present conditional adversarial domain adaptation, a new framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks are proposed to enable discriminative adversarial adaptation of multimode domains. Experiments testify that the proposed approaches exceed the state-of-the-art results on three domain adaptation datasets.","Mingsheng Long*,Tsinghua University;Zhangjie Cao,Tsinghua University;Jianmin Wang,""Tsinghua University, China"";Michael Jordan,UC Berkeley",longmingsheng@gmail.com;caozhangjie14@gmail.com;jimwang@tsinghua.edu.cn;jordan@cs.berkeley.edu,Deep Learning/Adversarial Networks*; Algorithms/Multitask and Transfer Learning,Poster
839,Neighbourhood Consensus Networks,"We address the problem of finding reliable dense correspondences between a pair of images. This is a challenging task due to strong appearance differences between the corresponding scene elements and ambiguities generated by repetitive patterns. The contributions of this work are threefold. First, inspired by the classic idea of disambiguating feature matches using semi-local constraints,  we develop an end-to-end trainable convolutional neural network architecture that identifies sets of spatially consistent  matches by analyzing neighbourhood consensus patterns in the 4D space of all possible correspondences between a pair of images without the need for a global geometric model.   
Second, we demonstrate that the model can be trained effectively from weak supervision in the form of matching and non-matching image pairs without the need for costly manual annotation of point to point correspondences.
Third, we show the proposed neighbourhood consensus network can be applied to a range of matching tasks including both category- and instance-level matching, obtaining state-of-the-art results on the PF Pascal dataset and the InLoc indoor visual localization benchmark.","Ignacio ROCCO*,Inria;Mircea Cimpoi,CIIRC CTU in Prague;Relja Arandjeloviƒá,DeepMind;Akihiko Torii,Tokyo Institute of Technology, Japan;Tomas Pajdla,Czech Technical University in Prague;Josef Sivic,INRIA",Ignacio.Rocco@inria.fr;Mircea.Cimpoi@cvut.cz;relja@google.com;torii@ctrl.titech.ac.jp;pajdla@cvut.cz;josef.sivic@ens.fr,Applications/Computer Vision*; Applications/Robotics; Deep Learning/CNN Architectures,Spotlight
853,Relating Leverage Scores and Density using Regularized Christoffel Functions,"Statistical leverage scores emerged as a fundamental tool for matrix sketching and column sampling with applications to low rank approximation, regression, random feature learning and quadrature. Yet, the very nature of this quantity is barely understood. Borrowing ideas from the orthogonal polynomial literature, we introduce the regularized Christoffel function associated to a positive definite kernel. This uncovers a variational formulation for leverage scores for kernel methods and allows to elucidate their relationships with the chosen kernel as well as population density. Our main result quantitatively describes a decreasing relation between leverage score and population density for a broad class of kernels on Euclidean spaces. Numerical simulations support our findings.","Edouard Pauwels*,IRIT;Francis Bach,INRIA - Ecole Normale Sup√©rieure;Jean-Philippe Vert,ENS Paris",epauwels@irit.fr;francis.bach@inria.fr;jean-philippe.vert@mines-paristech.fr,Algorithms/Kernel Methods*; Algorithms/Density Estimation; Theory/Spaces of Functions and Kernels,Poster
860,Non-Local Recurrent Network for Image Restoration,"Many classic methods have shown non-local self-similarity in natural images to be an effective prior for image restoration. However, it remains unclear and challenging to make use of this intrinsic property via deep networks. In this paper, we propose a non-local recurrent network (NLRN) as the first attempt to incorporate non-local operations into a recurrent neural network (RNN) for image restoration. The main contributions of this work are: (1) Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood. (2) We fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states. This new design boosts robustness against inaccurate correlation estimation due to severely degraded images. (3) We show that it is essential to maintain a confined neighborhood for computing deep feature correlation given degraded images. This is in contrast to existing practice that deploys the whole image. Extensive experiments on both image denoising and super-resolution tasks are conducted. Thanks to the recurrent non-local operations and correlation propagation, the proposed NLRN achieves superior results to state-of-the-art methods with many fewer parameters.","Ding Liu*,UIUC;Bihan Wen,University of Illinois at Urbana-Champaign;Yuchen Fan, Image Formation and Processing (IFP) Group, University of Illinois at Urbana-Champaign;Chen Change  Loy,Nanyang Technological University;Thomas Huang,UIUC",dingliu2@illinois.edu;bwen3@illinois.edu;yuchenf4@illinois.edu;ccloy@ntu.edu.sg;t-huang1@illinois.edu,Applications/Denoising*; Applications/Computer Vision,Poster
864,Bayesian Semi-supervised Learning with Graph Gaussian Processes,"We propose a data-efficient Gaussian process-based Bayesian approach to the semi-supervised learning problem on graphs. The proposed model shows extremely competitive performance when compared to the state-of-the-art graph neural networks on semi-supervised learning benchmark experiments, and outperforms the neural networks in active learning experiments where labels are scarce. Furthermore, the model does not require a validation data set for early stopping to control over-fitting. Our model can be viewed as an instance of empirical distribution regression weighted locally by network connectivity. We further motivate the intuitive construction of the model with a Bayesian linear model interpretation where the node features are filtered by an operator related to the graph Laplacian. The method can be easily implemented by adapting off-the-shelf scalable variational inference algorithms for Gaussian processes.","Yin Cheng Ng*,University College London;Nicolo Colombo,UCL;Ricardo Silva,UCL",yincheng.ng@gmail.com;nicolo.colombo@ucl.ac.uk;ricardo@stats.ucl.ac.uk,Probabilistic Methods/Gaussian Processes*; Algorithms/Semi-Supervised Learning,Poster
865,Foreground Clustering for Joint Segmentation and Localization in Videos and Images," This paper presents a novel framework in which video/image segmentation and localization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. The proposed framework leverages two representations at different levels, exploits the spatial relationship between bounding boxes and superpixels as linear constraints and  simultaneously discriminates between foreground and background at box and superpixel level. Different from previous approaches that mainly rely on discriminative clustering, we incorporate a foreground model that minimizes the histogram difference of an object across all image frames. Exploiting the geometric relation between the superpixels and bounding boxes enables the transfer of segmentation cues to improve localization output and vice-versa. Inclusion of the foreground model generalizes our discriminative framework to video data where the background tends to be similar and thus, not discriminative. We demonstrate the effectiveness of our unified framework on the YouTube Objects video dataset, Internet Object Discovery dataset and Pascal VOC 2007.","Abhishek Sharma*,No Affiliation",kein.iitian@gmail.com,Applications/Computer Vision*; Algorithms/Clustering; Algorithms/Multitask and Transfer Learning; Algorithms/Unsupervised,Poster
867,Video Prediction via Selective Sampling,"Most adversarial learning based video prediction methods suffer from image blur, since the commonly used adversarial and regression loss pair work rather in a competitive way than collaboration, yielding compromised blur effect. 
  In the meantime, as often relying on a single-pass architecture, the predictor is inadequate to explicitly capture the forthcoming uncertainty.
  Our work involves two key insights:
  (1) Video prediction can be approached as a stochastic process: we sample a collection of proposals conforming to possible frame distribution at following time stamp, and one can select the final prediction from it.
  (2) De-coupling combined loss functions into dedicatedly designed sub-networks encourages them to work in a collaborative way.
  Combining above two insights we propose a two-stage network called VPSS (\textbf{V}ideo \textbf{P}rediction via \textbf{S}elective \textbf{S}ampling).  
  Specifically a \emph{Sampling} module produces a collection of high quality proposals, facilitated by a multiple choice adversarial learning scheme, yielding diverse frame proposal set. 
  Subsequently a \emph{Selection} module selects high possibility candidates from proposals and combines them to produce final prediction.  
  Extensive experiments on diverse challenging datasets 
  demonstrate the effectiveness of proposed video prediction approach, i.e., yielding more diverse proposals and accurate prediction results.","Jingwei Xu*,Shanghai Jiao Tong University;Bingbing Ni,Shanghai Jiao Tong University;Xiaokang Yang,Shanghai Jiao Tong University of China",xjwxjw@sjtu.edu.cn;nibingbing@sjtu.edu.cn;xkyang@sjtu.edu.cn,Applications*; Applications/Video Analysis,Poster
870,Distilled Wasserstein Learning for Word Embedding and Topic Modeling,"We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics. 
The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model. 
The word distributions of topics, their optimal transport to the word distributions of documents, and the embeddings of words are learned in a unified framework. 
When learning the topic model, we leverage a distilled ground-distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports. 
Such a strategy provides the updating of word embeddings with robust guidance, improving algorithm convergence. 
As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.","Hongteng Xu*,Duke University;Wenlin Wang,Duke Univeristy;Wei Liu,Tencent AI Lab;Lawrence Carin Duke,CS",hongtengxu313@gmail.com;wenlin.wang@duke.edu;wl2223@columbia.edu;lcarin@duke.edu,Applications*; Applications/Natural Language Processing; Applications/Text Analysis,Poster
872,Learning to Exploit Stability for 3D Scene Parsing,"Human scene understanding uses a variety of visual and non-visual cues to perform inference on object types, poses, and relations. Physics is a rich and universal cue which we exploit to enhance scene understanding. We integrate the physical cue of stability into the learning process using a REINFORCE approach coupled to a physics engine, and apply this to the problem of producing the 3D bounding boxes and poses of objects in a scene. We first show that applying physics supervision to an existing scene understanding model increases performance, produces more stable predictions, and allows training to an equivalent performance level with fewer annotated training examples. We then present a novel architecture for 3D scene parsing named Prim R-CNN. With physics supervision, Prim R-CNN outperforms existing scene understanding approaches on this problem. Finally, we show that applying physics supervision on unlabeled real images improves real domain transfer of models training on synthetic data.","Yilun Du,MIT;Zhijian Liu,MIT;Hector Basevi,University of Birmingham;Ales Leonardis,University of Birmingham;Bill Freeman,MIT;Joshua Tenenbaum,MIT;Jiajun Wu*,MIT",yilundu@mit.edu;zhijian@mit.edu;h.r.a.basevi@cs.bham.ac.uk;a.leonardis@cs.bham.ac.uk;billf@mit.edu;jbt@mit.edu;jiajunwu.cs@gmail.com,Applications/Visual Scene Analysis and Interpretation*; Neuroscience and Cognitive Science/Reasoning; Neuroscience and Cognitive Science/Visual Percep,Poster
876,Neural Guided Constraint Logic Programming for Program Synthesis,"Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example problems by using a neural model to guide the search of a constraint logic programming system called miniKanren.  Internally, miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We present a Recurrent Neural Network model and a Gated Graph Neural Network model, both of which use these constraints as input to score candidate programs. We further present a transparent version of miniKanren that can be driven by an external agent, suitable for use by other researchers. We show that our neural-guided approach using constraints can synthesize problems faster in many cases, and has the potential to generalize to larger problems.","Lisa Zhang*,University of Toronto;Gregory Rosenblatt,University of Alabama at Birmingham;Ethan Fetaya,University of Toronto;Renjie Liao,University of Toronto;William Byrd,University of Alabama at Birmingham;Matthew Might,University of Alabama at Birmingham;Raquel Urtasun,Uber ATG, University of Toronto;Richard Zemel,University of Toronto",lczhang@cs.toronto.edu;gregr@uab.edu;ethanf@cs.toronto.edu;rjliao@cs.toronto.edu;webyrd@uab.edu;might@uab.edu;urtasun@cs.toronto.edu;zemel@cs.toronto.edu,Deep Learning/Program Induction,Poster
879,Genetic-Gated Networks for Deep Reinforcement Learning,"We introduce the Genetic-Gated Networks (G2Ns), simple neural networks that combine a gate vector composed of binary genetic genes in the hidden layer(s) of networks. Our method can take both advantages of gradient-free optimization and gradient-based optimization methods, of which the former is effective for problems with multiple local minima, while the latter can quickly find local minima. In addition, multiple chromosomes can define different models, making it easy to construct multiple models and can be effectively applied to problems that require multiple models. We show that this G2N can be applied to typical reinforcement learning algorithms to achieve a large improvement in sample efficiency and performance.","Simyung Chang,Seoul National University;John Yang,Seoul National University;Jaeseok Choi,Seoul National University;Nojun Kwak*,Seoul National University",timelighter@snu.ac.kr;yjohn@snu.ac.kr;jaeseok.choi@snu.ac.kr;nojunk@snu.ac.kr,Reinforcement Learning and Planning/Reinforcement Learning*; Optimization/Combinatorial Optimization; Reinforcement Learning and Planning/Multi-Agent RL,Poster
883,Fighting Boredom in Recommender Systems with Linear Reinforcement Learning,"A common assumption in recommender systems (RS) is the existence of a best fixed recommendation strategy. Such strategy may be simple and work at the item level (e.g., in multi-armed bandit it is assumed one best fixed arm/item exists) or implement more sophisticated RS (e.g., the objective of A/B testing is to find the
best fixed RS and execute it thereafter). We argue that this assumption is rarely verified in practice, as the recommendation process itself may impact the user‚Äôs
preferences. For instance, a user may get bored by a strategy, while she may gain interest again, if enough time passed since the last time that strategy was used. In
this case, a better approach consists in alternating different solutions at the right frequency to fully exploit their potential. In this paper, we first cast the problem as
a Markov decision process, where the rewards are a linear function of the recent history of actions, and we show that a policy considering the long-term influence
of the recommendations may outperform both fixed-action and contextual greedy policies. We then introduce an extension of the UCRL algorithm ( L IN UCRL ) to
effectively balance exploration and exploitation in an unknown environment, and we derive a regret bound that is independent of the number of states. Finally,
we empirically validate the model assumptions and the algorithm in a number of realistic scenarios.","Romain WARLOP*,Inria;Alessandro Lazaric,;J√©r√©mie Mary,",romainwarlop@gmail.com;alessandro.lazaric@gmail.com;jeremie.mary@gmail.com,Reinforcement Learning and Planning/Model-Based RL*; Algorithms/Collaborative Filtering,Poster
888,Enhancing the Accuracy and Fairness of Human Decision Making,"Societies often rely on human experts to take a wide variety of decisions affecting their members, from jail-or-release decisions taken by judges and stop-and-frisk decisions taken by police officers to accept-or-reject decisions taken  by academics. In this context, each decision is taken by an expert who is typically chosen uniformly at random from a pool of experts. However, these decisions may be imperfect due to limited experience, implicit biases, or faulty probabilistic reasoning. Can we improve the accuracy and fairness of the overall decision making process by optimizing the assignment between experts and decisions?

In this paper, we address the above problem from the perspective of sequential decision making and show that, for different fairness notions from the literature, it reduces to a sequence of (cons\-trained) weighted bipartite matchings, which can be solved efficiently using algorithms with approximation guarantees. Moreover, these algorithms also benefit from posterior sampling to actively trade off exploitation---selecting expert assignments which lead to accurate and fair decisions---and exploration---selecting expert assignments to learn about the experts' preferences and biases. We demonstrate the effectiveness of our algorithms on both synthetic and real-world data and show that they can significantly improve both the accuracy and fairness of the decisions taken by pools of experts.","Isabel Valera*,MPI-IS;Adish Singla,MPI-SWS;Manuel Gomez Rodriguez,Max-Planck Institute",isabel.valera@tuebingen.mpg.de;adishs@mpi-sws.org;manuelgr@mpi-sws.org,"Applications/Fairness, Accountability, and Transparency",Poster
894,Temporal Regularization for Markov Decision Process,"Several applications of Reinforcement Learning suffer from instability due to high variance. This is especially prevalent in high dimensional domains. Regularization is a commonly used technique in machine learning to reduce variance, at the cost of introducing some bias. Most existing regularization techniques focus on spatial (perceptual) regularization.  Yet in reinforcement learning, due to the nature of the Bellman equation, there is an opportunity to also exploit temporal regularization based on smoothness in value estimates over trajectories.  This paper explores a class of methods for temporal regularization. We formally characterize the bias induced by this technique using Markov chain concepts. We illustrate the various characteristics of temporal regularization via a sequence of simple discrete and continuous MDPs, and show that the technique provides improvement even in high-dimensional Atari games.","Pierre Thodoroff*,McGill;Audrey Durand,McGill University;Joelle Pineau,McGill / Facebook;Doina Precup,McGill University",pierre.thodoroff@mail.mcgill.ca;audrey.durand@mail.mcgill.ca;jpineau@cs.mcgill.ca;dprecup@cs.mcgill.ca,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Markov Decision Processes,Poster
902,The Pessimistic Limits and Possibilities of Margin-based Losses in Semi-supervised Learning,"Consider a classification problem where we have both labeled and unlabeled data available.  We show that for linear classifiers defined by convex margin-based surrogate losses that are decreasing,  it is impossible to construct any semi-supervised approach that is able to guarantee an improvement over the supervised classifier measured by this surrogate loss on the labeled and unlabeled data. For convex margin-based loss functions that also increase, we demonstrate safe improvements are possible.","Jesse Krijthe*,Radboud University Nijmegen;Marco Loog,TU Delft",jkrijthe@gmail.com;M.Loog@tudelft.nl,Algorithms/Semi-Supervised Learning*; Algorithms/Classification,Poster
914,Simple random search of static linear policies is competitive for reinforcement learning,"We introduce a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. We evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, indicating that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.","Horia Mania*,UC Berkeley;Aurelia Guy,UC Berkeley;Benjamin Recht,UC Berkeley",hmania@berkeley.edu;lia@berkeley.edu;brecht@berkeley.edu,Reinforcement Learning and Planning*; Optimization/Non-Convex Optimization; Reinforcement Learning and Planning/Decision and Control,Poster
916,Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization,"Responses generated by neural conversational models tend to lack informativeness and diversity. We present Adversarial Information Maximization (AIM), an adversarial learning framework that addresses these two related but distinct problems. To foster response diversity, we leverage adversarial training that allows distributional matching of synthetic and real responses. To improve informativeness, our framework explicitly optimizes a variational lower bound on pairwise mutual information between query and response. Empirical results from automatic and human evaluations demonstrate that our methods significantly boost informativeness and diversity.","Yizhe Zhang*,Microsoft;Michel Galley,Microsoft;Jianfeng  Gao,Microsoft Research;Zhe Gan,Microsoft;Xiujun Li,Microsoft;Chris Brockett,Microsoft;Bill Dolan,Microsoft",yizhe.zhang@microsoft.com;mgalley@microsoft.com;jfgao@microsoft.com;zhe.gan@microsoft.com;xiul@microsoft.com;chris.brockett@microsoft.com;billdol@microsoft.com,Applications/Dialog- or Communication-Based Learning*; Applications/Natural Language Processing,Poster
920,Entropy and mutual information in models of deep neural networks,"We examine a class of deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual information throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.","Marylou Gabri√©*,√âcole Normale Sup√©rieure;Andre Manoel,Inria Saclay √éle-de-France;Cl√©ment  Luneau,√âcole Polytechnique de Lausanne;Jean Barbier,√âcole Polytechnique de Lausanne;Nicolas Macris,√âcole Polytechnique de Lausanne;Florent Krzakala,√âcole Normale Sup√©rieure;Lenka Zdeborova,CEA Saclay",marylou.gabrie@ens.fr;andremanoel@gmail.com;clement.luneau@epfl.ch;jean.barbier@epfl.ch;nicolas.macris@epfl.ch;florent.krzakala@gmail.com;lenka.zdeborova@gmail.com,Theory/Statistical Physics of Learning*; Deep Learning; Theory/Information Theory,Spotlight
921,Collaborative Learning for Deep Neural Networks,"We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving generalization. Second, intermediate-level representation (ILR) sharing with backpropagation rescaling aggregates the gradient flows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on CIFAR and ImageNet datasets demonstrate that deep neural networks learned as a group in a collaborative way significantly reduce the generalization error and increase the robustness to label noise.","Guocong Song*,Playground Global;Wei Chai,Google",songgc@gmail.com;chaiwei@gmail.com,Deep Learning/Efficient Training Methods*; Applications/Object Recognition,Poster
922,High Dimensional Linear Regression using Lattice Basis Reduction,"We consider a high dimensional linear regression problem where the goal is to efficiently recover an unknown vector \beta^* from n noisy linear observations Y=X\beta^*+W \in \mathbb{R}^n, for known X \in \mathbb{R}^{n \times p} and unknown W \in \mathbb{R}^n. Unlike most of the literature on this model we make no sparsity assumption on \beta^*. Instead we adopt a regularization based on assuming that the underlying vectors \beta^* have rational entries with the same denominator Q \in \mathbb{Z}_{>0}. We call this Q-rationality assumption.  We propose a new polynomial-time algorithm for this task which is based on the seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction algorithm.  We establish that under the Q-rationality assumption, our algorithm recovers exactly the vector \beta^* for a large class of distributions for the iid entries of X and non-zero noise W. We prove that it is successful under small noise, even when the learner has access to only one observation (n=1). Furthermore, we prove that in the case of the Gaussian white noise for W, n=o(p/\log p) and Q sufficiently large, our algorithm tolerates a nearly optimal information-theoretic level of the noise.","Ilias Zadik*,MIT;David Gamarnik,MIT",izadik@mit.edu;gamarnik@mit.edu,Theory/Learning Theory*; Algorithms/Regression; Theory/Information Theory,Poster
926,Symbolic Graph Reasoning Meets Convolutions,"Beyond local convolution networks, we explore how to harness various external human knowledge for endowing the networks with the capability of semantic global reasoning. Rather than using separate graphical models (e.g. CRF) or constraints for modeling broader dependencies, we propose a new Symbolic Graph Reasoning (SGR) layer, which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph. To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; b) a graph reasoning module propagates information over knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping module learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features. The SGR layer can be injected between any convolution layers and instantiated with distinct prior graphs. Extensive experiments show incorporating SGR significantly improves plain ConvNets on three semantic segmentation tasks and one image classification task. More analyses show the SGR layer learns shared symbolic representations for domains/datasets with the different label set given a universal knowledge graph, demonstrating its superior generalization capability.","Xiaodan Liang*,Carnegie Mellon University;Zhiting Hu,Carnegie Mellon University;Hao Zhang,Petuum Inc.;Liang Lin,Sun Yat-sen University;Eric Xing,Petuum Inc. and CMU",xdliang328@gmail.com;zhitingh@cs.cmu.edu;hao.zhang@petuum.com;linliang@ieee.org;epxing@cs.cmu.edu,Applications/Image Segmentation*; Deep Learning/CNN Architectures,Poster
931,DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors,"Boltzmann machines are powerful distributions that have been shown to be an effective prior over binary latent variables in variational autoencoders (VAEs). However, previous methods for training discrete VAEs have used the evidence lower bound and not the tighter importance-weighted bound. We propose two approaches for relaxing Boltzmann machines to continuous distributions that permit training with importance-weighted bounds. These relaxations are based on generalized overlapping transformations and the Gaussian integral trick. Experiments on the MNIST and OMNIGLOT datasets show that these relaxations outperform previous discrete VAEs with Boltzmann priors. An implementation which reproduces these results is available.","Arash Vahdat*,D-Wave;Evgeny Andriyash,D-Wave;William Macready,D-Wave",vahdat.ce@gmail.com;eandriyash@dwavesys.com;wgm@dwavesys.com,Deep Learning/Generative Models*; Deep Learning/Deep Autoencoders; Probabilistic Methods/Latent Variable Models; Probabilistic M,Poster
932,Partially-Supervised Image Captioning,"Image captioning models are becoming increasingly successful at describing the content of images in restricted domains. However, if these models are to function in the wild --- for example, as aids for the visually impaired --- a much larger number and variety of visual concepts must be understood. In this work, we teach image captioning models new visual concepts with partial supervision, such as available from object detection and image label datasets. As these datasets contain text fragments rather than complete captions, we formulate this problem as learning from incomplete data. To flexibly characterize our uncertainty about the unobserved complete sequence, we represent each incomplete training sequence with its own finite state automaton encoding acceptable completions. We then propose a novel algorithm for training sequence models, such as recurrent neural networks, on incomplete sequences specified in this manner. In the context of image captioning, our method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities. Applying our approach to an existing neural captioning model, we achieve state of the art results on the novel object captioning task using the COCO dataset. We further show that we can train a captioning model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores.","Peter Anderson*,Georgia Tech;Stephen Gould,Australian National University, Australia;Mark Johnson,Macquarie University",peter.anderson@gatech.edu;stephen.gould@anu.edu.au;mark.johnson@mq.edu.au,Applications/Visual Question Answering*; Algorithms/Semi-Supervised Learning; Applications/Natural Language Processing; Deep Learning -,Poster
944,3D-Aware Scene Manipulation via Inverse Graphics,"We aim to obtain an interpretable, expressive, and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous representations learned by neural networks are often uninterpretable, limited to a single object, or lacking 3D knowledge. In this work, we propose 3D scene de-rendering networks (3D-SDN) to address the above issues by integrating disentangled representations for object semantics, appearance, and geometry into a deep generative model. Our scene encoder performs inverse graphics, translating a scene into a structured object representation. Our decoder has two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, appearance, and geometry supports various 3D-aware scene manipulation applications, \eg, rotating and moving objects freely while maintaining consistent shape and texture, and changing the object appearance without affecting its shape. We systematically evaluate our model and demonstrate that our editing scheme based on 3D-SDN is superior to its 2D counterpart.
    ","Shunyu Yao,Tsinghua University;Tzu-Ming Hsu,MIT;Jun-Yan Zhu,MIT;Jiajun Wu*,MIT;Antonio Torralba,MIT;Bill Freeman,MIT;Joshua Tenenbaum,MIT",shunyuyao.cs@gmail.com;stmharry@mit.edu;junyanz@mit.edu;jiajunwu.cs@gmail.com;torralba@mit.edu;billf@mit.edu;jbt@mit.edu,Applications/Visual Scene Analysis and Interpretation*; Applications/Computer Vision; Neuroscience and Cognitive Science/Visual Perception,Poster
959,Random Feature Stein Discrepancies,"Computable Stein discrepancies (SDs) have been deployed for a variety of applications, ranging from sampler selection in posterior inference to goodness-of-fit testing.  Existing convergence-determining SDs admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time SDs have been proposed for goodness-of-fit testing, they exhibit significant degradations in testing power---even when power is explicitly optimized. To address these shortcomings, we introduce feature Stein discrepancies (FSDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct FSDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations---random FSDs (RFSDs)---which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, RFSDs typically perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute.","Jonathan Huggins*,Harvard;Lester Mackey,Microsoft Research",jhuggins@mit.edu;lmackey@microsoft.com,Algorithms/Kernel Methods,Poster
962,Distributed Stochastic Optimization via Adaptive SGD,"Stochastic convex optimization algorithms are the most popular way to train machine learning models on large-scale data. Scaling up the training process of these models is crucial, but the most popular algorithm, Stochastic Gradient Descent (SGD), is a serial method that is surprisingly hard to parallelize. In this paper, we propose an efficient distributed stochastic optimization method by combing adaptive step sizes with variance reduction techniques. We achieve a linear speedup in the number of machines, constant memory footprint, and only a logarithmic number of communication rounds. Critically, our approach is a black-box reduction that parallelizes any serial SGD algorithm, allowing us to leverage the significant progress that has been made in designing adaptive SGD algorithms. In particular, we achieve optimal convergence rates without any prior knowledge of smoothness parameters, yielding a more robust algorithm that reduces the need for hyperparameter tuning. We implement our algorithm in the Spark distributed framework and exhibit dramatic performance gains on large-scale logistic regression problems.","Ashok Cutkosky*,Stanford;Robert Busa-Fekete,Yahoo! Research",ashokc@cs.stanford.edu;busarobi@gmail.com,Optimization/Convex Optimization*; Algorithms/Online Learning,Poster
968,Precision and Recall for Time Series,"Classical anomaly detection is principally concerned with point-based anomalies, those anomalies that occur at a single point in time. Yet, many real-world anomalies are range-based, meaning they occur over a period of time. In this paper, we present a new model that more accurately measures the correctness of anomaly detection systems for range-based anomalies, while subsuming the classical model's ability to classify point-based anomaly detection systems.","Nesime Tatbul*,Intel Labs and MIT;Tae Jun Lee,Microsoft;Stan Zdonik,Brown University;Mejbah Alam,Intel Labs;Justin Gottschlich,Intel Labs",tatbul@csail.mit.edu;tae_jun_lee@alumni.brown.edu;sbz@cs.brown.edu;mejbah.alam@intel.com;justin.gottschlich@intel.com,"Applications/Time Series Analysis*; Algorithms/Classification; Applications/Activity and Event Recognition; Data, Competitions, Im",Spotlight
979,Deep Attentive Tracking via Reciprocative Learning,"Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporal robust features. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training. The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches.","Shi Pu*,Beijing University of Posts and Telecommunications;Yibing Song,Tencent AI Lab;Chao Ma,University of Adelaide;Honggang Zhang,Beijing University of Posts and Telecommunications;Ming-Hsuan Yang,University of California at Merced",pushi_519200@bupt.edu.cn;dynamicstevenson@gmail.com;c.ma@adelaide.edu.au;zhhg@bupt.edu.cn;mhyang@ucmerced.edu,Applications/Tracking and Motion in Video*; Applications/Computer Vision,Poster
980,Virtual Class Enhanced Discriminative Embedding Learning,"Recently, learning discriminative features to improve the recognition performances gradually becomes the primary goal of deep learning, and numerous remarkable works have emerged. In this paper, we propose a novel yet extremely simple method Virtual Softmax to enhance the discriminative property of learned features by injecting a dynamic virtual negative class into the original softmax. Injecting virtual class aims to enlarge inter-class margin and compress intra-class distribution by strengthening the decision boundary constraint. Although it seems weird to optimize with this additional virtual class, we show that our method derives from an intuitive and clear motivation, and it indeed encourages the features to be more compact and separable. This paper empirically and experimentally demonstrates the superiority of Virtual Softmax, improving the performances on a variety of object classification and face verification tasks.","Binghui Chen*,BUPT;Weihong Deng,Beijing University of Posts and Telecommunications;Haifeng Shen,AI Labs, Didi Chuxing",chenbinghui@bupt.edu.cn;whdeng@bupt.edu.cn;shenhaifeng@didichuxing.com,Applications*; Applications/Computer Vision; Applications/Object Recognition; Deep Learning; Deep Learning ->,Spotlight
981,Attention in Convolutional LSTM for Gesture Recognition,"Convolutional long short-term memory (LSTM) networks have been widely used for action/gesture recognition, and different attention mechanisms have also been embedded into the LSTM or the convolutional LSTM (ConvLSTM) networks. Based on the previous gesture recognition architectures which combine the three-dimensional convolution neural network (3DCNN) and ConvLSTM, this paper explores the effects of attention mechanism in ConvLSTM. Several variants of ConvLSTM are evaluated: (a) Removing the convolutional structures of the three gates in ConvLSTM, (b) Applying the attention mechanism on the input of ConvLSTM, (c) Reconstructing the input and (d) output gates respectively with the modified channel-wise attention mechanism. The evaluation results demonstrate that the spatial convolutions in the three gates scarcely contribute to the spatiotemporal feature fusion, and the attention mechanisms embedded into the input and output gates cannot improve the feature fusion. In other words, ConvLSTM mainly contributes to the temporal fusion along with the recurrent steps to learn the long-term spatiotemporal features, when taking as input the spatial or spatiotemporal features. On this basis, a new variant of LSTM is derived, in which the convolutional structures are only embedded into the input-to-state transition of LSTM. The code of the LSTM variants is publicly available.","Liang Zhang,School of Computer Science and Technology, Xidian University, China;Guangming Zhu*,School of Computer Science and Technology, Xidian University, China;Lin Mei,The Third Research Institute of Ministry of Public Security, China;Peiyi Shen,School of Computer Science and Technology, Xidian University, China;Syed Afaq Ali  Shah,Department of Computer Science and Software Engineering, The University of Western Australia;Mohammed Bennamoun,University of Western Australia",liangzhang@xidian.edu.cn;gmzhu@xidian.edu.cn;l_mei72@hotmail.com;pyshen@xidian.edu.cn;afaq.shah@uwa.edu.au;mohammed.bennamoun@uwa.edu.au,"Applications/Body Pose, Face, and Gesture Analysis*; Deep Learning/Attention Models",Poster
988,Pelee: A Real-Time Object Detection System on Mobile Devices,"An increasing need of running Convolutional Neural Network (CNN) models on mobile devices with limited computing power and memory resource encourages studies on efficient model design. A number of efficient architectures have been proposed in recent years, for example, MobileNet, ShuffleNet, and NASNet-A. However, all these models are heavily dependent on depthwise separable convolution which lacks efficient implementation in most deep learning frameworks. In this study, we propose an efficient architecture named PeleeNet, which is built with conventional convolution instead. On ImageNet ILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy by 1.8% (72.4% vs. 70.6%) and 23% faster speed than MobileNet, the state-of-the-art efficient architecture. Meanwhile, PeleeNet is only 66% of the model size of MobileNet. We then propose a real-time object detection system by combining PeleeNet with Single Shot MultiBox Detector (SSD) method and optimizing the architecture for fast speed. Our proposed detection system, named Pelee, achieves 76.4% mAP (mean average precision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of 23.6 FPS on iPhone 8 and 74 FPS on NVIDIA TX2. The result on COCO outperforms YOLOv2 in consideration of a higher precision, 13.6 times lower computational cost and 11.3 times smaller model size. The code and models are open sourced.","Jun Wang*,University of Western Ontario;Xiang Li,Western University;Charles Ling,Western University",jwan563@uwo.ca;lxiang2@uwo.ca;charles.ling@uwo.ca,Deep Learning/CNN Architectures*; Applications/Computer Vision; Applications/Object Detection; Deep Learning/Efficient Infere,Poster
991,Universal Growth in Production Economies,"We study a simple variant of the von Neumann model of an expanding economy, in which multiple producers produce goods according to their production function. The players trade their goods at the market and then use the bundles received as inputs for the production in the next round. We show that a simple decentralized dynamic, where players update their  bids on the goods in the market proportionally to how useful the investments were, leads to growth of the economy in the long term (whenever growth is possible) but also creates unbounded inequality, i.e. very rich and very poor players emerge. We analyze several other phenomena, such as how the relation of a player with others influences its development and the Gini index of the system.","Simina Branzei*,Purdue University;Ruta Mehta,UIUC;Noam Nisan,Hebrew University of Jerusalem and Microsoft Research",simina.branzei@gmail.com;mehta.ruta@gmail.com;noam.nisan@gmail.com,Theory/Game Theory and Computational Economics*; Algorithms/Dynamical Systems; Probabilistic Methods/Distributed Inference,Poster
993,Bayesian Model Selection Approach to Boundary Detection with Non-Local Priors,"We propose a Bayesian model selection (BMS) boundary detection procedure using
non-local prior distributions for a sequence of data with multiple systematic mean changes.  By using the non-local priors in the Bayesian model selection framework, the BMS method can effectively suppress the non-boundary spike
points with large instantaneous changes. Further, we speed up the algorithm by
reducing the multiple change points to a series of single change point detection problems. We establish the consistency of the estimated number and
locations of the change points under various prior distributions. Extensive simulation studies  are conducted to compare the BMS with existing methods, and our method is  illustrated with application to the magnetic resonance imaging guided radiation therapy data.","Fei Jiang*,The University of Hong Kong;Guosheng Yin,University of Hong Kong;Francesca Dominici,Harvard University",feijiang@hku.hk;gyin@hku.hk;fdominic@hsph.harvard.edu,Probabilistic Methods/Bayesian Theory*; Applications/Signal Processing,Poster
997,Efficient Stochastic Gradient Hard Thresholding,"Stochastic gradient hard thresholding methods have recently been shown to work favorably in solving large-scale empirical risk minimization problems under sparsity or rank constraint. Despite the improved iteration complexity over full gradient methods, the gradient evaluation and hard thresholding complexity of the existing stochastic algorithms usually scales linearly with data size, which could still be expensive when data is huge and the hard thresholding step could be as expensive as singular value decomposition in rank-constrained problems. To address these deficiencies, we propose an efficient hybrid stochastic gradient hard thresholding (HSG-HT) method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds. Specifically, we prove that the stochastic gradient evaluation complexity of HSG-HT scales linearly with inverse of sub-optimality and its hard thresholding complexity scales logarithmically. By applying the heavy ball acceleration technique, we further propose an accelerated variant of HSG-HT which can be shown to have improved factor dependence on restricted condition number. Numerical results confirm our theoretical affirmation and demonstrate the computational efficiency of the proposed methods.","Pan Zhou*,NUS;Xiaotong Yuan,Nanjing University of Information Science and Technology;Jiashi Feng,NUS",pzhou@u.nus.edu;xtyuan1980@gmail.com;elefjia@nus.edu.sg,Algorithms*; Algorithms/Sparsity and Compressed Sensing,Poster
1006,SplineNets: Continuous Neural Decision Graphs,"We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). Our method dramatically reduces runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. Thus, functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e., conditioned on the computational path). In particular, we embed weights of functions on smooth, low dimensional manifolds parameterized by compact B-splines, and define decisions as choosing a position on these hyper-surfaces. We further show that by maximizing the mutual information between these latent coordinates and data or its labels, the network can be optimally utilized and specialized. Experiments on various image classification datasets show the power of this new paradigm over regular CNNs. ","Cem Keskin*,Google;Shahram Izadi,Google",cemkeskin@google.com;shahrami@google.com,Deep Learning/CNN Architectures*; Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning; Algorithms/Structured Pre,Poster
1008,Generalized Zero-Shot Learning with Deep Calibration Network,"A technical challenge of deep learning is recognizing target classes without seen data. Zero-shot learning leverages semantic representations such as attributes or class prototypes to bridge source and target classes. Existing standard zero-shot learning methods may be prone to overfitting the seen data of source classes as they are blind to the semantic representations of target classes. In this paper, we study generalized zero-shot learning that assumes accessible to target classes for unseen data during training, and prediction on unseen data is made by searching on both source and target classes. We propose a novel Deep Calibration Network (DCN) approach towards this generalized zero-shot learning paradigm, which enables simultaneous calibration of deep networks on the confidence of source classes and uncertainty of target classes. Our approach maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of seen data to both source and target classes are maximized. We show superior accuracy of our approach over the state of the art on benchmark datasets for generalized zero-shot learning, including AwA, CUB, SUN, and aPY.","Shichen Liu,Tsinghua University;Mingsheng Long*,Tsinghua University;Jianmin Wang,""Tsinghua University, China"";Michael Jordan,UC Berkeley",liushichen95@gmail.com;longmingsheng@gmail.com;jimwang@tsinghua.edu.cn;jordan@cs.berkeley.edu,Deep Learning/Few-Shot Learning Approaches,Poster
1012,Neural Architecture Search with Bayesian Optimisation and Optimal Transport,"Bayesian Optimisation (BO) refers to a class of methods for global optimisation
of a function f which is only accessible via point evaluations. It is
typically used in settings where f is expensive to evaluate. A common use case
for BO in machine learning is model selection, where it is not possible to
analytically model the generalisation performance of a statistical model, and
we resort to noisy and expensive training and validation procedures to choose
the best model. Conventional BO methods have focused on Euclidean and
categorical domains, which, in the context of model selection, only permits
tuning scalar hyper-parameters of machine learning algorithms. However, with
the surge of interest in deep learning, there is an increasing demand to tune
neural network architectures. In this work, we develop NASBOT, a Gaussian
process based BO framework for neural architecture search. To accomplish this,
we develop a distance metric in the space of neural network architectures which
can be computed efficiently via an optimal transport program. This distance
might be of independent interest to the deep learning community as it may find
applications outside of BO. We demonstrate that NASBOT outperforms other
alternatives for architecture search in several cross validation based model
selection tasks on multi-layer perceptrons and convolutional neural networks.","Kirthevasan Kandasamy*,Carnegie Mellon University;Willie Neiswanger,Carnegie Mellon University;Jeff Schneider,(organization);Barnabas Poczos,	Carnegie Mellon University;Eric Xing,Petuum Inc. and CMU",kandasamy@cs.cmu.edu;willie@cs.cmu.edu;schneide@cs.cmu.edu;bapoczos@cs.cmu.edu;epxing@cs.cmu.edu,Algorithms/AutoML*; Probabilistic Methods/Gaussian Processes,Spotlight
1018,Embedding Logical Queries on Knowledge Graphs,"Learning low-dimensional embeddings of knowledge graphs is a powerful approach used to predict unobserved or missing edges between entities. However, an open challenge in this area is developing techniques that can go beyond simple edge prediction and handle more complex logical queries, which might involve multiple unobserved edges, entities, and variables. For instance, given an incomplete biological knowledge graph, we might want to predict ""em what drugs are likely to target proteins involved with both diseases X and Y?"" -- a query that requires reasoning about all possible proteins that might interact with diseases X and Y. Here we introduce a framework to efficiently make predictions about conjunctive logical queries -- a flexible but tractable subset of first-order logic -- on incomplete knowledge graphs. In our approach, we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. By performing logical operations within a low-dimensional embedding space, our approach achieves a time complexity that is linear in the number of query variables, compared to the exponential complexity required by a naive enumeration-based approach. We demonstrate the utility of this framework in two application studies on real-world datasets with millions of relations: predicting logical relationships in a network of drug-gene-disease interactions and in a graph-based representation of social interactions derived from a popular web forum.","William Hamilton*,Stanford University;Payal Bajaj,Stanford University;Marinka Zitnik,Stanford;Dan Jurafsky,Stanford University;Jure Leskovec,Stanford",will.leif.hamilton@gmail.com;pbajaj@stanford.edu;marinka@cs.stanford.edu;jurafsky@stanford.edu;jure@cs.stanford.edu,Deep Learning/Embedding Approaches*; Algorithms/Relational Learning; Algorithms/Representation Learning; Applications/Network An,Poster
1022,Learning Optimal Reserve Price against Non-myopic Bidders,"We consider the problem of learning optimal reserve price in repeated auctions against non-myopic bidders, who may bid strategically in order to gain in future rounds even if the single-round auctions are truthful. Previous algorithms, e.g., empirical pricing, do not provide non-trivial regret rounds in this setting in general. We introduce algorithms that obtain small regret against non-myopic bidders either when the market is large, i.e., no bidder appears in a constant fraction of the rounds, or when the bidders are impatient, i.e., they discount future utility by some factor mildly bounded away from one. Our approach carefully controls what information is revealed to each bidder, and builds on techniques from differentially private online learning as well as the recent line of works on jointly differentially private algorithms.","Jinyan Liu*,The University of Hong Kong;Zhiyi Huang,University of Hong Kong;Xiangning Wang,The University of Hong Kong",jyliu678@gmail.com;zhiyi@cs.hku.hk;wangxn@hku.hk,Theory*; Algorithms/Online Learning; Theory/Game Theory and Computational Economics,Poster
1023,Sequential Context Encoding for Duplicate Removal,"Duplicate removal is a critical step to accomplish a reasonable amount of predictions in prevalent proposal-based object detection frameworks. Albeit simple and effective, most previous algorithms utilized a greedy process without making sufficient use of properties of input data. In this work, we design a new two-stage framework to effectively select the appropriate proposal candidate for each object. The first stage suppresses most of easy negative object proposals, while the second stage selects true positives in the reduced proposal set. These two stages share the same network structure, an encoder and a decoder formed as recurrent neural networks (RNN) with global attention and context gate. The encoder scans proposal candidates in a sequential manner to capture the global context information, which is then fed to the decoder to extract optimal proposals. In our extensive experiments, the proposed method outperforms other alternatives by a large margin.","Lu Qi*,The Chinese University of Hong Kong;Shu Liu,Chinese University of Hong Kong;Jianping Shi,Sensetime Group Limited;Jiaya Jia,Chinese University of Hong Kong",luqi@cse.cuhk.edu.hk;liushuhust@gmail.com;shijianping@sensetime.com;leojia@cse.cuhk.edu.hk,Applications/Object Detection*; Applications/Computer Vision; Applications/Object Recognition,Poster
1026,Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning,"This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific keypoints, along with their detectors to predict 3D keypoints in a single 2D input image. We demonstrate this framework on 3D pose estimation task by proposing a differentiable pose objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our network automatically discovers a consistent set of keypoints across viewpoints of a single object as well as across all object instances of a given object class. Importantly, we find that our end-to-end approach using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture for the pose estimation task. 
The discovered 3D keypoints across the car, chair, and plane
categories of ShapeNet are visualized at https://keypoints.github.io/","Supasorn Suwajanakorn*,UW/Google;Noah Snavely,Google;Jonathan Tompson,Google;Mohammad Norouzi,Google Brain",supasorn@gmail.com;snavely@google.com;tompson@google.com;mnorouzi@google.com,Applications/Computer Vision,Oral
1058,Nonparametric learning for Bayesian models via randomized objective functions,"We present a Bayesian nonparametric (NP) approach to learning from data that is centered around a conventional probabilistic model, but does not assume that this model is true. This affords a trivially parallelizable, scalable Monte Carlo sampling scheme based on the notion of randomized objective functions,  which map posterior samples from the baseline model into posterior samples from the NP update. This is particularly attractive for regularizing NP methods or correcting approximate models, such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.","Simon Lyddon*,University of Oxford;Stephen Walker,University of Texas at Austin;Chris Holmes,University of Oxford",lyddon@stats.ox.ac.uk;s.g.walker@math.utexas.edu;cholmes@stats.ox.ac.uk,Probabilistic Methods/Bayesian Nonparametrics*; Probabilistic Methods/Variational Inference,Poster
1069,SEGA: Variance Reduction via Gradient Sketching,"We propose a novel randomized first order optimization method---SEGA (SkEtched GrAdient method)---which progressively throughout its iterations builds a variance-reduced estimate of the gradient from random linear measurements (sketches) of the gradient provided  at each iteration by an oracle. In each iteration, SEGA updates the current estimate of the gradient  through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent methods, such as coordinate descent, SEGA can be used for optimization problems with  a non-separable proximal term. We provide a general convergence analysis and prove linear convergence for strongly convex objectives. In the special case of  coordinate sketches, SEGA can be enhanced with various techniques such as importance sampling, minibatching and acceleration, and its rate is up to a small constant factor identical to the best-known rate of coordinate descent. ","Filip Hanzely*,KAUST;Konstantin Mishchenko,King Abdullah University of Science & Technology (KAUST);Peter Richtarik,KAUST",filip.hanzely@kaust.edu.sa;konstantin.mishchenko@kaust.edu.sa;richtarik@gmail.com,Optimization/Convex Optimization*; Algorithms; Algorithms/Stochastic Methods; Optimization,Poster
1073,Automatic Program Synthesis of Long Programs with a Learned Garbage Collector,"We consider the problem of generating automatic code given sample input-output pairs. We train a neural network to map from the current state and the outputs to the program's next statement. The neural network optimizes multiple tasks concurrently: The next operation out of a set of high level commands, the operands of the next statement, and which variables can be dropped from memory. Using our method we are able to create programs that are more than twice as long as existing state-of-the-art solutions, while improving the success rate for comparable lengths, and cutting the run-time by two orders of magnitude. ","Amit Zohar*,Tel Aviv Universtiy;Lior Wolf,Tel Aviv University, Israel",amit.zhr@gmail.com;wolf@cs.tau.ac.il,Deep Learning/Program Induction,Poster
1077,One-Shot Unsupervised Cross Domain Translation,"Given a single image $x$ from domain $A$ and a set of images from domain $B$, our task is to generate the analogous of $x$ in $B$. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain $B$ is trained. Then, given the new sample $x$, we create a variational autoencoder for domain $A$ by adapting the layers that are close to the image in order to directly fit $x$, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample $x$, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain $A$. Our code will be made publicly available. ","Sagie Benaim,Tel Aviv University;Lior Wolf*,Tel Aviv University, Israel",sagiebenaim@gmail.com;wolf@cs.tau.ac.il,Algorithms/Unsupervised Learning*; Applications/Computer Vision,Poster
1078,Regularizing by the Variance of the Activations' Sample-Variances,"Normalization techniques play an important role in supporting efficient and often more effective training of deep neural networks. While conventional methods explicitly normalize the activations, we suggest to add a loss term instead. This new loss term encourages the variance of the activations to be stable and not vary from one random mini-batch to the next. As we prove, this encourages the activations to be distributed around a few distinct modes. We also show that if the inputs are from a mixture of two Gaussians, the new loss would either join the two together, or separate between them optimally in the LDA sense, depending on the prior probabilities. Finally, we are able to link the new regularization term to the batchnorm method, which provides it with a regularization perspective. Our experiments demonstrate an improvement in accuracy over the batchnorm technique for both CNNs and fully connected networks. ","Etai Littwin,;Lior Wolf*,Tel Aviv University, Israel",etai.littwin@gmail.com;wolf@cs.tau.ac.il,Deep Learning/Efficient Training Methods,Poster
1080,"Overlapping Clustering, and One (class) SVM to Bind Them All","People belong to multiple communities, words belong to multiple topics, and books cover multiple genres; overlapping clusters are commonplace. Many existing overlapping clustering methods model each person (or word, or book) as a non-negative weighted combination of ""exemplars"" who belong solely to one community, with some small noise. Geometrically, each person is a point on a cone whose corners are these exemplars. This basic form encompasses the widely used Mixed Membership Stochastic Blockmodel of networks and its degree-corrected variants, as well as topic models such as LDA. We show that a simple one-class SVM yields provably consistent parameter inference for all such models, and scales to large datasets. Experimental results on several simulated and real datasets show our algorithm (called \svmcone) is both accurate and scalable.","Xueyu  Mao*,University of Texas at Austin;Purnamrita Sarkar,University of Texas at Austin;Deepayan Chakrabarti,University of Texas at Austin",xmao@cs.utexas.edu;purna.sarkar@austin.utexas.edu;deepay@utexas.edu,Theory/Frequentist Statistics*; Algorithms/Clustering; Applications/Network Analysis; Theory/Large Deviations and Asymptoti,Spotlight
1093,Algorithmic Linearly Constrained Gaussian Processes,"We algorithmically construct multi-output Gaussian process priors which satisfy linear differential equations. Our approach attempts to parametrize all solutions of the equations using Gr√∂bner bases. If successful, a push forward Gaussian process along the paramerization is the desired prior. We consider several examples from physics, geomathmatics and control, among them the full inhomogeneous system of Maxwell's equations. By bringing together stochastic learning and computeralgebra in a novel way, we combine noisy observations with precise algebraic computations.","Markus Lange-Hegermann*,Hochschule Ostwestfalen-Lippe",markus.lange-hegermann@hs-owl.de,Probabilistic Methods/Gaussian Processes*; Algorithms/Kernel Methods; Algorithms/Regression; Algorithms/Stochastic Methods; Probabilis,Poster
1098,DeepExposure: Learning to Expose Photos with Asynchronously Reinforced Adversarial Learning,"The accurate exposure is the key of capturing high-quality photos in computational
photography, especially for mobile phones that are limited by sizes of camera
modules. Inspired by exposure blending with luminosity masks usually applied
by professional photographers, in this paper, we develop a novel algorithm for
learning local exposures with deep reinforcement learning and adversarial learning.
To be specific, we segment an image into sub-images that can reflect variations
of dynamic range exposures according to raw low-level features. Based on these
sub-images, a local exposure for each sub-image is automatically learned by
virtue of policy network sequentially while the reward of learning is globally
designed for striking a balance of overall exposures. The aesthetic evaluation
function is approximated by discriminator in generative adversarial networks. The
reinforcement learning and the adversarial learning are trained collaboratively by
asynchronous deterministic policy gradient and generative loss approximation.
To further simplify the algorithmic architecture, we also prove the feasibility of
leveraging the discriminator as the value function. Further more, we employ
each local exposure to retouch the raw input image respectively, thus delivering
multiple retouched images under different exposures which are fused with exposure
blending. The extensive experiments verify that our algorithms are superior to
state-of-the-art methods in terms of quantitative accuracy and visual illustration.","Runsheng Yu,South China Normal University;Wenyu Liu,Peking University;Yasen Zhang,Xiaomi AI Lab;Zhi Qu,Xiaomi AI Lab;Deli Zhao*,Xiaomi AI Lab;Bo Zhang,Xiaomi AI Lab",runshengyu@gmail.com;liuwenyu@pku.edu.cn;zhangyasen@xiaomi.com;quzhi@xiaomi.com;zhaodeli@gmail.com;zhangbo@xiaomi.com,Applications/Computational Photography*; Applications/Computer Vision; Reinforcement Learning and Planning/Reinforcement Learning,Poster
1099,Norm matters: efficient and accurate normalization schemes in deep networks,"Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used $L^2$ batch-norm, using normalization in $L^1$ and $L^\infty$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.","Elad Hoffer*,Technion;Ron Banner,Intel;Itay Golan,Technion;Daniel Soudry,Technion",elad.hoffer@gmail.com;ron.banner@intel.com;itaygolan@gmail.com;daniel.soudry@gmail.com,Deep Learning*; Deep Learning/CNN Architectures; Deep Learning/Efficient Training Methods,Spotlight
1105,Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms,"Recent methods for learning a linear subspace from data corrupted by outliers are based on convex L1 and nuclear norm optimization and require the dimension of the subspace and the number of outliers to be sufficiently small [1]. In sharp contrast, the recently proposed Dual Principal Component Pursuit (DPCP) method [2] can provably handle subspaces of high dimension by solving a non-convex L1 optimization problem on the sphere. However, its geometric analysis is based on quantities that are difficult to interpret and are not amenable to  statistical analysis. In this paper we provide a refined geometric analysis and a new statistical analysis that show that DPCP can tolerate as many outliers as the square of the number of inliers, thus improving upon other provably correct robust PCA methods. We also propose a scalable Projected Sub-Gradient Descent method (DPCP-PSGD) for solving the DPCP problem and show it admits linear convergence even though the underlying optimization problem is non-convex and non-smooth. Experiments on road plane detection from 3D point cloud data demonstrate that DPCP-PSGD can be more efficient than the traditional RANSAC algorithm, which is one of the most popular methods for such computer vision applications.","Zhihui Zhu*,Johns Hopkins University;Yifan Wang,ShanghaiTech University;Daniel Robinson,Johns Hopkins University;Daniel Naiman,Johns Hopkins University;Rene Vidal,Johns Hopkins University;Manolis Tsakiris,ShanghaiTech University",zhihuizhu90@gmail.com;yifan1@cs.washington.edu;daniel.p.robinson@gmail.com;daniel.naiman@jhu.edu;rvidal@cis.jhu.edu;endomorphism83@gmail.com,"Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA)*; Optimization/Non-Convex Optimization",Poster
1106,MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval,"This paper addresses the general problem of blind echo retrieval, i.e., given M sensors measuring in the discrete-time domain M mixtures of K delayed and attenuated copies of an unknown source signal, can the echo location and weights be recovered? This problem has broad applications in fields such as sonars, seismology, ultrasounds or room acoustics. It belongs to the broader class of blind channel identification problems, which have been intensively studied in signal processing. All existing methods proceed in two steps: (i) blind estimation of sparse discrete-time filters and (ii) echo information retrieval by peak picking. The precision of these methods is fundamentally limited by the rate at which the signals are sampled: estimated echo locations are necessary on-grid, and since true locations never match the sampling grid, the weight estimation precision is also strongly limited. This is the so-called basis-mismatch problem in compressed sensing. We propose a radically different approach to the problem, building on top of the framework of finite-rate-of-innovation sampling. The approach operates directly in the parameter-space of echo locations and weights, and enables near-exact blind and off-grid echo retrieval from discrete-time measurements. It is shown to outperform conventional methods by several orders of magnitudes in precision.","Helena Peic Tukuljac*,√âcole polytechnique f√©d√©rale de Lausanne;Antoine Deleforge,INRIA;Remi Gribonval,INRIA",helena.peictukuljac@epfl.ch;antoine.deleforge@inria.fr;remi.gribonval@inria.fr,Applications/Signal Processing*; Algorithms/Sparsity and Compressed Sensing; Algorithms/Spectral Methods,Poster
1112,Mixture Matrix Completion,"Completing a data matrix X has become an ubiquitous problem in modern data science, with motivations in recommender systems, computer vision, and networks inference, to name a few. One typical assumption is that X is low-rank. A more general model assumes that each column of X corresponds to one of several low-rank matrices. This paper generalizes these models to what we call mixture matrix completion (MMC): the case where each entry of X corresponds to one of several low-rank matrices. MMC is a more accurate model for recommender systems, and brings more flexibility to other completion and clustering problems. We make four fundamental contributions about this new model. First, we show that MMC is theoretically possible (well-posed). Second, we give its precise information-theoretic identifiability conditions. Third, we derive the sample complexity of MMC. Finally, we give a practical algorithm for MMC with performance comparable to the state-of-the-art for simpler related problems, both on synthetic and real data.","Daniel Pimentel-Alarcon*,Georgia State University",pimentel@gsu.edu,Algorithms/Missing Data*; Algorithms/Classification; Algorithms/Clustering; Algorithms/Collaborative Filtering; Algor,Poster
1119,Trajectory Convolution for Action Recognition,"How to leverage the temporal dimension is a key question in video analysis. Recent work suggests an efficient approach to video feature learning, namely,
factorizing 3D convolutions into separate components respectively for spatial and temporal convolutions. The temporal convolution, however, comes with an implicit assumption ‚Äì the feature maps across time steps are well aligned so that the features at the same locations can be aggregated. This assumption may be overly strong in practical applications, especially in action recognition where the motion of people or objects is a crucial aspect. In this work, we propose a new CNN architecture TrajectoryNet, which incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the standard temporal convolution. This operation explicitly takes into account the location changes caused by deformation or motion, allowing the visual features to be aggregated along the the motion paths. On two very large-scale action recognition datasets, namely, Something-Something and Kinetics, the proposed network architecture achieves notable improvement over strong baselines.","Yue Zhao*,The Chinese University of Hong Kong;Yuanjun Xiong,Amazon;Dahua Lin,The Chinese University of Hong Kong",zy317@ie.cuhk.edu.hk;bitxiong@gmail.com;dhlin@ie.cuhk.edu.hk,Applications*; Applications/Activity and Event Recognition; Applications/Computer Vision; Applications/Vid,Poster
1122,The Description Length of Deep Learning models,"Deep learning models often have more parameters than observations, and
still perform well. This is sometimes described as a paradox. In this work,
we show experimentally that despite their huge number of parameters, deep neural
networks can compress the data losslessly \emph{even when taking the cost of encoding the
parameters into account}. Such a compression viewpoint
originally motivated the use of
\emph{variational methods} in neural networks \cite{Hinton,Schmidhuber1997}. However, we
show that these variational methods provide surprisingly poor compression
bounds, despite being explicitly built to minimize such bounds.
This might explain the relatively poor practical
performance of variational methods in deep learning. Better
encoding methods, imported from the Minimum Description Length (MDL)
toolbox, yield much better compression values on deep networks.","L√©onard Blier*,Ecole Normale Sup√©rieure;Yann Ollivier,Facebook Artificial Intelligence Research",leonard.blier@ens.fr;yol@fb.com,Theory/Information Theory*; Deep Learning; Probabilistic Methods/Variational Inference,Poster
1123,A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem,"Bandit learning is characterized by the tension between long-term exploration and short-term exploitation.  However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. In such settings, one might like to run a ``greedy'' algorithm, which always makes the optimal decision for the individuals at hand --- but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm.

We give a smoothed analysis, showing that even when contexts may be chosen by an adversary, small perturbations of the adversary's choices suffice for the algorithm to achieve ``no regret'', perhaps (depending on the specifics of the setting) with a constant amount of initial training data.  This suggests that in slightly perturbed environments, exploration and exploitation need not be in conflict in the linear setting.","Sampath Kannan,University of Pennsylvania;Jamie Morgenstern*,Georgia Tech;Aaron Roth,University of Pennsylvania;Bo Waggoner,UPenn;Zhiwei  Steven Wu,University of Minnesota",kannan@cis.upenn.edu;jamiemor@cis.upenn.edu;aaroth@cis.upenn.edu;bwag@seas.upenn.edu;steven7woo@gmail.com,"Algorithms/Bandit Algorithms*; Algorithms/Online Learning; Applications/Fairness, Accountability, and Transparency; Theory ->",Spotlight
1125,Revisiting Decomposable Submodular Function Minimization with Incidence Relations,"We introduce a new approach to decomposable submodular function minimization (DSFM) that exploits incidence relations. Incidence relations describe which variables effectively influence the component functions, and when properly utilized, they allow for improving the convergence rates of DSFM solvers. Our main results include the precise parametrization of the DSFM problem based on incidence relations, the development of new scalable alternative projections and parallel coordinate descent methods and an accompanying rigorous analysis of their convergence rates. ","Pan Li*,UIUC;Olgica Milenkovic,University of Illinois UC",panli2@illinois.edu;milenkov@illinois.edu,Optimization/Submodular Optimization*; Optimization/Combinatorial Optimization; Optimization/Convex Optimization,Poster
1141,A Practical Algorithm for Distributed Clustering and Outlier Detection,"We study the classic k-means/median clustering, which are fundamental problems in unsupervised learning, in the setting where data are partitioned across multiple sites, and where we are allowed to discard a small portion of the data by labeling them as outliers.  We propose a simple approach based on constructing small summary for the original dataset. The proposed method is time and communication efficient, has good approximation guarantees, and can identify the global outliers effectively.  
To the best of our knowledge, this is the first practical algorithm with theoretical guarantees for distributed clustering with outliers. Our experiments on both real and synthetic data have demonstrated the clear superiority of our algorithm against all the baseline algorithms in almost all metrics. ","Jiecao Chen,Indiana University Bloomington;Erfan Sadeqi Azer,Indiana University;Qin Zhang*,Indiana University Bloomington",chenjiecao@gmail.com;esadeqia@indiana.edu;qzhangcs@indiana.edu,Algorithms/Clustering,Poster
1143,Learning to Reconstruct Shapes from Unseen Categories,"From a single view, humans are able to hallucinate the full 3D shape of the object in the image, even if it is from a novel, unseen category. Contemporary AI systems for single-image 3D reconstruction often lack this ability, because the shape priors they learned is often tied to the training object classes. In this paper, we study the task of single-image 3D reconstruction, but attempting to recover the full 3D shape of an object outside the training categories. Our model combines 2.5D sketches (depths and silhouettes), spherical shape representations, and 3D voxels in a principled manner. Experiments demonstrate that it achieves state-of-the-art results on generalizing to diverse novel object categories.","Xiuming Zhang,MIT;Zhoutong Zhang,MIT;Chengkai Zhang,MIT;Joshua Tenenbaum,MIT;Bill Freeman,MIT;Jiajun Wu*,MIT",xiuming@mit.edu;ztzhang@mit.edu;ckzhang@mit.edu;jbt@mit.edu;billf@mit.edu;jiajunwu.cs@gmail.com,Applications/Object Recognition*; Applications/Computer Vision; Neuroscience and Cognitive Science/Reasoning; Neuroscience and C,Oral
1146,BourGAN: Generative Networks with Metric Embeddings,"This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens, we embed subsamples of the dataset from an arbitrary metric space into the L2 space, while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically, it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis, and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples, outperforming several recent GAN variants on a number of metrics and offering new features.","Chang Xiao*,Columbia University;Peilin Zhong,Columbia University;Changxi Zheng,Columbia University",chang@cs.columbia.edu;pz2225@columbia.edu;cxz@cs.columbia.edu,Deep Learning*; Deep Learning/Adversarial Networks; Deep Learning/Embedding Approaches; Deep Learning/Gener,Spotlight
1147,Smoothed analysis of the low-rank approach for smooth semidefinite programs,"  We consider semidefinite programs (SDPs) of size $n$ with equality constraints. 
 In order to overcome the scalability issues arising for large instances, Burer and Monteiro proposed a factorized approach based on optimizing over a matrix $Y$ of size $n\times k$ such that $X=YY^*$ is the SDP variable. The advantages of such formulation are twofold: the dimension of the optimization variable is reduced and positive semidefiniteness is naturally enforced. However, problem in $Y$ is non-convex. In prior work, it has been shown that, when the constraints on the factorized variable regularly define a smooth manifold, almost all second-order stationary points (SOSPs) are optimal. Nevertheless, in practice, one can only compute points which approximately satisfy necessary optimality conditions, so that it is crucial to know whether such points are also approximately optimal. To this end, and under similar assumptions, we use smoothed analysis to show that ASOSPs for a randomly perturbed objective function are approximate global optima, as long as the number of constraints scales sub-quadratically with the desired rank of the optimal solution. In this setting, an approximate optimum $Y$ maps to the approximate optimum $X=YY^*$ of the  SDP. We particularize our results to SDP relaxations of phase retrieval.","Thomas Pumir*,Princeton University;Samy Jelassi,Princeton University;Nicolas Boumal,Princeton",tpumir@princeton.edu;sjelassi@princeton.edu;nboumal@math.princeton.edu,Optimization*; Optimization/Convex Optimization; Optimization/Non-Convex Optimization,Oral
1148,Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning,"Object-oriented representations in reinforcement learning have shown promise in transfer learning, with previous research introducing a propositional object-oriented framework that has provably efficient learning bounds with respect to sample complexity. However, this framework has limitations in terms of the classes of tasks it can efficiently learn. In this paper we introduce a novel deictic object-oriented framework that has provably efficient learning bounds and can solve a broader range of tasks. Additionally, we show that this framework is capable of zero-shot transfer of transition dynamics across tasks and demonstrate this empirically for the Taxi and Sokoban domains.","Ofir Marom*,University of the Witwatersrand;Benjamin Rosman,CSIR",ofiremarom@gmail.com;benjros@gmail.com,Reinforcement Learning and Planning/Model-Based RL*; Algorithms/Multitask and Transfer Learning,Poster
1156,Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate,"Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for ``overfitted'' / interpolated classifiers appears to be  ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is robust  even when the data contain large amounts of label noise. 

Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including  geometric simplicial interpolation algorithm and weighted $k$-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in  classification and regression problems. These schemes have an inductive bias that benefits from higher dimension, a kind of ``blessing of dimensionality''. Finally, connections to kernel machines, random forests, and adversarial examples in the interpolated regime are discussed.","Mikhail Belkin,Ohio State University;Daniel Hsu*,Columbia University;Partha Mitra,Cold Spring Harbor Laboratory",mbelkin@cse.ohio-state.edu;djhsu@cs.columbia.edu;mitra@cshl.edu,Theory*; Theory/Learning Theory,Poster
1162,Breaking the Span Assumption Yields Fast Finite-Sum Minimization,"In this paper, we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of $n$ smooth functions, such as SAGA, SAG, SDCA, and SDCA without duality. Most finite sum algorithms follow what we call the ``span assumption'': Their updates are in the span of a sequence of component gradients chosen in a random IID fashion. In the big data regime, where the condition number $\kappa=O(n)$, the span assumption prevents algorithms from converging to an approximate solution of accuracy $\epsilon$ in less than $n\ln(1/\eps)$ iterations. SVRG and SARAH do not follow the span assumption since they are updated with a hybrid of full-gradient and component-gradient information. We show that because of this, they can be up to $\Omega(1+(\ln(n/\kappa))_+)$ times faster. In particular, to obtain an accuracy $\epsilon = 1/n^\alpha$ for $\kappa=n^\beta$ and $\alpha,\beta\in(0,1)$, modified SVRG requires $O(n)$ iterations, whereas algorithms that follow the span assumption require $\cO\p{n\ln\p{n}}$ iterations. Moreover, we present lower bound results that show this speedup is optimal, and provide analysis to help explain why this speedup exists. With the understanding that the span assumption is a point of weakness of finite sum algorithms, future work may purposefully exploit this to yield even faster algorithms in the big data regime.","Robert Hannah*,University of California, Los Angeles;Yanli Liu,University of California, Los Angeles;Daniel O'Connor,University of San Francisco;Wotao Yin,University of California, Los Angeles",roberthannah89@gmail.com;yanli.young.liu@gmail.com;daniel.v.oconnor@gmail.com;wotaoyin@math.ucla.edu,Optimization/Convex Optimization*; Theory/Computational Complexity,Poster
1169,Structured Local Minima in Sparse Blind Deconvolution,"Blind deconvolution is a ubiquitous problem of recovering two unknown signals from their convolution. Unfortunately, this is an ill-posed problem in general. This paper focuses on the {\em short and sparse} blind deconvolution problem, where the one unknown signal is short and the other one is sparsely and randomly supported. This variant captures the structure of the unknown signals in several important applications. We assume the short signal to have unit $\ell^2$ norm and cast the blind deconvolution problem as a nonconvex optimization problem over the sphere. We demonstrate that (i) in a certain region of the sphere, every local optimum is close to some shift truncation of the ground truth, and (ii) for a generic short signal of length $k$, when the sparsity of activation signal $\theta\lesssim k^{-2/3}$ and number of measurements $m\gtrsim\poly\paren{k}$, a simple initialization method together with a descent algorithm which escapes strict saddle points recovers a near shift truncation of the ground truth kernel.  ","Yuqian Zhang*,Columbia University;Han-Wen Kuo,Columbia University;John Wright,Columbia University, USA",yz2409@columbia.edu;hk2673@columbia.edu;jw2966@columbia.edu,Optimization/Non-Convex Optimization*; Algorithms/Sparsity and Compressed Sensing; Applications/Signal Processing,Poster
1179,GIANT: Globally Improved Approximate Newton Method for Distributed Optimization,"For distributed computing environment, we consider the empirical risk minimization problem and propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, which is sent to the main driver. The main driver, then, averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. GIANT is highly communication efficient and naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. Theoretically, we show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Further, and in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. We conduct large-scale experiments on a computer cluster and, empirically, demonstrate the superior performance of GIANT.","Shusen Wang*,Stevens Institute of Technology;Farbod Roosta-Khorasani,University of Queensland;Peng Xu,Stanford University;Michael Mahoney,""University of California, Berkeley""",wssatzju@gmail.com;fred.roosta@uq.edu.au;pengxu@stanford.edu;mmahoney@stat.berkeley.edu,Algorithms*; Algorithms/Classification; Algorithms/Regression; Algorithms/Stochastic Methods; Optimizati,Poster
1190,"Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data","We propose a novel class of network models for temporal dyadic interaction data. Our objective is to capture important features often observed in social interactions: sparsity, degree heterogeneity, community structure and reciprocity. We use mutually-exciting Hawkes processes to model the interactions between each (directed) pair of individuals. The intensity of each process allows interactions to arise as responses to opposite interactions (reciprocity), or due to shared interests between individuals (community structure). For sparsity and degree heterogeneity, we build the non time dependent part of the intensity function on compound random measures following Todeschini et al., 2016.  We conduct experiments on real-world temporal interaction data and show that the proposed model outperforms competing approaches for link prediction, and leads to interpretable parameters.","Xenia Miscouridou*,University of Oxford;Francois Caron,Oxford;Yee Whye Teh,University of Oxford",xenia.miscouridou@spc.ox.ac.uk;caron@stats.ox.ac.uk;y.w.teh@stats.ox.ac.uk,Probabilistic Methods/Bayesian Nonparametrics*; Applications/Network Analysis,Poster
1194,Non-monotone Submodular Maximization in Exponentially Fewer Iterations,"In this paper we consider parallelization for applications whose objective can be
expressed as maximizing a non-monotone submodular function under a cardinality constraint. Our main result is an algorithm whose approximation is arbitrarily close
to 1/2e in O(log^2 n) adaptive rounds, where n is the size of the ground set. This is an exponential speedup in parallel running time over any previously studied algorithm for constrained non-monotone submodular maximization. Beyond its provable guarantees, the algorithm performs well in practice. Specifically, experiments on traffic monitoring and personalized data summarization applications show that the algorithm finds solutions whose values are competitive with state-of-the-art algorithms while running in exponentially fewer parallel iterations.","Eric Balkanski*,Harvard;Adam Breuer,Harvard University;Yaron Singer,Harvard University",ericbalkanski@g.harvard.edu;breuer@g.harvard.edu;yaron@seas.harvard.edu,Optimization/Submodular Optimization*; Optimization/Combinatorial Optimization,Poster
1207,MetaGAN: An Adversarial Approach to Few-Shot Learning,"In this paper, we propose a conceptually simple and general framework called MetaGAN for few-shot learning problems. Most state-of-the-art few-shot classification models can be integrated with MetaGAN in a principled and straightforward way. By introducing an adversarial generator conditioned on tasks, we augment vanilla few-shot classification models with the ability to discriminate between real and fake data.  We argue that this GAN-based approach can help few-shot classifiers to learn sharper decision boundary, which could generalize better. We show that with our MetaGAN framework, we can extend supervised few-shot learning models to naturally cope with unsupervised data. Different from previous work in semi-supervised few-shot learning, our algorithms can deal with semi-supervision at both sample-level and task-level. We give theoretical justifications of the strength of MetaGAN, and validate the effectiveness of MetaGAN on challenging few-shot image classification benchmarks.","Ruixiang ZHANG*,HKUST;Tong Che,MILA;Zoubin Ghahramani,University of Cambridge;Yoshua Bengio,Universite de Montreal;Yangqiu Song,Hong Kong University of Science and Technology",sodabeta7@gmail.com;tong.che@umontreal.ca;zoubin@eng.cam.ac.uk;yoshua.umontreal@gmail.com;yqsong@cse.ust.hk,Deep Learning/Meta-Learning*; Algorithms/Semi-Supervised Learning; Deep Learning/Adversarial Networks; Deep Learning/Few-,Poster
1212,Local Differential Privacy for Evolving Data,"There are now several large scale deployments of differential privacy used to collect statistical information about users. However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result, these systems do not provide meaningful privacy guarantees over long time scales. Moreover, existing techniques to mitigate this effect do not apply in the ``local model'' of differential privacy that these systems use.

In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common (but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation.","Matthew  Joseph*,University of Pennsylvania;Aaron Roth,University of Pennsylvania;Jonathan Ullman,Northeastern University;Bo Waggoner,UPenn",majos@cis.upenn.edu;aaroth@cis.upenn.edu;jullman@ccs.neu.edu;bwag@seas.upenn.edu,"Applications/Privacy, Anonymity, and Security*; Algorithms",Spotlight
1217,Gaussian Process Conditional Density Estimation,"Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GP) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images.","Vincent Dutordoir*,PROWLER.io;Hugh Salimbeni,Imperial College;James Hensman,PROWLER.io;Marc Deisenroth,Imperial College London",vincent@prowler.io;hrs13@ic.ac.uk;james@prowler.io;m.deisenroth@imperial.ac.uk,Algorithms/Density Estimation*; Probabilistic Methods/Gaussian Processes,Poster
1219,Meta-Gradient Reinforcement Learning,"The goal of reinforcement learning algorithms is to estimate and/or optimise the value function. However, unlike supervised learning, no teacher or oracle is available to provide the true value function. Instead, the majority of reinforcement learning algorithms estimate and/or optimise a proxy for the value function. This proxy is typically based on a sampled and bootstrapped approximation to the true value function, known as a \emph{return}. The particular choice of return is one of the chief components determining the nature of the algorithm: the rate at which future rewards are discounted; when and how values should be bootstrapped; or even the nature of the rewards themselves. It is well-known that these decisions are crucial to the overall success of RL algorithms. We introduce a novel, gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art.","Zhongwen Xu*,DeepMind;Hado van Hasselt,DeepMind;David Silver,-",zhongwen@google.com;hado@google.com;davidsilver@google.com,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Reinforcement Learning,Poster
1229,Modular Networks: Learning to Decompose Neural Computation,"Scaling model capacity has been vital in the success of deep learning. For a typical network, necessary compute resources and training time grow dramatically with model size. Conditional computation is a promising way to increase the number of parameters with a relatively small increase in resources. We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. In contrast to existing approaches, training does not rely on regularization to enforce diversity in module use. We apply modular networks both to image recognition and language modeling tasks, where we achieve superior performance compared to several baselines. Introspection reveals that modules specialize to interpretable contexts.","Louis Kirsch*,University College London;Julius Kunze,University College London;David Barber,UCL",mail@louiskirsch.com;juliuskunze@gmail.com;david.barber@ucl.ac.uk,Probabilistic Methods/Latent Variable Models*; Algorithms/Representation Learning; Deep Learning/Optimization for Deep Networks; Deep Learnin,Poster
1233,Learning to Navigate in Cities Without a Map,"Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation (""I am here"") and a representation of the goal (""I am going there""). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. A key contribution of this paper is an interactive navigation environment that uses Google Street View for its photographic content and worldwide coverage. Our baselines demonstrate that deep reinforcement learning agents can learn to navigate in multiple cities and to traverse to target destinations that may be kilometres away. A video summarizing our research and showing the trained agent in diverse city environments as well as on the transfer task is available at: https://sites.google.com/view/learn-navigate-cities-nips18","Piotr Mirowski*,DeepMind;Matt Grimes,DeepMind;Mateusz Malinowski,DeepMind;Karl Moritz Hermann,DeepMind;Keith Anderson,DeepMind;Denis Teplyashin,DeepMind;Karen Simonyan,DeepMind;Koray Kavukcuoglu,;Andrew Zisserman,Oxford University;Raia Hadsell,Google Deepmind",piotrmirowski@google.com;mkg@google.com;mateuszm@google.com;kmh@google.com;keithanderson@google.com;teplyashin@google.com;simonyan@google.com;korayk@google.com;zisserman@google.com;raia@google.com,Reinforcement Learning and Planning/Navigation*; Reinforcement Learning and Planning/Reinforcement Learning,Poster
1236,Query Complexity of Bayesian Private Learning,"We study the query complexity of Bayesian Private Learning: a learner wishes to locate a random target within an interval by submitting queries, in the presence of an adversary who observes all of her queries but not the responses. How many queries are necessary and sufficient in order for the learner to accurately estimate the target, while simultaneously concealing the target from the adversary? 

Our main result is a query complexity lower bound that is tight up to the first order. We show that if the learner wants to estimate the target within an error of $\epsilon$, while ensuring that no adversary estimator can achieve a constant additive error with probability greater than $1/L$, then the query complexity is on the order of $L\log(1/\epsilon)$ as $\epsilon \to 0$. Our result demonstrates that increased privacy, as captured by $L$, comes at the expense of a \emph{multiplicative} increase in query complexity. The proof  builds on Fano's inequality and properties of certain proportional-sampling estimators.","Kuang Xu*,Stanford Graduate School of Business",kuangxu@gmail.com,"Algorithms/Online Learning*; Applications/Privacy, Anonymity, and Security",Poster
1239,A theory on the absence of spurious solutions for nonconvex and nonsmooth optimization,"We study the set of continuous functions that admit no spurious local optima (i.e. local minima that are not global minima) which we term global functions. They satisfy various powerful properties for analyzing nonconvex and nonsmooth optimization problems. For instance, they satisfy a theorem akin to the fundamental uniform limit theorem in the analysis regarding continuous functions. Global functions are also endowed with useful properties regarding the composition of functions and change of variables. Using these new results, we show that a class of non-differentiable nonconvex optimization problems arising in tensor decomposition applications are global functions. This is the first result concerning nonconvex methods for nonsmooth objective functions. Our result provides a theoretical guarantee for the widely-used $\ell_1$ norm to avoid outliers in nonconvex optimization.","Cedric Josz*,UC Berkeley;Yi Ouyang,UC Berkeley;Richard Zhang,UC Berkeley;Javad Lavaei,UC Berkeley;Somayeh Sojoudi,UC Berkeley",cedric.josz@gmail.com;ouyangyii@gmail.com;ryz@berkeley.edu;lavaei@berkeley.edu;sojoudi@berkeley.edu,Optimization/Non-Convex Optimization*; Applications/Matrix and Tensor Factorization,Poster
1242,Recurrent World Models Facilitate Policy Evolution,"A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper: https://nipsanon.github.io","David Ha*,Google Brain;J√ºrgen  Schmidhuber,IDSIA - Lugano",hadavid@google.com;juergen@idsia.ch,Neuroscience and Cognitive Science*; Neuroscience and Cognitive Science/Problem Solving,Oral
1243,Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling,"Ridge leverage scores provide a balance between low-rank approximation and regularization, and are ubiquitous in randomized linear algebra and machine learning.  Deterministic algorithms are also of interest in the moderately big data regime, because deterministic algorithms provide interpretability to the practitioner by having no failure probability and always returning the same results. We provide provable guarantees for deterministic column sampling using ridge leverage scores.   The matrix sketch returned by our algorithm is a column subset of the original matrix, yielding additional interpretability.  Like the randomized counterparts, the deterministic algorithm provides $(1+\epsilon)$  error column subset selection, $(1+\epsilon)$ error projection-cost preservation, and an additive-multiplicative spectral bound.  We also show that under the assumption of power-law decay of ridge leverage scores, this deterministic algorithm is provably as accurate as randomized algorithms. Lastly, ridge regression is frequently used to regularize ill-posed linear least-squares problems.  While ridge regression provides shrinkage for the regression coefficients, many of the coefficients remain small but non-zero. Performing ridge regression with the matrix sketch returned by our algorithm and a particular regularization parameter forces coefficients to zero and has a provable $(1+\epsilon)$ bound on the statistical risk.  As such, it is an interesting alternative to elastic net regularization.","Shannon McCurdy*,UC Berkeley",smccurdy@berkeley.edu,Algorithms*; Algorithms/Regression; Applications/Computational Biology and Bioinformatics,Poster
1244,Wasserstein Variational Inference,"This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques. ","Luca Ambrogioni*,Donders Institute;Umut G√º√ßl√º,Radboud University, Donders Institute for Brain, Cognition and Behaviour;Yaƒümur G√º√ßl√ºt√ºrk, Radboud University, Donders Institute for Brain, Cognition and Behaviour;Max Hinne,University of Amsterdam;Marcel van Gerven,Radboud University, Donders Institute for Brain, Cognition and Behaviour;Eric Maris,Donders Institute",luca.ambrogioni@gmail.com;u.guclu@donders.ru.nl;y.gucluturk@donders.ru.nl;m.hinne@uva.nl;m.vangerven@donders.ru.nl;e.maris@donders.ru.nl,Probabilistic Methods/Variational Inference*; Deep Learning/Deep Autoencoders; Deep Learning/Efficient Inference Methods,Poster
1246,"How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)","Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called ""internal covariate shift"". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These findings bring us closer to a true understanding of our DNN training toolkit.","Shibani Santurkar,MIT;Dimitris Tsipras*,MIT;Andrew Ilyas,MIT;Aleksander Madry,MIT",shibani@mit.edu;tsipras@mit.edu;ailyas@mit.edu;madry@mit.edu,Deep Learning/Optimization for Deep Networks*; Deep Learning; Deep Learning/Efficient Training Methods,Oral
1247,Verifiable Reinforcement Learning via Policy Extraction,"While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.","Osbert Bastani*,University of Pennsylvania;Yewen Pu,MIT;Armando Solar-Lezama,MIT",obastani@csail.mit.edu;yewenpu@mit.edu;asolar@csail.mit.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Decision and Control; Theory/Control Theory,Poster
1249,Leveraged volume sampling for linear regression,"  Suppose an n x d design matrix in a linear regression problem is given, 
but the response for each point is hidden unless explicitly requested. 
The goal is to sample only a small number k << n of the responses, 
and then produce a weight vector whose sum of squares loss over *all* points is at most 1+epsilon times the minimum. 
When k is very small (e.g., k=d), jointly sampling diverse subsets of
points is crucial. One such method called ""volume sampling"" has
a unique and desirable property that the weight vector it produces is an unbiased
estimate of the optimum. It is therefore natural to ask if this method
offers the optimal unbiased estimate in terms of the number of
responses k needed to achieve a 1+epsilon loss approximation.

Surprisingly we show that volume sampling can have poor behavior when
we require a very accurate approximation -- indeed worse than some
i.i.d. sampling techniques whose estimates are biased, such as
leverage score sampling. 
We then develop a new rescaled variant of volume sampling that
produces an unbiased estimate which avoids
this bad behavior and has at least as good a tail bound as leverage
score sampling: sample size k=O(d log d + d/epsilon) suffices to
guarantee total loss at most 1+epsilon times the minimum
with high probability. Thus, we improve on the best previously known
sample size for an unbiased estimator, k=O(d^2/epsilon).

Our rescaling procedure leads to a new efficient algorithm
for volume sampling which is based
on a ""determinantal rejection sampling"" technique with
potentially broader applications to determinantal point processes.
Other contributions include introducing the
combinatorics needed for rescaled volume sampling and developing tail
bounds for sums of dependent random matrices which arise in the
process.","Michal Derezinski*,UC Berkeley;Manfred K. Warmuth,UCSC;Daniel Hsu,Columbia University",michal.derezinski@gmail.com;manfred@ucsc.edu;djhsu@cs.columbia.edu,Algorithms/Regression*; Algorithms/Active Learning; Theory/Learning Theory,Spotlight
1253,Supervised Local Modeling for Interpretability,"Model interpretability is an increasingly important component of practical machine learning systems, with example-based, local, and global explanations representing some of the most common forms of explanations. We present a novel explanation system called SLIM that leverages favorable properties of all three of these explanation types. By combining local linear modeling techniques with dual interpretations of random forests (as a supervised neighborhood approach and  as a feature selection method), we present a novel local explanation system with several favorable properties.
First, SLIM sidesteps the typical accuracy-interpretability trade-off, as it is  highly accurate while also providing both example-based and local explanations.
Second, while SLIM does not provide global explanations, it can detect global patterns and thus diagnose limitations in its local explanations.  
Third, SLIM can select an appropriate explanation for a new test point when restricted to an existing set of exemplar explanations. 
Finally, in addition to providing faithful self-explanations, SLIM can be deployed as a black-box explanation system.","Gregory Plumb*,CMU;Denali Molitor,UCLA;Ameet Talwalkar,CMU",plumbdg@gmail.com;dmolitor@math.ucla.edu;talwalkar@cmu.edu,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Boosting and Ensemble Methods; Algorithms/Classification; Algorithms/Regression",Poster
1261,A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication,"The large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks.  Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost.  However,  there is still a lack of understanding about how sparse and quantized communication  affects the convergence rate of  the training algorithm. In this paper, we study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization.  We show that $O(1/\sqrt{MK})$ convergence rate can be achieved if  the sparsification and quantization hyperparameters are configured properly.  We also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communication cost  while preserving the $O(1/\sqrt{MK})$ convergence rate. Our evaluation validates our theoretical results and shows that our PQASGD can converge as fast as full-precision SGD with only $3\%-5\%$ communication data size.  ","Peng Jiang*,The Ohio State University;Gagan Agrawal,Ohio State University",jiang.952@osu.edu;agrawal@cse.ohio-state.edu,Deep Learning/Efficient Training Methods*; Optimization/Non-Convex Optimization,Poster
1262,Active Learning for Non-Parametric Regression Using Purely Random Trees,"Active learning is the task of using labelled data to select additional points to label, with the goal of fitting the most accurate model with a fixed budget of labelled points. In binary classification active learning is known to produce faster rates than passive learning for a broad range of settings. However in regression restrictive structure and tailored methods have been needed to obtain theoretically superior rates. In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling. When implemented with Mondrian Trees our algorithm is tuning parameter free, consistent and minimax optimal for Lipschitz functions.","Jonathan Goetz*,University of Michigan;Ambuj Tewari,University of Michigan;Paul Zimmerman,University of Michigan",jrgoetz@umich.edu;tewaria@umich.edu;paulzim@umich.edu,Algorithms/Active Learning*; Algorithms/Regression; Theory/Frequentist Statistics,Poster
1277,Tree-to-tree Neural Networks for Program Translation,"Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.","Xinyun Chen*,UC Berkeley;Chang Liu,UC Berkeley;Dawn Song,UC Berkeley",xinyun.chen@berkeley.edu;liuchang@eecs.berkeley.edu;dawnsong@cs.berkeley.edu,Deep Learning/Recurrent Networks*; Algorithms/Structured Prediction; Deep Learning/Attention Models; Deep Learning/Supervised,Poster
1283,Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks,"Real-world image recognition is often challenged by the variability of visual styles including object textures, lighting conditions, filter effects, etc. Although these variations have been deemed to be implicitly handled by more training data and deeper networks, recent advances in image style transfer suggest that it is also possible to explicitly manipulate the style information. Extending this idea to general visual recognition problems, we present Batch-Instance Normalization (BIN) to explicitly normalize unnecessary styles from images. Considering certain style features play an essential role in discriminative tasks, BIN learns to selectively normalize only disturbing styles while preserving useful styles. The proposed normalization module is easily incorporated into existing network architectures such as Residual Networks, and surprisingly improves the recognition performance in various scenarios. Furthermore, experiments verify that BIN effectively adapts to completely different tasks like object classification and style transfer, by controlling the trade-off between preserving and removing style variations. BIN can be simply implemented with only a few lines of code using recent deep learning frameworks.","Hyeonseob Nam*,Lunit Inc.;Hyo-Eun Kim,Lunit Inc.",hsnam@lunit.io;hekim@lunit.io,Deep Learning/CNN Architectures*; Applications/Computer Vision,Poster
1286,Structural Causal Bandits: Where to Intervene?,"We study the problem of identifying the best action in a sequential decision-making setting when the reward distributions of the arms exhibit non-trivial dependencies, which are governed by the underlying causal structure of the domain where the agent is deployed. In this setting, playing an arm corresponds to intervening on a set of variables and setting them to specific values. In this paper, we start by showing that whenever the causal model relating the arms is unknown, the strategy of simultaneously intervening in all variables can, in general, lead to a sub-optimal policy (regardless of the number of iterations performed in the environment). We then derive structural properties implied by the given causal model, which is assumed to be known, albeit without parametrization. We further propose an algorithm that takes as input the causal structure and finds a minimal, sound, and complete set of qualified arms that the agent can play so as to maximize its reward. We empirically demonstrate that this algorithm leads to optimal, order of magnitude faster convergence rates when compared with its causal-insensitive counterparts.","Sanghack Lee*,Purdue University;Elias Bareinboim,Purdue University",sanghack.lee@gmail.com;eb@purdue.edu,Probabilistic Methods/Causal Inference*; Probabilistic Methods/Graphical Models,Poster
1289,Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog,"Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence.
Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take. 
To ask the adequate question, deep learning and reinforcement learning have been recently applied. 
However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences.
Motivated by theory of mind, we propose ``Answerer in Questioner's Mind'' (AQM), a novel algorithm for goal-oriented dialog. 
With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer.
The questioner figures out the answerer‚Äôs intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question.
We test our framework on two goal-oriented visual dialog tasks: ``MNIST Counting Dialog'' and ``GuessWhat?!.''
In our experiments, AQM outperforms comparative algorithms by a large margin.","Sang-Woo Lee*,Seoul National University;Yu-Jung Heo,Seoul National University;Byoung-Tak Zhang,Seoul National University",slee@bi.snu.ac.kr;yjheo@bi.snu.ac.kr;btzhang@bi.snu.ac.kr,Applications/Dialog- or Communication-Based Learning*; Applications/Visual Question Answering,Spotlight
1294,A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation,"We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.","Alexander Liu,NTU;Yen-Cheng Liu,Georgia Institute of Technology;Yu-Ying Yeh,National Taiwan University;Yu-Chiang Frank Wang*,National Taiwan University",b03902034@ntu.edu.tw;r04921003@ntu.edu.tw;b99202023@ntu.edu.tw;ycwang@ntu.edu.tw,Applications/Computer Vision*; Algorithms/Multitask and Transfer Learning; Algorithms/Representation Learning; Deep Learning,Poster
1314,Online Learning with an Unknown Fairness Metric,"We consider the problem of online learning in the linear contextual bandits setting, but in which there are also strong individual fairness constraints governed by an unknown similarity metric. These constraints demand that we select similar actions or individuals with approximately equal probability DHPRZ12, which may be at odds with optimizing reward, thus modeling settings where profit and social policy are in tension. We assume we learn about an unknown Mahalanobis similarity metric from only weak feedback that identifies fairness violations, but does not quantify their extent. This is intended to represent the interventions of a regulator who ""knows unfairness when he sees it"" but nevertheless cannot enunciate a quantitative fairness metric over individuals. Our main result is an algorithm in the adversarial context setting that has a number of fairness violations that depends only logarithmically on T, while obtaining an optimal O(sqrt(T)) regret bound to the best fair policy.","Stephen Gillen,University of Pennsylvania;Christopher Jung*,University of Pennsylvania;Michael Kearns,University of Pennsylvania;Aaron Roth,University of Pennsylvania",stepe@math.upenn.edu;chrisjung.sy@gmail.com;mkearns@cis.upenn.edu;aaroth@cis.upenn.edu,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Bandit Algorithms; Algorithms/Metric Learning; Algorithms/Online Learning; Algor",Poster
1319,Isolating Sources of Disentanglement in Variational Autoencoders,"We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate the beta-TCVAE (Total Correlation Variational Autoencoder) algorithm, a refinement and plug-in replacement of the beta-VAE for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.","Ricky T. Q. Chen*,University of Toronto;Xuechen Li,University of Toronto;Roger Grosse,-;David Duvenaud,University of Toronto",rtqichen@cs.toronto.edu;lxuechen@cs.toronto.edu;rgrosse@cs.toronto.edu;duvenaud@cs.toronto.edu,Deep Learning/Generative Models*; Algorithms/Representation Learning; Algorithms/Unsupervised Learning; Deep Learning/Deep Au,Oral
1329,Contextual bandits with surrogate losses: Margin bounds and efficient algorithms,"We use surrogate losses to obtain several new regret bounds and new algorithms for contextual bandit learning. Using the ramp loss, we derive a new margin-based regret bound in terms of standard sequential complexity measures of a benchmark class of real-valued regression functions. Using the hinge loss, we derive an efficient algorithm with a $\sqrt{dT}$-type mistake bound against benchmark policies induced by $d$-dimensional regressors. Under realizability assumptions, our results also yield classical regret bounds.","Dylan Foster,Cornell University;Akshay Krishnamurthy*,Microsoft",djf244@cornell.edu;akshay@cs.umass.edu,Algorithms/Online Learning*; Algorithms/Bandit Algorithms; Theory/Learning Theory,Poster
1344,Representation Learning for Treatment Effect Estimation from Observational Data,"Estimating individual treatment effect (ITE) is a challenging problem in causal inference, due to missing counterfactuals and the selection bias. Existing ITE estimation methods mainly focus on balancing the distributions of control and treated groups, but ignore the local similarity information that is helpful. In this paper, we propose a local similarity preserved individual treatment effect (SITE) estimation method based on deep representation learning. SITE preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. Experimental results on synthetic and three real-world datasets demonstrate the advantages of the proposed SITE method, compared with the state-of-the-art ITE estimation methods.","Liuyi Yao*,SUNY Buffalo, USA;Sheng Li,University of Georgia;Yaliang Li,Tencent Medical AI Lab;Mengdi Huai,State University of New York at Buffalo;Jing Gao,University at Buffalo;Aidong Zhang,SUNY Buffalo",liuyiyao@buffalo.edu;shengli.uga@gmail.com;yaliangli@tencent.com;mengdihu@buffalo.edu;jing@buffalo.edu;azhang@buffalo.edu,Algorithms/Representation Learning,Poster
1356,Representation Balancing MDPs for Off-policy Policy Evaluation,"We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work, we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning, and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective, we develop a learning algorithm of an MDP model with a balanced representation, and show that our approach can yield substantially lower MSE in a common synthetic domain and on a challenging real-world sepsis management problem.","Yao Liu*,Stanford University;Omer Gottesman,Harvard Univerisity;Aniruddh Raghu,University of Cambridge;Matthieu Komorowski,Imperial College London;Aldo Faisal,Imperial College London;Finale Doshi-Velez,Harvard;Emma Brunskill,Stanford University",yaoliu@stanford.edu;gottesman@fas.harvard.edu;aniruddhraghu@gmail.com;matthieu.komorowski@gmail.com;a.faisal@imperial.ac.uk;finale@seas.harvard.edu;ebrun@cs.stanford.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Probabilistic Methods/Causal Inference; Reinforcement Learning and Planning/Markov Decision Pr,Poster
1357,Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering,"Accurately answering a question about a given image requires combining observations with general knowledge.  While this is effortless for humans, reasoning with general knowledge remains an algorithmic challenge. To advance research in this direction, a novel `fact-based' visual question answering (FVQA) task has been introduced recently along with a large set of curated facts which link two entities, i.e., two possible answers, via a relation. Given a question-image pair, deep net techniques have been employed to successively reduce the large set of facts until one of the two entities of the final remaining fact is predicted as the answer. We observe that a successive process which considers one fact at a time to form a local decision is sub-optimal. Instead, we develop an entity graph and a graph convolutional net method to `reason' about the correct answer by jointly considering all entities. We show on the challenging FVQA dataset that this leads to an improvement in accuracy of around 10% compared to the state-of-the-art.","Medhini Narasimhan*,UIUC;Svetlana Lazebnik,UIUC;Alexander Schwing,UIUC",medhini2@illinois.edu;slazebni@illinois.edu;aschwing@illinois.edu,Applications/Visual Question Answering*; Applications/Computer Vision,Poster
1359,Causal Inference on Discrete Data using Hidden Compact Representation,"	Inferring causal relations from a set of observations is one of the fundamental problems across several disciplines. For continuous variables, recently a number of causal discovery methods have demonstrated their effectiveness in distinguishing the cause from effect by exploring certain properties of the conditional distribution, but causal discovery on categorical data still remains to be a challenging problem, because it is generally not easy to find a compact description of the causal mechanism for the true causal direction. In this paper we make an attempt to find a way to solve this problem by assuming a two-stage causal process: the first stage maps the cause to a hidden variable of a lower cardinality, and the second stage generates the effect from the hidden representation. In this way, the causal mechanism admits a simple, compact representation. We show that under this model, the causal direction is identifiable under some weak technique conditions on the true causal mechanism. We also provide an effective solution to recover the above hidden compact representation model under the likelihood framework. Empirical studies verify the effectiveness of the proposed approach on both synthetic and real-world data.","Ruichu Cai,Guangdong University of Technology;Jie Qiao*,Guangdong University of Technology;Kun Zhang,Carnegie Mellon University;Zhenjie Zhang,Singapore R&D, Yitu Technology Ltd.,;Zhifeng Hao,Guangdong University of Technology	",cairuichu@gmail.com;qiaojie.chn@qq.com;kunz1@cmu.edu;zhenjie.zhang@yitu-inc.com;zfhao@gdut.edu.cn,Probabilistic Methods/Causal Inference*; Deep Learning/Generative Models; Probabilistic Methods/Graphical Models,Poster
1376,Natasha 2: Faster Non-Convex Optimization Than SGD,"(this is a theory paper)

We design a stochastic algorithm to find $\varepsilon$-approximate local minima of any smooth nonconvex function in rate $O(\varepsilon^{-3.25})$, with only oracle access to stochastic gradients. The best result was essentially $O(\varepsilon^{-4})$ by stochastic gradient descent (SGD).","Zeyuan Allen-Zhu*,Microsoft Research AI",zeyuan@csail.mit.edu,Optimization/Non-Convex Optimization*; Theory/Learning Theory,Spotlight
1380,Minimax Statistical Learning with Wasserstein distances,"As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for transport-based domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.","Jaeho Lee*,University of Illinois at Urbana-Champaign;Maxim Raginsky,University of Illinois",jlee620@illinois.edu;maxim@illinois.edu,Theory/Learning Theory,Spotlight
1388,Provable Variational Inference for Constrained Log-Submodular Models,"Submodular maximization problems appear in several areas of machine learning and data science, as many useful modelling concepts such as diversity and coverage satisfy this natural diminishing returns property. Because the data defining these functions, as well as the decisions made with the computed solutions are subject to statistical noise and randomness, it is arguably necessary to go beyond computing a single approximate optimum and quantify its inherent uncertainty. To this end, we define a rich class of probabilistic models associated with constrained submodular maximization problems. These capture log-submodular dependencies of arbitrary order between the variables, but also satisfy hard combinatorial constraints. Namely, the variables are assumed to take on one of ‚Äî possibly exponential ‚Äî set of states, which form the bases of a matroid. To perform inference in these models we design novel variational inference algorithms, which carefully leverage the combinatorial and probabilistic properties of these objects. In addition to providing completely tractable and well-understood variational approximations, our approach results in the minimization of a convex upper bound on the log-partition function. The bound can be efficiently evaluated using greedy algorithms and optimized using any first-order method. Moreover, for the case of facility location and weighted coverage functions, we prove the first constant factor guarantee in this setting ‚Äî an efficiently certifiable e/(e-1) approximation of the log-partition function. Finally, we empirically demonstrate the effectiveness of our approach on several instances.","Josip Djolonga*,ETH Zurich;Stefanie  Jegelka,MIT;Andreas Krause,ETH Z√ºrich",josipd@inf.ethz.ch;stefje@csail.mit.edu;krausea@ethz.ch,Optimization/Submodular Optimization*; Deep Learning/Efficient Inference Methods; Probabilistic Methods/Variational Inference,Poster
1399,Learning Hierarchical Semantic Image Manipulation through Structured Representations,"Understanding, reasoning, and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation of natural image manifold through color strokes, key-points, textures, and holes-to-fill. In this work, we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ structured semantic layout as our intermediate representations for manipulation. Initialized with coarse-level bounding boxes, our layout generator first creates pixel-wise semantic layout capturing the object shape, object-object interactions, and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding, removing, and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation, interactive image editing, and data-driven image manipulation.","Seunghoon Hong*,University of Michigan;Xinchen Yan,University of Michigan;Honglak Lee,University of Michingan Ann Arbor, USA;Thomas Huang,University of Michigan",hongseu@umich.edu;xcyan@umich.edu;honglak@eecs.umich.edu;thomaseh@umich.edu,Applications/Computer Vision,Poster
1430,Processing of missing data by neural networks,"We propose a general, theoretically justified mechanism for processing missing data by neural networks. Our idea is to replace typical neuron's response in the first hidden layer by its expected value. This approach can be applied for various types of networks at minimal cost in their modification. Moreover, in contrast to recent approaches, it does not require complete data for training. Experimental results performed on different types of architectures show that our method gives better results than typical imputation strategies and other methods dedicated for incomplete data.","Marek ≈ömieja*,Jagiellonian University;≈Åukasz Struski,Jagiellonian University;Jacek Tabor,Jagiellonian University;Bartosz Zieli≈Ñski,Jagiellonian University;PRzemys≈Çaw Spurek,Jagiellonian University",marek.smieja@ii.uj.edu.pl;lukasz.struski@uj.edu.pl;jacek.tabor@uj.edu.pl;bartosz.zielinski@uj.edu.pl;przemyslaw.spurek@uj.edu.pl,Algorithms/Missing Data*; Deep Learning,Poster
1441,Safe Active Learning for Time-Series Modeling with Gaussian Processes,"Learning time-series models is useful for many applications, such as simulation and forecasting. In this study, we consider the problem of actively learning time-series models, while taking given safety constraints into account. For time-series modeling, we employ a Gaussian process with nonlinear exogenous input structure. The proposed approach generates data, i.e. input and output trajectories, appropriate for time-series model learning by dynamically exploring the input space. The basic idea behind the proposed approach is to parametrize the input trajectory as consecutive trajectory sections, which are determined stepwise given safety requirements and past observations. We analyze the proposed algorithm and, empirically, evaluate it on a technical application. The results show the effectiveness of our approach in a realistic industrial setting.","Christoph Zimmer*,Bosch Center for Artificial Intelligence;Mona Meister,Bosch Center for Artificial Intelligence;Nguyen-Tuong Duy,Bosch",Christoph.Zimmer@de.bosch.com;mona.meister@de.bosch.com;duy.nguyen-tuong@de.bosch.com,Algorithms/Active Learning*; Algorithms/Regression; Applications/Time Series Analysis; Probabilistic Methods/Gaussian Pr,Poster
1446,Optimal Algorithms for Non-Smooth Distributed Optimization in Networks,"In this work, we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the Lipschitz continuity of the global objective function, and (2) the Lipschitz continuity of local individual functions. Under the local regularity assumption, we provide the first optimal first-order decentralized algorithm called multi-step primal-dual (MSPD) and its corresponding optimal convergence rate. A notable aspect of this result is that, for non-smooth functions, while the dominant term of the error is in $O(1/\sqrt{t})$, the structure of the communication network only impacts a second-order term in $O(1/t)$, where $t$ is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly-convex objective functions. Under the global regularity assumption, we provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function, and show that DRS is within a $d^{1/4}$ multiplicative factor of the optimal convergence rate, where $d$ is the underlying dimension.","Kevin Scaman*,Huawei;Francis Bach,INRIA - Ecole Normale Sup√©rieure;Sebastien Bubeck,Microsoft Research;Laurent Massoulie,Microsoft-Inria Joint Center;Yin Tat Lee,UW",kevin.scaman@gmail.com;francis.bach@inria.fr;sebubeck@microsoft.com;laurent.massoulie@inria.fr;yintat@uw.edu,Optimization/Convex Optimization*; Applications/Communication- or Memory-Bounded Learning; Applications/Network Analysis,Oral
1455,Computing Higher Order Derivatives of Matrix and Tensor Expressions,"Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly.  Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup between one and four orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives.","S√∂ren Laue,Friedrich Schiller University Jena;Matthias Mitterreiter,Friedrich Schiller University Jena;Joachim Giesen*,Friedrich Schiller University Jena",soeren.laue@uni-jena.de;matthias.mitterreiter@uni-jena.de;joachim.giesen@uni-jena.de,Optimization*; Algorithms/AutoML; Optimization/Convex Optimization; Optimization/Non-Convex Optimization,Poster
1466,Paraphrasing Complex Network: Network Compression via Factor Transfer,"Many researchers have sought ways of model compression to reduce the size of a deep neural network DNN with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.","Jangho Kim,Seoul National University;Seonguk Park,Seoul National University;Nojun Kwak*,Seoul National University",kjh91@snu.ac.kr;swpark0703@snu.ac.kr;nojunk@snu.ac.kr,Deep Learning/Efficient Training Methods*; Deep Learning/Supervised Deep Networks,Poster
1467,Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net,"The lasso and elastic net linear regression models impose a double-exponential prior distribution on the model parameters to achieve   regression shrinkage and variable selection,  allowing the inference of robust models from large data sets.  However, there has been limited success in deriving estimates for the full posterior distribution of regression coefficients in these models, due to a need to evaluate analytically intractable partition function integrals. Here, the Fourier transform is used to express these integrals as complex-valued oscillatory integrals over ""regression frequencies"". This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net, where the non-differentiability of the double-exponential prior has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefficients, and allows for Bayesian inference of much higher dimensional models than previously possible.","Tom Michoel*,University of Bergen",tom.michoel@gmail.com,Probabilistic Methods/Bayesian Theory*; Theory/Large Deviations and Asymptotic Analysis,Poster
1477,Demystifying excessively volatile human learning: A Bayesian persistent prior and a neural approximation,"Understanding how humans and animals learn about statistical regularities in stable and volatile environments, and utilize these regularities to make predictions and decisions, is an important problem in neuroscience and psychology. Using a Bayesian modeling framework, specifically the Dynamic Belief Model (DBM), it has previously been shown that humans tend to make the {\it default} assumption that environmental statistics undergo abrupt, unsignaled changes, even when environmental statistics are actually stable. Because exact Bayesian inference in this setting, an example of switching state space models, is computationally intense, a number of approximately Bayesian and heuristic algorithms have been proposed to account for learning/prediction in the brain. Here, we examine a neurally plausible algorithm, a special case of leaky integration dynamics we denote as EXP (for exponential filtering), that is significantly simpler than all previously suggested algorithms except for the delta-learning rule, and which far outperforms the delta rule in approximating Bayesian prediction performance. We derive the theoretical relationship between DBM and EXP, and show that EXP gains computational efficiency by foregoing the representation of inferential uncertainty (as does the delta rule), but that it nevertheless achieves near-Bayesian performance due to its ability to incorporate a ""persistent prior"" influence unique to DBM and absent from the other algorithms. Furthermore, we show that EXP is comparable to DBM but better than all other models in reproducing human behavior in a visual search task, suggesting that human learning and prediction also incorporates an element of persistent prior. More broadly, our work demonstrates that when observations are information-poor, detecting changes or modulating the learning rate is both {\it difficult} and (thus) {\it unnecessary} for making Bayes-optimal predictions.","Chaitanya Ryali*,University of California, San Diego;Gautam Reddy,University of California, San Diego;Angela Yu,UC San Diego",rckrishn@ucsd.edu;gnallama@physics.ucsd.edu;ajyu@ucsd.edu,Neuroscience and Cognitive Science/Neuroscience*; Neuroscience and Cognitive Science/Human or Animal Learning,Poster
1478,Empirical Risk Minimization Under Fairness Constraints,"We address the problem of algorithmic fairness: ensuring that sensitive variables do not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our approach. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches.","Michele Donini,Istituto Italiano di Tecnologia ;Luca Oneto*,Nil;Shai Ben-David,Universitys of Waterloo;John  Shawe-Taylor,University College London;Massimiliano Pontil,IIT",donini.michele@gmail.com;luca.oneto@unige.it;bendavid.shai@gmail.com;j.shawe-taylor@ucl.ac.uk;massimiliano.pontil@gmail.com,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Classification; Algorithms/Kernel Methods; Theory/Learning Theory",Poster
1481,Unsupervised Learning of Shape and Pose with Differentiable Point Clouds,"We address the problem of learning accurate 3D shape and camera pose from a collection of unlabeled category-specific images. We train a convolutional network to predict both the shape and the pose from a single image by minimizing the reprojection error: given several views of an object, the projections of the predicted shapes to the predicted camera poses should match the provided views. To deal with pose ambiguity, we introduce an ensemble of pose predictors that we then distill it to a single ``student'' model. To allow for efficient learning of high-fidelity shape representation, we represent the shapes by point clouds and devise a formulation allowing for differentiable projection of these. Our experiments show that the distilled ensemble of pose predictors learns to estimate the pose accurately, while the point cloud representation allows to predict detailed shape models.","Eldar Insafutdinov ,Max-Planck-Institute for informatics; Alexey Dosovitskiy*,Intel Labs",eldar@mpi-inf.mpg.de;adosovitskiy@gmail.com,Applications/Computer Vision*; Algorithms/Unsupervised Learning; Deep Learning; Deep Learning/Generative Models,Poster
1484,Continuous-time Value Function Approximation in Reproducing Kernel Hilbert Spaces,"Motivated by the success of reinforcement learning (RL) for discrete-time tasks such as AlphaGo and Atari games, there has been a recent surge of interest in using RL for continuous-time control of physical systems (cf. many challenging tasks in OpenAI Gym and the DeepMind Control Suite).  Since discretization of time is susceptible to error, it is methodologically more desirable to handle the system dynamics directly in continuous time.  However, very few techniques exist for continuous-time RL and they lack flexibility in value function approximation.  In this paper, we propose a novel framework for continuous-time value function approximation based on reproducing kernel Hilbert spaces.  The resulting framework is so flexible that it can accommodate any kind of kernel-based approach, such as Gaussian processes and the adaptive projected subgradient method, and it allows us to handle uncertainties and nonstationarity without prior knowledge about the environment or what basis functions to employ.  We demonstrate the validity of the presented framework through experiments.","Motoya Ohnishi*,Keio University/KTH Royal Institute of Technology/RIKEN;Masahiro Yukawa,Keio University;Mikael Johansson,KTH Royal Institute of Technology;Masashi Sugiyama,RIKEN/The University of Tokyo",ohnishi@ykw.elec.keio.ac.jp;yukawa@elec.keio.ac.jp;mikaelj@kth.se;sugi@k.u-tokyo.ac.jp,Algorithms/Kernel Methods*; Reinforcement Learning and Planning/Model-Based RL,Poster
1485,Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation,"There has been a recent surge of interest in developing theoretically faster algorithms for leading eigenvector computation. The key to achieving faster convergence rates therein is to use the classic shift-and-invert preconditioning technique on top of power methods. The underlying problem then can be reduced to a series of linear system subproblems that can leverage fast approximate least squares solvers. Despite the simplicity of the power iterations as the base method, it may suffer from making limited progress towards solutions. In this work, we consider that the shift-and-invert preconditioning is paired with a new base method, namely gradient descent search. By virtue of the flexibility of setting step-sizes in gradient search processes, we expect the shift-and-inverted gradient descent solver can outperform the shift-and-inverted power methods. In particular, we present a novel convergence analysis for this new pairing that achieves a rate at $\tilde{O}(\sqrt{\frac{\lambda_{1}}{\lambda_{1}-\lambda_{p+1}}})$, where $\lambda_{i}$ represents the $i$-th largest eigenvalue of the given real symmetric matrix and $p$ is the multiplicity of $\lambda_{1}$. Our experimental studies show that the proposed algorithm can be significantly faster than the shift-and-inverted power method in practice.","Zhiqiang Xu*,KAUST",zhiqiangxu2001@gmail.com,Optimization/Non-Convex Optimization*; Algorithms/Relational Learning; Applications/Matrix and Tensor Factorization; Theory/Comput,Poster
1491,Factored Bandits,"We introduce the factored bandits model, which is a framework for learning with
limited (bandit) feedback, where actions can be decomposed into a Cartesian
product of atomic actions. Factored bandits incorporate rank-1 bandits as a special
case, but significantly relax the assumptions on the form of the reward function. We
provide an anytime algorithm for stochastic factored bandits and up to constants
matching upper and lower regret bounds for the problem. Furthermore, we show
that with a slight modification the proposed algorithm can be applied to utility
based dueling bandits. We obtain an improvement in the additive terms of the regret
bound compared to state of the art algorithms (the additive terms are dominating
up to time horizons which are exponential in the number of arms).","Julian Zimmert*,University of Copenhagen;Yevgeny Seldin,University of Copenhagen",julian.zimmert@gmail.com;yevgeny.seldin@gmail.com,Algorithms/Bandit Algorithms,Poster
1493,Delta-encoder: an effective sample synthesis method for few-shot object recognition,"Learning to classify new categories based on just one or a few examples is a long-standing challenge in modern computer vision. In this work, we proposes a simple yet effective method for few-shot (and one-shot) object recognition. Our approach is based on a modified auto-encoder, denoted Delta-encoder, that learns to synthesize new samples for an unseen category just by seeing few examples from it. The synthesized samples are then used to train a classifier. The proposed approach learns to both extract transferable intra-class deformations, or ""deltas"", between same-class pairs of training examples, and to apply those deltas to the few provided examples of a novel class (unseen during training) in order to efficiently synthesize samples from that new class. The proposed method improves over the state-of-the-art in one-shot object-recognition and compares favorably in the few-shot case. Upon acceptance code will be made available.","Eli Schwartz,IBM-Research;Leonid Karlinsky*,IBM-Research;Joseph Shtok,IBM-Reseach;Sivan Harary,IBM-Research;Mattias Marder,IBM-Research;Abhishek Kumar,IBM Research;Rogerio Feris,IBM Research;Raja Giryes,Tel Aviv University;Alex Bronstein,Technion",elisch@ibm.com;leonidka@il.ibm.com;josephs@il.ibm.com;sivangl@il.ibm.com;mattiasm@il.ibm.com;abhishk@us.ibm.com;rsferis@us.ibm.com;raja@tauex.tau.ac.il;bron@cs.technion.ac.il,Deep Learning/Few-Shot Learning Approaches*; Algorithms/Classification; Applications/Computer Vision; Deep Learning/Deep Autoencoders; D,Spotlight
1495,Metric on Nonlinear Dynamical Systems with Koopman Operators,"The development of a metric for structural data is a long-term problem in pattern recognition and machine learning. In this paper, we develop a general metric for comparing nonlinear dynamical systems that is defined with Koopman operator in reproducing kernel Hilbert spaces. Our metric includes the existing fundamental metrics for dynamical systems, which are basically defined with principal angles between some appropriately-chosen subspaces, as its special cases. We also describe the estimation of our metric from finite data. We empirically illustrate our metric with an example of rotation dynamics in a unit disk in a complex plane, and evaluate the performance with real-world time-series data.","Isao Ishikawa*,RIKEN AIP;Keisuke Fujii,RIKEN AIP Center;Masahiro Ikeda,RIKEN AIP;Yuka Hashimoto,Keio University;Yoshinobu Kawahara,Osaka University / RIKEN",isao.ishikawa@riken.jp;keisuke.fujii.zh@riken.jp;masahiro.ikeda@riken.jp;yukahashimoto@keio.jp;ykawahara@sanken.osaka-u.ac.jp,Algorithms/Dynamical Systems*; Theory/Spaces of Functions and Kernels,Poster
1496,Learning a High Fidelity Pose Invariant Model for High-resolution Face Frontalization,"Face frontalization refers to the process of synthesizing the frontal view of a face from a given profile.  Due to self-occlusion and appearance distortion in the wild, it is extremely challenging to recover faithful results and preserve texture details in a high-resolution. This paper proposes a High Fidelity Pose Invariant Model (HF-PIM) to produce photographic and identity-preserving results. HF-PIM frontalizes the profiles through a novel texture warping procedure and leverages a dense correspondence field to bind the 2D and 3D surface spaces. We decompose the prerequisite of warping into dense correspondence field estimation and facial texture map recovering, which are both well addressed by deep networks. Different from those reconstruction methods relying on 3D data, we also propose Adversarial Residual Dictionary Learning (ARDL) to supervise facial texture map recovering with only monocular images. Exhaustive experiments on both controlled and uncontrolled environments demonstrate that the proposed method not only boosts the performance of pose-invariant face recognition but also dramatically improves high-resolution frontalization appearances.","Jie Cao*,Institute of Automation, Chinese Academy of Sciences;Yibo Hu,Institute of Automation, Chinese Academy of Sciences;Hongwen Zhang,CASIA;Ran He,Institute of Automation, Chinese Academy of Sciences;Zhenan Sun,Chinese of Academy of Sciences",jie.cao@cripac.ia.ac.cn;yibo.hu@cripac.ia.ac.cn;hongwen.zhang@cripac.ia.ac.cn;rhe@nlpr.ia.ac.cn;znsun@nlpr.ia.ac.cn,"Applications/Body Pose, Face, and Gesture Analysis*; Applications/Computer Vision; Deep Learning/Adversarial Networks",Poster
1499,Mirrored Langevin Dynamics,"We consider the problem of sampling from constrained distributions, which has posed significant challenges to both non-asymptotic analysis and algorithmic design. We propose a unified framework, which is inspired by the classical mirror descent, to derive novel first-order sampling schemes. We prove that, for a general target distribution with strongly convex potential, our framework implies the existence of a first-order algorithm achieving O~(\epsilon^{-2}d) convergence, suggesting that the state-of-the-art O~(\epsilon^{-6}d^5) can be vastly improved. With the important Latent Dirichlet Allocation (LDA) application in mind, we specialize our algorithm to sample from Dirichlet posteriors, and derive the first non-asymptotic O~(\epsilon^{-2}d^2) rate for first-order sampling. We further extend our framework to the mini-batch setting and prove convergence rates when only stochastic gradients are available. Finally, we report promising experimental results for LDA on real datasets.","Ya-Ping Hsieh*,√âcole Polytechnique F√©d√©rale d;Ali Kavis,EPFL;Paul Rolland,EPFL;Volkan Cevher,EPFL",ya-ping.hsieh@epfl.ch;ali.kavis@epfl.ch;paul.rolland@epfl.ch;volkan.cevher@epfl.ch,Probabilistic Methods/MCMC,Spotlight
1504,Moonshine: Distilling with Cheap Convolutions,"Many engineers wish to deploy modern neural networks in memory-limited settings; but the development of flexible methods for reducing memory use is in its infancy, and there is little knowledge of the resulting cost-benefit. We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used. Using attention transfer, we provide Pareto curves/tables for distillation of residual networks with four benchmark datasets, indicating the memory versus accuracy payoff. We show that substantial memory savings are possible with very little loss of accuracy, and confirm that distillation provides student network performance that is better than training that student architecture directly on data.","Elliot Crowley*,University of Edinburgh;Gavin Gray,University of Edinburgh;Amos Storkey,U Edinburgh",elliot.j.crowley@ed.ac.uk;G.D.B.Gray@sms.ed.ac.uk;a.storkey@ed.ac.uk,Deep Learning/CNN Architectures*; Applications/Hardware and Systems,Poster
1520,Stochastic Cubic Regularization for Fast Nonconvex Optimization,"This paper proposes a stochastic variant of a classic algorithm---the cubic-regularized Newton method [Nesterov and Polyak]. The proposed algorithm efficiently escapes saddle points and finds approximate local minima for general smooth, nonconvex functions in only $\mathcal{\tilde{O}}(\epsilon^{-3.5})$ stochastic gradient and stochastic Hessian-vector product evaluations. The latter can be computed as efficiently as stochastic gradients. This improves upon the $\mathcal{\tilde{O}}(\epsilon^{-4})$ rate of stochastic gradient descent. Our rate matches the best-known result for finding local minima without requiring any delicate acceleration or variance-reduction techniques. ","Nilesh Tripuraneni*,UC Berkeley;Mitchell Stern,UC Berkeley;Chi Jin,UC Berkeley;Jeffrey Regier,UC Berkeley;Michael Jordan,UC Berkeley",nilesh_tripuraneni@berkeley.edu;mitchell@berkeley.edu;chijin@berkeley.edu;regier@berkeley.edu;jordan@cs.berkeley.edu,Optimization/Non-Convex Optimization,Oral
1521,Adaptation to Easy Data in Prediction with Limited Advice,"We derive an online learning algorithm with improved regret guarantees for ``easy'' loss sequences. We consider two types of ``easiness'': (a) stochastic loss sequences and (b) adversarial loss sequences with small effective range of the losses. While a number of algorithms have been proposed for exploiting small effective range in the full information setting, Gerchinovitz and Lattimore [2016] have shown the impossibility of regret scaling with the effective range of the losses in the bandit setting. We show that just one additional observation per round is sufficient to bypass the impossibility result. The proposed Second Order Difference Adjustments (SODA) algorithm requires no prior knowledge of the effective range of the losses, $\varepsilon$, and achieves an $O(\varepsilon \sqrt{KT \ln K}) + \tilde{O}(\varepsilon K \sqrt[4]{T})$ expected regret guarantee, where $T$ is the time horizon and $K$ is the number of actions. The scaling with the effective loss range is achieved under significantly weaker assumptions than those made by Cesa-Bianchi and Shamir [2018] in an earlier attempt to bypass the impossibility result. We also provide regret lower bound of $\Omega(\varepsilon\sqrt{T K})$, which almost matches the upper bound. In addition, we show that in the stochastic setting SODA achieves an $O\left(\sum_{a:\Delta_a>0} \frac{K\varepsilon^2}{\Delta_a}\right)$ pseudo-regret bound that holds simultaneously with the adversarial regret guarantee. In other words, SODA is safe against an unrestricted oblivious adversary and provides improved regret guarantees for at least two different types of ``easiness'' simultaneously.","Tobias Thune*,University of Copenhagen;Yevgeny Seldin,University of Copenhagen",tobias.thune@di.ku.dk;yevgeny.seldin@gmail.com,Algorithms/Bandit Algorithms*; Algorithms/Online Learning,Poster
1532,Differentially Private Bayesian Inference for Exponential Families,"The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods, it gives properly calibrated posterior beliefs in the non-asymptotic data regime.","Garrett Bernstein*,University of Massachusetts Amherst;Daniel Sheldon,University of Massachusetts, Amherst",gbernstein@gmail.com;sheldon@cs.umass.edu,"Probabilistic Methods/Latent Variable Models*; Applications/Privacy, Anonymity, and Security",Poster
1533,Playing hard exploration games by watching YouTube,"Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent‚Äôs exact environment setup and the demonstrator‚Äôs action and reward trajectories. Here we propose a method that overcomes these limitations in two stages. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to learn a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma‚Äôs Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.","Yusuf Aytar*,DeepMind;Tobias Pfaff,DeepMind;David Budden,DeepMind;Thomas Paine,DeepMind;Ziyu Wang,-;Nando de Freitas,DeepMind",yusufaytar@google.com;tpfaff@google.com;budden@google.com;tpaine@google.com;ziyu@google.com;nandodefreitas@google.com,Reinforcement Learning and Planning/Reinforcement Learning*; Applications/Game Playing; Deep Learning; Reinforcement Learning and Planning; Reinforcement Lear,Spotlight
1534,Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base,"We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. To handle enormous ellipsis phenomena in conversation, we introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances. Dialog memory management is embodied in a generative model, in which a logical form is interpreted in a top-down manner following a small and flexible grammar. We learn the model from denotations without explicit annotation of logical forms, and evaluate it on a large-scale dataset consisting of 200K dialogs over 12.8M entities. Results verify the benefits of modeling dialog memory, and show that our semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin. ","Daya Guo*,Sun Yat-Sen University;Duyu Tang,Microsoft Research;Nan Duan,Microsoft Research;Ming Zhou,Microsoft Research;Jian Yin,Sun Yat-Sen University",guody5@mail2.sysu.edu.cn;dutang@microsoft.com;nanduan@microsoft.com;mingzhou@microsoft.com;issjyin@mail.sysu.edu.cn,Applications/Natural Language Processing,Poster
1538,Norm-Ranging LSH for Maximum Inner Product Search,"Neyshabur and Srebro proposed Simple-LSH, which is the state-of-the-art hashing method for maximum inner product search (MIPS) with performance guarantee. We found that the performance of Simple-LSH, in both theory and practice, suffers from long tails in the 2-norm distribution of real datasets. We propose Norm-ranging LSH, which addresses the excessive normalization problem caused by long tails in Simple-LSH by partitioning a dataset into multiple sub-datasets and building a hash index for each sub-dataset independently. We prove that Norm-ranging LSH has lower query time complexity than Simple-LSH. We also show that the idea of partitioning the dataset can improve other hashing based methods for MIPS. To support efficient query processing on the hash indexes of the sub-datasets, a novel similarity metric is formulated. Experiments show that Norm-ranging LSH achieves an order of magnitude speedup over Simple-LSH for the same recall, thus significantly benefiting applications that involve MIPS.     ","Xiao Yan*,The Chinese University of Hong Kong;Jinfeng Li,The Chinese University of Hong Kong;Xinyan Dai,;Hongzhi Chen,CUHK;James Cheng,CUHK",xyan@cse.cuhk.edu.hk;jfli@cse.cuhk.edu.hk;xydai@cse.cuhk.edu.hk;hzchen@cse.cuhk.edu.hk;jcheng@cse.cuhk.edu.hk,Applications/Information Retrieval,Poster
1544,Optimization over Continuous and Multi-dimensional Decisions with Observational Data,"We consider the optimization of an uncertain objective over continuous and multi-dimensional decision spaces in problems in which we are only provided with observational data. We propose a novel algorithmic framework that is tractable, asymptotically consistent, and superior to comparable methods on example problems. Our approach leverages predictive machine learning methods and incorporates information on the uncertainty of the predicted outcomes for the purpose of prescribing decisions. We demonstrate the efficacy of our method on examples involving both synthetic and real data sets.","Dimitris Bertsimas,MIT;Christopher McCord*,MIT",dbertsim@mit.edu;cgm708@gmail.com,Algorithms*; Probabilistic Methods/Causal Inference,Poster
1546,Fast Estimation of Causal Interactions using Wold Processes,"We here focus on the task of learning Granger causality matrices for multivariate point processes. In order to tackle this task, our work is the first to explore Wold processes. By doing so, we are able to develop asymptotically fast MCMC learning algorithms. With $N$ being the total number of events and $K$ the number of processes, our learning algorithm has a $O(N(log(N)+log(K))$ cost per iteration. This is much faster than the $O(N^3 K^2)$ or $O(K^3)$ for the state of the art. Our approach, called Granger-Busca, is validated on real-world data being three times more accurate (in Precision@10) than recent baselines.  Granger-Busca is also the only approach able to train models for large sets of data.","Flavio Figueiredo*,UFMG;Guilherme Borges,UFMG;Pedro O.S. Vaz de Melo,Universidade Federal de Minas Gerais, Brazil;Renato Assun√ß√£o,UFMG",flaviovdf@dcc.ufmg.br;guilhermeborges@dcc.ufmg.br;olmo@dcc.ufmg.br;assuncao@dcc.ufmg.br,Probabilistic Methods/Causal Inference*; Applications/Network Analysis; Applications/Time Series Analysis; Applications/Web Applicat,Poster
1547,When do random forests fail?,"Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions.
In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. 
We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. 
As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent.  
As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests. ","Cheng Tang*,George Washington University;Damien Garreau,Max Planck Institute;Ulrike von Luxburg,University of T√ºbingen",tangch@gwmail.gwu.edu;damien.garreau@tuebingen.mpg.de;luxburg@informatik.uni-tuebingen.de,Algorithms/Boosting and Ensemble Methods*; Algorithms/Regression; Theory/Learning Theory,Poster
1560,Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes,"While designing the state space of an MDP, it is common to include states that are transient or not reachable by any policy (e.g., in mountain car, the product space of speed and position contains configurations that are not physically reachable). This leads to defining weakly-communicating or multi-chain MDPs. In this paper, we introduce TUCRL, the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process (MDP) without requiring any form of prior knowledge. In particular, for any MDP with Sc communicating states, A actions and Gc < Sc possible communicating next states, we derive a O(Dc \sqrt{Gc Sc A T}) regret bound, where Dc is the diameter (i.e., the longest shortest path) of the communicating part of the MDP. This is in contrast with optimistic algorithms (e.g., UCRL, Optimistic PSRL) that suffer linear regret in weakly-communicating MDPs, as well as posterior sampling or regularised algorithms (e.g., REGAL), which require prior knowledge on the bias span of the optimal policy to bias the exploration to achieve sub-linear regret. We also prove that in weakly-communicating MDPs, no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP. Finally, we report numerical simulations supporting our theoretical findings and showing how TUCRL overcomes the limitations of the state-of-the-art.","Ronan Fruit*,INRIA Lille - Sequel Team;Matteo Pirotta,INRIA Lille - Sequel Team;Alessandro Lazaric,FAIR",ronan.fruit@inria.fr;matteo.pirotta@inria.fr;lazaric@fb.com,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Exploration,Spotlight
1561,Optimistic Optimization of a Brownian,"In this paper, we address the problem of optimizing a Brownian motion. More precisely, we consider a (random) realization $W$ of a Brownian motion on $[0,1]$. Now, given this function, our goal is to return an $\epsilon$-approximation of its maximum using the smallest possible number of function evaluations. This number is called sample complexity of the algorithm. We provide an algorithm with sample complexity of order $\log^2(1/\epsilon)$. This improves over previous results of Al-Mharmah 7 and Calvin [1996] and Calvin et al. [2017]  which provided polynomial rates only. Our algorithm is adaptive --- each query depends on previous values --- and can be seen as an instance of the  optimism-in-the-face-of-uncertainty principle. ","Jean-Bastien Grill,Inria Lille - Nord Europe;Michal Valko*,Inria;Remi Munos,DeepMind",jeanbastien.grill@gmail.com;michal.valko@inria.fr;munos@google.com,Algorithms/Bandit Algorithms*; Algorithms/Online Learning,Poster
1568,Practical Methods for Graph Two-Sample Testing,"Hypothesis testing for graphs has been an important tool in several applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question.
In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests, or more precisely, their bootstrapped variants. We also propose two new tests based on asymptotic distributions, and show that the proposed tests are computationally less expensive and, in some cases, more reliable than the existing methods.","Debarghya Ghoshdastidar*,University of T√ºbingen;Ulrike von Luxburg,University of T√ºbingen",debarghya.ghoshdastidar@uni-tuebingen.de;luxburg@informatik.uni-tuebingen.de,Applications/Network Analysis*; Theory/Frequentist Statistics,Poster
1571,NAIS-Net: Stable Deep Networks from Non-Autonomous  Differential Equations,"This paper introduces Non-Autonomous Input-Output Stable Network (NAIS-Net), a very deep architecture where each stacked processing block is derived from a time-invariant non-autonomous dynamical system. Non-autonomy is implemented by skip connections from the block input to each of the unrolled processing stages and allows stability to be enforced so that blocks can be unrolled adaptively to a  pattern-dependent processing depth. NAIS-Net induces non-trivial, Lipschitz input-output maps, even for an infinite unroll length. We prove that the network is globally asymptotically stable so that for every initial condition there is exactly one input-dependent equilibrium assuming tanh units, and multiple stable equilibria for ReL units. An efficient implementation that enforces the stability under derived conditions for both fully-connected and convolutional layers is also presented. Experimental results show how NAIS-Net exhibits stability in practice, yielding a significant reduction in generalization gap compared to ResNets.","Marco Ciccone*,Politecnico di Milano;Marco Gallieri,NNAISENSE;Jonathan Masci,NNAISENSE;Christian Osendorfer,NNAISENSE;Faustino Gomez,NNAISENSE",marco.ciccone@polimi.it;marco@nnaisense.com;jonathan@nnaisense.com;christian@nnaisense.com;tino@nnaisense.com,Deep Learning*; Algorithms/Dynamical Systems; Algorithms/Representation Learning; Theory/Control Theory,Poster
1573,On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport,"Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.","L√©na√Øc Chizat*,INRIA;Francis Bach,INRIA - Ecole Normale Sup√©rieure",lenaic.csl@gmail.com;francis.bach@inria.fr,Optimization/Non-Convex Optimization*; Optimization/Convex Optimization,Poster
1578,Constructing Deep Neural Networks by Bayesian Network Structure Learning,"We introduce a principled approach for unsupervised structure learning of deep neural networks. We propose a new interpretation for depth and inter-layer connectivity where conditional independencies in the input distribution are encoded hierarchically in the network structure. Thus, the depth of the network is determined inherently. The proposed method casts the problem of neural network structure learning as a problem of Bayesian network structure learning. Then, instead of directly learning the discriminative structure, it learns a generative graph, constructs its stochastic inverse, and then constructs a discriminative graph.
We prove that conditional-dependency relations among the latent variables in the generative graph are preserved in the class-conditional discriminative graph. 
We demonstrate on image classification benchmarks that the deepest layers (convolutional and dense) 
of common networks can be replaced by significantly smaller learned structures, while maintaining classification accuracy---state-of-the-art on tested benchmarks. Our structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.","Raanan Rohekar*,Intel Corporation;Shami Nisimov,Intel Corporation;Yaniv Gurwicz,Intel Corporation;Guy Koren,Intel Corporation;Gal Novik,Intel Corporation",raanan.y.yehezkel.rohekar@intel.com;shami.nisimov@intel.com;yaniv.gurwicz@intel.com;guy.koren@intel.com;gal.novik@intel.com,Algorithms/Model Selection and Structure Learning*; Algorithms/Unsupervised Learning; Deep Learning,Poster
1587,Weakly Supervised Dense Event Captioning in Videos,"Dense event captioning aims to detect and describe all events of interest contained in a video. Despite the advanced development in this area, existing methods tackle this task by making use of dense temporal annotations, which is dramatically source-consuming. This paper formulates a new problem: weakly supervised dense event captioning, which does not require temporal segment annotations for model training.  Our solution is based on the one-to-one correspondence assumption, each caption describes one temporal segment, and each temporal segment has one caption, which holds in current benchmark datasets and  most real world cases. We decompose the problem into a pair of dual problems: event captioning and sentence localization and present a cycle system to train our model. Extensive experimental results are provided to  demonstrate the ability of our model  on both dense event captioning and sentence localization in videos.","Xuguang Duan,Tsinghua University;Wenbing Huang*,Tencent AI Lab;Chuang Gan,MIT;Jingdong Wang,Microsoft Research;Wenwu Zhu,Tsinghua University;Junzhou Huang,University of Texas at Arlington",dxg18@mails.tsinghua.edu.cn;hwenbing@126.com;ganchuang1990@gmail.com;jingdw@microsoft.com;wwzhu@tsinghua.edu.cn;jzhuang@uta.edu,Applications/Activity and Event Recognition*; Applications/Video Analysis,Poster
1588,Faithful Inversion of Generative Models for Effective Amortized Inference,"Inference amortization methods share information across multiple posterior-inference problems, allowing each to be carried out more efficiently. Generally, they require the inversion of the dependency structure in the generative model, as the modeller must learn a mapping from observations to distributions approximating the posterior. Previous approaches have involved inverting the dependency structure in a heuristic way that fails to capture these dependencies correctly, thereby limiting the achievable accuracy of the resulting approximations. We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model. Such inverses have two crucial properties: (a) they do not encode any independence assertions that are absent from the model and; (b) they are local maxima for the number of true independencies encoded. We prove the correctness of our approach and empirically show that the resulting minimally faithful inverses lead to better inference amortization than existing heuristic approaches.","Stefan Webb*,Oxford;Adam Golinski,University of Oxford;Rob Zinkov,University of Oxford;N Siddharth,Unversity of Oxford;Tom Rainforth,University of Oxford;Yee Whye Teh,University of Oxford;Frank Wood,University of British Columbia",stefandwebb@gmail.com;adamg@robots.ox.ac.uk;zinkov@robots.ox.ac.uk;nsid@robots.ox.ac.uk;twgr@robots.ox.ac.uk;y.w.teh@stats.ox.ac.uk;fwood@cs.ubc.ca,Probabilistic Methods/Variational Inference*; Deep Learning/Generative Models,Poster
1591,From Stochastic Planning to Marginal MAP,"It is well known that the problems of stochastic planning and probabilistic inference are closely related. This paper makes two contributions in this context. The first is to provide an analysis of the recently developed SOGBOFA heuristic planning algorithm, that was shown to be effective for problems with large factored state and action spaces. It is shown that SOGBOFA can be seen as a specialized inference algorithm that computes its solutions through a combination of a symbolic variant of belief propagation and gradient ascent. The second contribution is a new solver for Marginal MAP (MMAP) inference. We introduce a new reduction from MMAP to maximum expected utility problems which are  suitable for the symbolic computation in SOGBOFA. This yields a novel algebraic gradient-based solver (AGS) for MMAP. An experimental evaluation illustrates the potential of AGS in solving difficult MMAP problems. ","Hao Cui,Tufts University;Radu Marinescu,IBM;Roni Khardon*,Tufts University",Hao.Cui@tufts.edu;radu.marinescu@ie.ibm.com;roni@cs.tufts.edu,Reinforcement Learning and Planning/Planning*; Probabilistic Methods; Probabilistic Methods/Belief Propagation; Reinforcement Learning and Plann,Poster
1594,On Binary Classification in Extreme Regions,"In pattern recognition, a random label Y is to be predicted based upon observing a random vector X valued in $\mathbb{R}^d$ with d>1 by means of a classification rule with minimum probability of error. In a wide variety of applications, ranging from finance/insurance to environmental sciences through teletraffic data analysis for instance, extreme (i.e. very large) observations X are of crucial importance, while contributing in a negligible manner to the (empirical) error however, simply because of their rarity. As a consequence, empirical risk minimizers generally perform very poorly in extreme regions. It is the purpose of this paper to develop a general framework for classification in the extremes. Precisely, under non-parametric heavy-tail assumptions for the class distributions, we prove that a natural and asymptotic notion of risk, accounting for predictive performance in extreme regions of the input space, can be defined and show that minimizers of an empirical version of a non-asymptotic approximant of this dedicated risk, based on a fraction of the largest observations, lead to classification rules with good generalization capacity, by means of maximal deviation inequalities in low probability regions. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed.","Hamid Jalalzai,Telecom ParisTech;Stephan Clemencon*,Telecom ParisTech;Anne Sabourin,Telecom ParisTech",hamid.jalalzai@telecom-paristech.fr;clemenco@telecom-paristech.fr;anne.sabourin@telecom-paristech.fr,Theory*; Theory/Learning Theory,Poster
1595,Near-Optimal Policies for Dynamic Multinomial Logit Assortment Selection Models,"In this paper we consider the dynamic assortment selection problem under an uncapacitated multinomial-logit (MNL) model. By carefully analyzing a revenue  potential function, we show that a trisection based algorithm achieves an item-independent regret bound of O(sqrt(T log log T), which matches information theoretical lower bounds up to iterated logarithmic terms. Our proof technique draws tools from the unimodal/convex bandit literature as well as adaptive confidence parameters in minimax multi-armed bandit problems.","Yining Wang*,CMU;Xi Chen,NYU;Yuan Zhou,Indiana University Bloomington",ynwang.yining@gmail.com;xichen@nyu.edu;yzhoucs@iu.edu,Algorithms/Online Learning*; Theory/Learning Theory,Poster
1596,Q-learning with Nearest Neighbors,"We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available.  We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a $d$-dimensional state space and the discounted factor $\gamma \in (0,1)$, given an arbitrary sample path with ``covering time'' $L$, we establish that the algorithm is guaranteed to output an $\varepsilon$-accurate estimate of the optimal Q-function using  $\Ot(L/(\varepsilon^3(1-\gamma)^7))$ samples. For instance, for a well-behaved MDP, the covering time of the sample path under the purely random policy scales as $\Ot(1/\varepsilon^d),$ so the sample complexity scales as $\Ot(1/\varepsilon^{d+3}).$ Indeed, we establish a lower bound that argues that the dependence of $ \Omegat(1/\varepsilon^{d+2})$ is necessary. ","Devavrat Shah,MIT;Qiaomin Xie*,MIT",devavrat@mit.edu;qxie@mit.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Theory/Learning Theory,Poster
1598,Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization,"We present a unified framework to analyze the global convergence of Langevin dynamics based algorithms for nonconvex finite-sum optimization with $n$ component functions.  At the core of our analysis is a direct analysis of the ergodicity of the numerical approximations to Langevin dynamics, which leads to faster convergence rates. Specifically, we show that gradient Langevin dynamics (GLD) and stochastic gradient Langevin dynamics (SGLD)  converge to the \textit{almost minimizer}\footnote{Following \citet{raginsky2017non}, an almost minimizer is defined to be a point which is within the ball of the global minimizer with radius $O(d\log(\beta+1)/\beta)$, where $d$ is the problem dimension and $\beta$ is the inverse temperature parameter.} within $\tilde O\big(nd/(\lambda\epsilon) \big)$\footnote{$\tO(\cdot)$ notation hides polynomials of logarithmic terms and constants.} and $\tilde O\big(d^7/(\lambda^5\epsilon^5) \big)$ stochastic gradient evaluations respectively, where $d$ is the problem dimension, and $\lambda$ is the spectral gap of the Markov chain generated by GLD. Both results improve upon the best known gradient complexity\footnote{Gradient complexity is defined as the total number of stochastic gradient evaluations of an algorithm, which is the number of stochastic gradients calculated per iteration times the total number of iterations.} results \citep{raginsky2017non}. 
Furthermore, for the first time we prove the global convergence guarantee for variance reduced stochastic gradient Langevin dynamics (VR-SGLD) to the almost minimizer within $\tilde O\big(\sqrt{n}d^5/(\lambda^4\epsilon^{5/2})\big)$ stochastic gradient evaluations, which outperforms the gradient complexities of GLD and SGLD in a wide regime.  
Our theoretical analyses shed some light on using Langevin dynamics based algorithms for nonconvex optimization with provable guarantees.","Pan Xu,UCLA;Jinghui Chen,University of Virginia;Difan Zou,University of California, Los Angeles ;Quanquan Gu*,UCLA",px3ds@virginia.edu;jc4zg@virginia.edu;dz5an@virginia.edu;qg5w@virginia.edu,Probabilistic Methods/MCMC*; Optimization/Non-Convex Optimization,Spotlight
1602,Efficiency of adaptive importance sampling,"The \textit{sampling policy} of stage $t$, formally expressed as a probability density function $q_t$, stands for the distribution of the sample $(x_{t,1},\ldots, x_{t,n_t})$ generated at $t$. 
From the past samples, some information depending on some \textit{objective} is derived leading eventually to update the sampling policy to $q_{t+1}$. This generic approach characterizes \textit{adaptive importance sampling} (AIS) schemes. Each stage $t$ is formed with two steps : (i) to explore the space with $n_t$ points according to $q_t$ and (ii) to exploit the current amount of information to update the sampling policy. The very fundamental question raised in the paper concerns the behavior of empirical sums based on AIS. Without making any assumption on $n_t$, the theory developed involves no restriction on the split of computational resources between the explore (i) and the exploit (ii) step. It is shown that the asymptotic behavior of AIS is the same as some ``oracle'' strategy that knows the optimal sampling policy from the beginning. From a practical perspective, weighted AIS is introduced, a new method that allows to forget poor samples from early stages.","Fran√ßois Portier*,T√©l√©com ParisTech;Bernard Delyon,University of Rennes 1",francois.portier@gmail.com;bernard.delyon@univ-rennes1.fr,Theory/Large Deviations and Asymptotic Analysis*; Algorithms/Online Learning; Probabilistic Methods/MCMC,Poster
1605,Learning latent variable structured prediction models with Gaussian perturbations,"The standard margin-based structured prediction commonly uses a maximum loss over all possible structured outputs [23, 1, 5, 22]. The large-margin formulation including latent variables [27, 18] not only results in a non-convex formulation but also increases the search space by a factor of the size of the latent space. Recent work [11] has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution, with theoretical guarantees. We extend this work by including latent variables. We study a new family of loss functions under Gaussian perturbations and analyze the effect of the latent space on the generalization bounds. We show that the non-convexity of learning with latent variables originates naturally, as it relates to a tight upper bound of the Gibbs decoder distortion with respect to the latent space. Finally, we provide a formulation using random samples that produces a tighter upper bound of the Gibbs decoder distortion up to a statistical accuracy, which enables a faster evaluation of the objective function.  We illustrate the method with synthetic experiments and a computer vision application.","Kevin Bello*,Purdue University;Jean Honorio,Purdue",kbellome@purdue.edu;jhonorio@purdue.edu,Theory/Learning Theory*; Algorithms/Structured Prediction,Poster
1614,The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal,"We analyze the Kozachenko‚ÄìLeonenko (KL) fixed k-nearest neighbor estimator for the differential entropy. We obtain the first uniform upper bound on its performance for any fixed k over H\""{o}lder balls on a torus without assuming any conditions on how close the density could be from zero. Accompanying a recent minimax lower bound over the H\""{o}lder ball, we show that the KL estimator for any fixed k is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter s of the H\""{o}lder ball for $s \in (0,2]$ and arbitrary dimension d, rendering it the first estimator that provably satisfies this property.","Jiantao Jiao,University of California, Berkeley;Weihao Gao*,UIUC;Yanjun Han,Stanford University",jiantao@berkeley.edu;wgao9@illinois.edu;yjhan@stanford.edu,Theory/Information Theory,Spotlight
1615,Deep Reinforcement Learning of Marked Temporal Point Processes,"In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent's policy using the intensity and mark distribution of the corresponding process and then derive 
a flexible policy gradient method, which embeds the agent's actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in viral marketing and personalized teaching and, using data gathered from Twitter and Duolingo, we show that it may be able to find interventions to help marketers and learners achieve their goals more effectively than alternatives.","Utkarsh Upadhyay,MPI-SWS;Abir De,MPI-SWS;Manuel Gomez Rodriguez*,Max-Planck Institute",utkarshu@mpi-sws.org;abir.iitkgp@gmail.com;manuelgr@mpi-sws.org,Reinforcement Learning and Planning/Reinforcement Learning*; Applications/Time Series Analysis,Poster
1625,Evidential Deep Learning to Quantify Classification Uncertainty,"Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet prior on the softmax output, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data.  The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-sample queries and endurance against adversarial perturbations.","Murat Sensoy*,Ozyegin University;Lance Kaplan,U.S. Army Research Laboratory;Melih Kandemir,Bosch Center for Artificial Intelligence",murat.sensoy@ozyegin.edu.tr;lance.m.kaplan.civ@mail.mil;melih.kandemir@bosch.com,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Classification; Deep Learning",Poster
1628,Parsimonious Bayesian deep networks,"Combining Bayesian nonparametrics and a forward model selection strategy, we construct parsimonious Bayesian deep networks (PBDNs) that infer capacity-regularized network architectures from the data and require neither cross-validation nor fine-tuning when training the model. One of the two essential components of a PBDN is the development of a special infinite-wide single-hidden-layer neural network, whose number of active hidden units can be inferred from the data. The other one is the construction of a greedy layer-wise learning algorithm that uses a forward model selection criterion to determine when to stop adding another hidden layer. We develop both Gibbs sampling and stochastic gradient descent based maximum a posteriori inference for PBDNs, providing state-of-the-art classification accuracy and interpretable data subtypes near the decision boundaries, while maintaining low computational complexity for out-of-sample prediction. ","Mingyuan Zhou*,University of Texas at Austin",Mingyuan.Zhou@mccombs.utexas.edu,Probabilistic Methods/Bayesian Nonparametrics*; Algorithms/Model Selection and Structure Learning; Deep Learning,Poster
1637,Single-Agent Policy Tree Search With Guarantees,"We introduce two novel tree search algorithms that use a policy to guide
search. The first algorithm is a best-first enumeration that uses a cost
function that allows us to provide an upper bound on the number of nodes
to be expanded before reaching a goal state. We show that this best-first
algorithm is particularly well suited for ``needle-in-a-haystack'' problems.
The second algorithm, which is based on sampling, provides an
upper bound on the expected number of nodes to be expanded before
reaching a set of goal states. We show that this algorithm is better
suited for problems where many paths lead to a goal. We validate these tree
search algorithms on 1,000 computer-generated levels of Sokoban, where the
policy used to guide search comes from a neural network trained using A3C. Our
results show that the policy tree search algorithms we introduce are
competitive with a state-of-the-art domain-independent planner that uses
heuristic search.","Laurent Orseau*,DeepMind;Levi Lelis,Universidade Federal de Vi√ßosa;Tor Lattimore,DeepMind;Theophane Weber,DeepMind",lorseau@google.com;levilelis@gmail.com;tor.lattimore@gmail.com;theophane@google.com,Reinforcement Learning and Planning/Planning*; Applications/Game Playing; Deep Learning/CNN Architectures,Poster
1639,Semi-crowdsourced Clustering with Deep Generative Models,"We consider the semi-supervised clustering problem where crowdsourcing provides
 noisy information about the pairwise comparisons on a subset of data, i.e., some
 sample pairs are (or are not) in the same clusters. We propose a new approach
 for clustering, which effectively combines the low-level features and a subset of
 noisy pairwise annotations. We build a deep generative model to characterize
 the generative process of low-level features and a relational model for the noisy
 pairwise annotations, which share the latent variables. Fast amortized and natural
gradient stochastic variational inference algorithms are developed for the model
 and its fully Bayesian variant. Our empirical results on synthetic and real-world datasets show that the proposed method outperforms previous methods.","Yucen Luo*,Tsinghua University;Tian Tian,Tsinghua University;Jiaxin Shi,Tsinghua University;Jun Zhu,Tsinghua University;Bo Zhang,Tsinghua University",luoyc15@mails.tsinghua.edu.cn;rossowhite@163.com;shijx15@mails.tsinghua.edu.cn;dcszj@tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,Probabilistic Methods/Latent Variable Models*; Deep Learning/Generative Models; Probabilistic Methods/Variational Inference,Poster
1640,The committee machine: Computational to statistical gaps in learning a two-layers neural network,"Heuristic tools from statistical physics have been used in the past to compute the optimal learning and generalization errors in the teacher-student scenario in multi- layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.","Benjamin Aubin,Ipht Saclay;Antoine Maillard,Ecole Normale Sup√©rieure;Jean Barbier,√âcole Polytechnique de Lausanne;Florent Krzakala*,√âcole Normale Sup√©rieure;Nicolas Macris,√âcole Polytechnique de Lausanne;Lenka Zdeborova,CEA Saclay",aubin.benjamin@gmail.com;antoine.maillard@ens.fr;jean.barbier@epfl.ch;florent.krzakala@gmail.com;nicolas.macris@epfl.ch;lenka.zdeborova@gmail.com,Theory/Statistical Physics of Learning*; Probabilistic Methods/Belief Propagation; Theory/Information Theory; Theory/Large Deviation,Spotlight
1645,Realistic Evaluation of Deep Semi-Supervised Learning Algorithms,"Semi-supervised learning (SSL) provides a powerful framework for leveraging
unlabeled data when labels are limited or expensive to obtain. SSL algorithms based
on deep neural networks have recently proven successful on standard benchmark
tasks. However, we argue that these benchmarks fail to address many issues that
these algorithms would face in real-world applications. After creating a unified
reimplementation of various widely-used SSL techniques, we test them in a suite
of experiments designed to address these issues. We find that the performance
of simple baselines which do not use unlabeled data is often underreported, that
SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and
that performance can degrade substantially when the unlabeled dataset contains
out-of-class examples. To help guide SSL research towards real-world applicability,
we make our unified reimplemention and evaluation platform publicly available.","Avital Oliver*,Google Brain;Augustus Odena,Google Brain;Colin Raffel,Google;Ekin Cubuk,Google Brain;Ian Goodfellow,Google Brain",avitalo@google.com;augustusodena@google.com;craffel@google.com;cubuk@google.com;goodfellow@google.com,"Data, Competitions, Implementations, and Software/Benchmarks*; Algorithms/Semi-Supervised Learning",Spotlight
1652,Contextual Combinatorial Multi-armed Bandits with Volatile Arms and Submodular Reward,"In this paper, we study the stochastic contextual combinatorial multi-armed bandit (CC-MAB) framework that is tailored for volatile arms and submodular reward functions. CC-MAB inherits properties from both contextual bandit and combinatorial bandit: it aims to select a set of arms in each round based on the side information (a.k.a. context) associated with the arms. By ``volatile arms'', we mean that the available arms to select from in each round may change; and by ``submodular rewards'', we mean that the total reward achieved by selected arms is not a simple sum of individual rewards but demonstrates a feature of diminishing returns determined by the relations between selected arms (e.g. relevance and redundancy). Volatile arms and submodular rewards are often seen in many real-world applications, e.g. recommender systems and crowdsourcing, in which multi-armed bandit (MAB) based strategies are extensively applied. Although there exist works that investigate these issues separately based on standard MAB, jointly considering all these issues in a single MAB problem requires very different algorithm design and regret analysis. Our algorithm CC-MAB provides an online decision-making policy in a contextual and combinatorial bandit setting and effectively addresses the issues raised by volatile arms and submodular reward functions. The proposed algorithm is proved to achieve $O(cT^{\frac{2\alpha+D}{3\alpha + D}}\log(T))$ regret after a span of $T$ rounds. The performance of CC-MAB is evaluated by experiments conducted on a real-world crowdsourcing dataset, and the result shows that our algorithm outperforms the prior art. ","Lixing Chen,University of Miami;Jie Xu*,University of Miami;Zhuo  Lu,University of South Florida",lx.chen@miami.edu;jiexu@miami.edu;zhuolu@usf.edu,Algorithms/Bandit Algorithms,Poster
1658,Training deep learning based denoisers without ground truth data,"  Recent deep learning based denoisers often outperform state-of-the-art conventional denoisers such as BM3D. They are typically trained to minimize the mean squared error (MSE) between the output of a deep neural network and the ground truth image. In deep learning based denoisers, it is important to use high quality noiseless ground truth for high performance, but it is often challenging or even infeasible to obtain such a clean image in application areas such as hyperspectral remote sensing and medical imaging. We propose a Stein's Unbiased Risk Estimator (SURE) based method for training deep neural network denoisers only with noisy images. We demonstrated that our SURE based method without ground truth was able to train deep neural network denoisers to yield performance close to deep learning denoisers trained with ground truth and to outperform state-of-the-art BM3D. Further improvements were achieved by including noisy test images for training denoiser networks using our proposed SURE based method.","Shakarim Soltanayev,UNIST;Se Young Chun*,Ulsan National Institute of Science and Technology",shakarim@unist.ac.kr;sychun@unist.ac.kr,Applications/Denoising*; Applications/Signal Processing; Deep Learning/Few-Shot Learning Approaches,Poster
1669,Re-evaluating evaluation,"Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task.  The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation -- since there is no harm (computational cost aside) from including all available tasks and agents. ","David Balduzzi*,DeepMind;Karl Tuyls,Deepmind;Julien Perolat,DeepMind;Thore Graepel,DeepMind",dbalduzzi@gmail.com;karltuyls@google.com;perolat@google.com;thore@google.com,"Applications/Fairness, Accountability, and Transparency",Poster
1670,"Deep, complex, invertible  networks for inversion of transmission effects in multimode optical fibres","We use complex-weighted, deep convolutional networks to invert the effects of multimode optical fibre distortion of a coherent input image. We generated experimental data based on collections of optical fibre responses to greyscale, input images generated with coherent light, and measuring only image amplitude  (not amplitude and phase as is typical) at the output of the 10 metre long 105 micrometre diameter multimode fibre. This data is made available as the {\it Optical fibre inverse problem} Benchmark collection. The experimental data is used to train complex-weighted models with a range of regularisation approaches and subsequent denoising autoencoders. A new {\it unitary regularisation} approach for complex-weighted networks is proposed which performs best in robustly inverting the fibre transmission matrix, and which fits well with the physical theory. The use of unitary layers allows analytic inversion of the network via its complex conjugate transpose, and we demonstrate simultaneous optimisation of both the forward and inverse models.","Ois√≠n Moran,University of Glasgow;Piergiorgio Caramazza,University of Glasgow;Daniele Faccio,University of Glasgow;Roderick Murray-Smith*,University of Glasgow",oisin.moran@ucdconnect.ie;piergiorgio.caramazza@gmail.com;Daniele.Faccio@glasgow.ac.uk;rod@dcs.gla.ac.uk,Applications,Poster
1675,Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals,"Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8--12\,Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex.","Tom Dupr√© La Tour*,Telecom ParisTech;Thomas Moreau,Inria;Mainak Jas,Telecom ParisTech;Alexandre  Gramfort,Inria",tom.duprelatour@telecom-paristech.fr;thomas.moreau@inria.fr;mainak.jas@telecom-paristech.fr;alexandre.gramfort@inria.fr,Neuroscience and Cognitive Science/Brain Imaging*; Algorithms/Sparse Coding and Dimensionality Expansion,Poster
1679,Data-Efficient Hierarchical Reinforcement Learning,"Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a  number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.","Ofir Nachum*,Google;Shixiang Gu,University of Cambridge/Max Planck Institute for Intelligent Systems;Honglak Lee,Google;Sergey Levine,UC Berkeley",ofirnachum@google.com;sg717@cam.ac.uk;honglak@google.com;svlevine@eecs.berkeley.edu,Reinforcement Learning and Planning/Hierarchical RL*; Reinforcement Learning and Planning/Navigation; Reinforcement Learning and Planning/Reinforcem,Poster
1682,Speaker-Follower Models for Vision-and-Language Navigation,"Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this presents a double challenge: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and empirically difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to synthesize new instructions for data augmentation and to implement pragmatic reasoning for evaluating candidate action sequences. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three pieces of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.","Daniel Fried*,University of California, Berkeley;Ronghang Hu,University of California, Berkeley;Volkan Cirik,Carnegie Mellon University;Anna Rohrbach,UC Berkeley;Jacob Andreas,University of California, Berkeley;Louis-Philippe Morency,Carnegie Mellon University;Taylor Berg-Kirkpatrick,Carnegie Mellon University;Kate Saenko,Boston University;Dan Klein,University of California, Berkeley;Trevor Darrell,UC Berkeley",dfried@cs.berkeley.edu;ronghang@berkeley.edu;vcirik@cs.cmu.edu;anna.rohrbach@berkeley.edu;jda@cs.berkeley.edu;morency@cs.cmu.edu;tberg@cs.cmu.edu;saenko@bu.edu;klein@cs.berkeley.edu;trevor@eecs.berkeley.edu,Applications*; Applications/Computer Vision; Applications/Natural Language Processing; Applications/Visual,Poster
1687,Inequity aversion improves cooperation in intertemporal social dilemmas,"Groups of humans are often able to find ways to cooperate with one another in complex, temporally extended social dilemmas. Models based on behavioral economics are only able to explain this phenomenon for unrealistic stateless matrix games. Recently, multi-agent reinforcement learning has been applied to generalize social dilemma problems to temporally and spatially extended Markov games. However, this has not yet generated an agent that learns to cooperate in social dilemmas as humans do. A key insight is that many, but not all, human individuals have inequity averse social preferences. This promotes a particular resolution of the matrix game social dilemma wherein inequity-averse individuals are personally pro-social and punish defectors. Here we extend this idea to Markov games and show that it promotes cooperation in several types of sequential social dilemma, via a profitable interaction with policy learnability. In particular, we find that inequity aversion improves temporal credit assignment for the important class of intertemporal social dilemmas. These results help explain how large-scale cooperation may emerge and persist.","Edward Hughes*,Google DeepMind;Joel Z Leibo,DeepMind;Matthew Phillips,DeepMind;Karl Tuyls,Deepmind;Edgar Due√±ez-Guzman,DeepMind;Antonio Garc√≠a Casta√±eda,DeepMind;Iain Dunning,;Tina Zhu,DeepMind;Kevin McKee,DeepMind;Raphael Koster,DeepMind;Heather Roff,DeepMind;Thore Graepel,DeepMind",edwardhughes@google.com;jzl@google.com;matthew.phillips.12@ucl.ac.uk;karltuyls@google.com;duenez@google.com;antoniogc@google.com;idunning@google.com;tinazhu@google.com;kevinrmckee@google.com;rkoster@google.com;hroff@google.com;thore@google.com,Applications/Computational Social Science*; Reinforcement Learning and Planning/Multi-Agent RL,Poster
1690,Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization Bounds,"Gaussian Processes (GPs) are a generic modelling tool for supervised learning. While they have been successfully applied on large datasets, their use in safety-critical applications is hindered by the lack of good performance guarantees. To this end, we propose a method to learn GPs and their sparse approximations by directly optimizing a PAC-Bayesian bound on their generalization performance, instead of maximizing the marginal likelihood. Besides its theoretical appeal, we find in our evaluation that our learning method is robust and yields significantly better generalization guarantees than other common GP approaches on several regression benchmark datasets.","David Reeb*,Bosch Center for Artificial Intelligence (BCAI), Robert Bosch GmbH;Andreas Doerr,BCAI, MPI-IS AMD;Sebastian Gerwinn,Bosch Center for Artificial Intelligence (BCAI), Robert Bosch GmbH;Barbara Rakitsch,Bosch Center for Artificial Intelligence",David.Reeb@de.bosch.com;andreasdoerr@gmx.net;Sebastian.Gerwinn@de.bosch.com;barbara.rakitsch@googlemail.com,Probabilistic Methods/Gaussian Processes*; Algorithms/Kernel Methods; Algorithms/Regression; Probabilistic Methods/Bayesian Theory; Th,Poster
1693,High-dimensional Bayesian optimization via collaborative filtering,"In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data pre-processing methods and machine learning models, has long been one of the goals of the machine learning community. 
In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible ML pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.","Nicolo Fusi*,MICROSOFT;Rishit Sheth,Tufts University;Melih Elibol,University of California at Berkeley",fusi@microsoft.com;rishit.sheth@tufts.edu;elibol@gmail.com,Probabilistic Methods/Gaussian Processes,Poster
1695,Stochastic Spectral and Conjugate Descent Methods,"The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.","Dmitry Kovalev,MIPT;Peter Richtarik*,KAUST;Eduard Gorbunov,MIPT;Elnur Gasanov,MIPT",dakovalev1@mail.ru;richtarik@gmail.com;ed-gorbunov@yandex.ru;elnur.gasanov@phystech.edu,Optimization/Convex Optimization,Poster
1702,Recurrent Relational Networks,"This paper is concerned with learning to solve tasks that require a chain of interde-
pendent steps of relational inference, like answering complex questions about the
relationships between objects, or solving puzzles where the smaller elements of a
solution mutually constrain each other. We introduce the recurrent relational net-
work, a general purpose module that operates on a graph representation of objects.
As a generalization of Santoro et al. [2017]‚Äôs relational network, it can augment
any neural network model with the capacity to do many-step relational reasoning.
We achieve state of the art results on the bAbI textual question-answering dataset
with the recurrent relational network, consistently solving 20/20 tasks. As bAbI is
not particularly challenging from a relational reasoning point of view, we introduce
Pretty-CLEVR, a new diagnostic dataset for relational reasoning. In the Pretty-
CLEVR set-up, we can vary the question to control for the number of relational
reasoning steps that are required to obtain the answer. Using Pretty-CLEVR, we
probe the limitations of multi-layer perceptrons, relational and recurrent relational
networks. Finally, we show how recurrent relational networks can learn to solve
Sudoku puzzles from supervised training data, a challenging task requiring upwards
of 64 steps of relational reasoning. We achieve state-of-the-art results amongst
comparable methods by solving 96.6% of the hardest Sudoku puzzles.","Rasmus Palm*,Technical University Denmark;Ulrich Paquet,DeepMind;Ole Winther,DTU",rasmusbergpalm@gmail.com;upaq@google.com;ole.winther@gmail.com,"Deep Learning*; Algorithms/Classification; Algorithms/Relational Learning; Data, Competitions, Implementations",Poster
1713,But How Does It Work in Theory? Linear SVM with Random Features,"We prove that, under low noise assumptions, the support vector machine with $N\ll m$ random features (RFSVM) can achieve the learning rate faster than $O(1/\sqrt{m})$ on a training set with $m$ samples when an optimized feature map is used. Our work extends the previous fast rate analysis of random features method from least square loss to 0-1 loss. We also show that the reweighted feature selection method, which approximates the optimized feature map, helps improve the performance of RFSVM in experiments on a synthetic data set.","Yitong Sun*,University of Michigan;Anna Gilbert,University of Michigan;Ambuj Tewari,University of Michigan",syitong@umich.edu;annacg@umich.edu;tewaria@umich.edu,Algorithms/Kernel Methods*; Theory/Learning Theory; Theory/Regularization; Theory/Spaces of Functions and Kernels,Poster
1717,Learning to Optimize Tensor Programs,"We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.","Tianqi Chen*,University of Washington;Lianmin Zheng,Shanghai Jiao Tong University;Eddie Yan,university of washington;Ziheng Jiang,Fudan University;Thierry Moreau,university of washington;Luis Ceze,University of Washington;Carlos Guestrin,University of Washington	;Arvind Krishnamurthy,University of Washington",tqchen@cs.washington.edu;lianminzheng@gmail.com;eqy@cs.washington.edu;jzhtomas@gmail.com;moreau@cs.washington.edu;luisceze@cs.washington.edu;guestrin@cs.washington.edu;arvind@cs.washington.edu,Applications*; Applications/Hardware and Systems; Deep Learning,Spotlight
1726,Boosting Black Box Variational Inference,"Approximating a probability density in a tractable manner is a central task in Bayesian statistics. Variational Inference (VI) is a popular technique that achieves tractability by choosing a relatively simple representation set. Borrowing ideas from the classic boosting framework, recent approaches attempt to \emph{boost} VI by replacing the selection of a single density with a greedily constructed mixture of densities. In order to guarantee convergence, previous works impose stringent assumptions that require significant effort for practitioners. Specifically, they require a custom implementation of the greedy step (called the LMO) for every probabilistic model with respect to an unnatural variational family of truncated distributions. Our work fixes these issues with novel theoretical and algorithmic insights. On the theoretical side, we show that boosting VI satisfies a relaxed smoothness assumption which is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm. Furthermore, we rephrase the LMO problem and propose to maximize the Residual ELBO (RELBO) which replaces the standard ELBO optimization in VI. 
These theoretical enhancements allow for black box implementation of the boosting subroutine.  Finally, we present a stopping criterion drawn from the duality gap in the classic FW analyses. We also present exhaustive experiments to illustrate the usefulness of our theoretical and algorithmic contributions.","Francesco Locatello*,ETH Z√ºrich;Gideon Dresdner,ETH Z√ºrich;Rajiv Khanna,The University of Texas at Austin;Isabel Valera,MPI-IS;Gunnar Raetsch,ETH Zurich",locatelf@ethz.ch;dgideon@ethz.ch;rajivak@utexas.edu;isabel.valera@tuebingen.mpg.de;raetsch@inf.ethz.ch,Probabilistic Methods/Variational Inference,Spotlight
1737,Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes,"We prove that œ¥(k d^2 / Œµ^2) samples are necessary and sufficient for learning a mixture of k Gaussians in R^d, up to error Œµ in total variation distance. This improves both the known upper bounds and lower bounds for this problem. For mixtures of axis-aligned Gaussians, we show that O(k d / Œµ^2) samples suffice, matching a known lower bound.

The upper bound is based on a novel technique for distribution learning based on a notion of sample compression. Any class of distributions that allows such a sample compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in R^d has an efficient sample compression.","Hassan Ashtiani*,University of Waterloo;Shai Ben-David,University of Waterloo;Nick Harvey,University of British Columbia;Christopher Liaw,University of British Columbia;Abbas Mehrabian,Mcgill University;Yaniv Plan,University of British Columbia",mh.zokaei@gmail.com;shai@uwaterloo.ca;nickhar@cs.ubc.ca;cvliaw@cs.ubc.ca;abbas.mehrabian@gmail.com;yaniv@math.ubc.ca,Theory/Learning Theory*; Algorithms/Density Estimation,Oral
1760,Actor-Critic Policy Optimization in PartiallyObservable Multiagent Environments,"Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using RL-style function approximation. We evaluate on commonly-used benchmark Poker domains, comparing to several state-of-the-art baselines, showing empirical convergence to approximate Nash equilibria in self-play, without any domain-specific state space reductions.","Srinivasan Sriram,Google;Marc Lanctot*,Deepmind;Vinicius Zambaldi,DeepMind;Julien Perolat,DeepMind;Karl Tuyls,Deepmind;Remi Munos,DeepMind;Michael Bowling,DeepMind",srsrinivasan@google.com;lanctot@google.com;vzambaldi@google.com;perolat@google.com;karltuyls@google.com;munos@google.com;bowlingm@google.com,Reinforcement Learning and Planning/Multi-Agent RL*; Applications/Game Playing; Reinforcement Learning and Planning/Reinforcement Learning; Theory,Poster
1767,Step Size Matters in Deep Learning,"Training a neural network with the gradient descent algorithm gives rise to a discrete-time nonlinear dynamical system. Consequently, behaviors that are typically observed in these systems emerge during training, such as convergence to an orbit but not to a fixed point or dependence of convergence on the initialization. Step size of the algorithm plays a critical role in these behaviors: it determines the subset of the local optima that the algorithm can converge to, and it specifies the magnitude of the oscillations if the algorithm converges to an orbit. To elucidate the effects of the step size on training of neural networks, we study the gradient descent algorithm as a discrete-time dynamical system, and by analyzing the Lyapunov stability of different solutions, we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm. The results provide an explanation for several phenomena observed in practice, including the deterioration in the training error with increased depth, the hardness of estimating linear mappings with large singular values, and the distinct performance of deep residual networks.","Kamil Nar*,UC Berkeley;Shankar Sastry,UC Berkeley",nar@berkeley.edu;sastry@eecs.berkeley.edu,Deep Learning/Optimization for Deep Networks*; Optimization/Non-Convex Optimization; Theory/Control Theory,Spotlight
1768,Derivative Estimation in Random Design,"We propose a nonparametric derivative estimation method for randomdesign without
having to estimate the regression function. The method is based on a variance-reducing linear combination of symmetric difference quotients. First, we discuss
the special case of uniformrandomdesign and establish the estimator‚Äôs asymptotic
properties. Secondly, we generalize these results for any distribution of the dependent variable and compare the proposed estimator with popular estimators for
derivative estimation such as local polynomial regression and smoothing splines.","Yu Liu,Iowa State University;Kris De Brabanter*,ISU",yuliu@iastate.edu;kbrabant@iastate.edu,Algorithms/Regression*; Algorithms/Kernel Methods; Theory/Frequentist Statistics,Poster
1775,Zeroth-order (Non)-Convex Stochastic Optimization via Conditional Gradient and Gradient Updates,"In this paper, we propose and analyze zeroth-order stochastic approximation algorithms for nonconvex and convex optimization. Specifically, we propose generalizations of the conditional gradient algorithm achieving rates similar to the standard stochastic gradient algorithm using only zeroth-order information. Furthermore, under a structural sparsity assumption, we first illustrate an implicit regularization phenomenon where the standard stochastic gradient algorithm with zeroth-order information adapts to the sparsity of the problem at hand by just varying the step-size. Next, we propose a truncated stochastic gradient algorithm with zeroth-order information, whose rate of convergence depends only poly-logarithmically on the dimensionality.","Krishna Balasubramanian*,University of California, Davis;Saeed Ghadimi,Princeton University",kriznakumar@gmail.com;sghadimi@princeton.edu,Optimization*; Optimization/Convex Optimization; Optimization/Non-Convex Optimization; Theory/Regularizati,Poster
1776,Latent Gaussian Activity Propagation: Using Smoothness and Structure to Separate and Localize Sounds in Large Noisy Environments,"We present an approach for simultaneously separating and localizing
multiple sound sources using recorded microphone data. Inspired by topic
models, our approach is based on a probabilistic model of inter-microphone
phase differences, and poses separation and localization as a Bayesian
inference problem. We assume sound activity is locally smooth across time,
frequency, and location, and use the known position of the microphones to
obtain a consistent separation. We compare the performance of our method
against existing algorithms on simulated anechoic voice data and find that it
obtains high performance across a variety of input conditions.","Daniel Johnson*,Harvey Mudd College;Daniel Gorelik,Harvey Mudd College;Ross Mawhorter,Harvey Mudd College;Kyle Suver,Harvey Mudd College;Weiqing Gu,Harvey Mudd College;Steven Xing,Intel Corporation;Cody Gabriel,Intel Corporation;Peter Sankhagowit,Intel Corporation",ddjohnson@hmc.edu;dgorelik@hmc.edu;rmawhorter@g.hmc.edu;ksuver@hmc.edu;gu@hmc.edu;steven.xing@intel.com;cody.gabriel@intel.com;peter.sankhagowit@intel.com,Applications/Source Separation*; Applications/Audio and Speech Processing; Applications/Denoising; Applications/Signal Proce,Poster
1781,Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference Aggregation,"In this paper we present a hybrid active sampling strategy for pairwise preference aggregation, which aims at recovering the underlying rating of the test candidates from sparse and noisy pairwise labeling. Our method employs Bayesian optimization framework and Bradley-Terry model to construct the utility function, then to obtain the Expected Information Gain (EIG) of each pair. For computational efficiency, Gaussian-Hermite quadrature is used for estimation of EIG. In this work, a hybrid active sampling strategy is proposed, either using Global Maximum (GM) EIG sampling or Minimum Spanning Tree (MST) sampling in each trial, which is determined by the test budget. The proposed method has been validated on both simulated and real-world datasets, where it shows higher preference aggregation ability than the state-of-the-art methods.","Jing LI*,University of Nantes;Rafal Mantiuk,University of Cambridge;Junle Wang,Tencent Turing Lab;Suiyi Ling,universit√© de nantes;Patrick Le Callet,""Universite de Nantes, France""",jing.li.univ@gmail.com;rkm38@cam.ac.uk;wangjunle@gmail.com;suiyi.ling@univ-nantes.fr;patrick.lecallet@univ-nantes.fr,Algorithms/Active Learning*; Algorithms/Ranking and Preference Learning; Applications/Information Retrieval; Applications -,Poster
1783,Infinite-Horizon Gaussian Processes,"Gaussian processes provide a flexible framework for forecasting, removing noise, and interpreting long temporal datasets. State space modelling (Kalman filtering) enables these non-parametric models to be deployed on long datasets by reducing the complexity to linear in the number of data points. The complexity is still cubic in the state dimension m which is an impediment to practical application. In certain special cases (Gaussian likelihood, regular spacing) the GP posterior will reach a steady posterior state when the data are very long. We leverage this and formulate an inference scheme for GPs with general likelihoods, where inference is based on single-sweep EP (assumed density filtering). The infinite-horizon model tackles the cubic cost in the state dimensionality and reduces the cost in the state dimension m to O(m^2) per data point. The model is extended to online-learning of hyperparameters. We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100 Hz.","Arno Solin*,Aalto University;James Hensman,PROWLER.io;Richard Turner,",arno.solin@aalto.fi;james@prowler.io;ret26@cam.ac.uk,Probabilistic Methods/Gaussian Processes*; Probabilistic Methods,Poster
1790,Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization,"Stochastic optimization naturally arises in machine learning. Efficient algorithms with provable guarantees, however, are still largely missing, when the objective function is nonconvex and the data points are dependent. This paper studies this fundamental challenge through a streaming PCA problem for stationary time series data. Specifically, our goal is to estimate the principle component of time series data with respect to the covariance matrix of the stationary distribution. Computationally, we propose a variant of Oja's algorithm combined with downsampling to control the bias of the stochastic gradient caused by the data dependency. Theoretically, we quantify the uncertainty of our proposed stochastic algorithm based on diffusion approximations. This allows us to prove the asymptotic rate of convergence and further implies near optimal asymptotic sample complexity. Numerical experiments are provided to support our analysis.","Minshuo Chen,Georgia Tech;Lin Yang,Princeton University;Mengdi Wang,Princeton University;Tuo Zhao*,Georgia Tech",mchen393@gatech.edu;lin.yang@princeton.edu;mengdiw@princeton.edu;tuo.zhao@isye.gatech.edu,"Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA)*; Algorithms/Unsupervised Learning; Reinforcement Learning and Planning/Decision and Control; Th",Poster
1794,Sequence-to-Segment Networks for Segment Detection,"Detecting segments of interest from an input sequence is a challenging problem which  often requires not only good knowledge of individual target segments, but also contextual understanding of the entire input sequence and the relationships between the target segments.  To address this problem, we propose the Sequence-to-Segment Network (S$^2$N), a novel end-to-end sequential encoder-decoder architecture. S$^2$N first encodes the input into a sequence of hidden states that progressively capture both local and holistic information. It then employs a novel decoding architecture, called Segment Detection Unit (SDU), that integrates the decoder state and encoder hidden states to detect segments sequentially.  During training, we formulate the assignment of predicted segments to ground truth as bipartite matching and use the Earth Mover's Distance to calculate the localization errors. We experiment with S$^2$N on temporal action proposal generation and video summarization and show that S$^2$N achieves state-of-the-art performance on both tasks.","Zijun Wei*,Stony Brook University;Boyu Wang,Stony Brook University;Minh Hoai Nguyen,Stony Brook University;Jianming Zhang,Adobe Research;Zhe Lin,Adobe Research;Xiaohui Shen,ByteDance AI Lab; Radomir Mech,Adobe Systems Incorporated;Dimitris   Samaras,Stony Brook University",zijwei@cs.stonybrook.edu;boywang@cs.stonybrook.edu;minhhoai@cs.stonybrook.edu;jianmzha@adobe.com;zlin@adobe.com;shenxiaohui@gmail.com;rmech@adobe.com;samaras@cs.stonybrook.edu,Applications/Video Analysis*; Applications/Computer Vision; Deep Learning/Recurrent Networks,Poster
1803,Scaling the Poisson GLM to massive neural datasets through polynomial approximations,"Recent advances in recording technologies have allowed neuroscientists to record simultaneous spiking activity from hundreds to thousands of neurons in multiple brain regions. Such largescale recordings pose a major challenge to existing statistical methods for neural data analysis. Here we develop highly scalable approximate inference methods for Poisson generalized linear models (GLMs) that allow for efficient and regularized estimation of high-dimensional GLM parameters using a single pass over the data. Our approach relies on a recently proposed method for obtaining global polynomial approximate sufficient statistics \cite{huggins2017pass}, which we adapt to the Poisson GLM setting. First, we consider a quadratic approximation to the Poisson GLM log-likelihood and derive closed-form solutions for the approximate maximum likelihood and MAP estimates, posterior distribution, and marginal likelihood. We show that the approximation allows for efficient regularization via Gaussian evidence optimization for hyperparameters governing shrinkage, smoothness, and sparsity of GLM weights. Second, we consider an estimator based on a fourth order approximation to the log-likelihood, which improves accuracy of the estimator albeit at increased computational cost and a loss of closed-form expressions for approximate Bayesian inference. We validate the quadratic and fourth order estimators using simulations and medium-scale spike train recordings from primate retina. Finally, we use the highly scalable quadratic estimator to fit a fully-coupled Poisson GLM to spike train data recorded from 831 neurons across five regions of the mouse brain for a duration of 44 minutes, binned at 1 ms resolution, using a single pass over the data. Across all neurons, this model is fit to over $2$ billion spike count bins and has $831^2 \approx 691$K coupling filters revealing fine-timescale statistical dependencies between neurons within and across cortical and subcortical areas.","David Zoltowski*,Princeton University;Jonathan Pillow,Princeton University",dz5@princeton.edu;pillow@princeton.edu,Neuroscience and Cognitive Science/Neural Coding*; Algorithms/Regression; Neuroscience and Cognitive Science/Neuroscience,Poster
1804,Multiplicative Weights Updates with Constant Step-Size in Graphical Constant-Sum Games,"Since Multiplicative Weights (MW) updates are the discrete analogue of the continuous Replicator Dynamics (RD), some researchers had expected their qualitative behaviours would be similar. We show that this is false in the context of graphical constant-sum games, which include two-person zero-sum games as special cases. In such games which have a fully-mixed Nash Equilibrium (NE), RD have the permanence and Poincare recurrence properties, but we show that MW updates with constant step-size eps do not. We show that the regret of RD is O(1/T); for MW updates, we prove a regret lower bound of Omega( 1 / (eps T) ).

For showing the regret results, we adopt a dynamical-system perspective instead of the now popular optimization perspective. Interestingly, the regret perspective can be useful for better understanding of the behaviours of MW updates. In a two-person zero-sum game, if it has a unique NE which is fully mixed, then we show, via regret, that for any sufficiently small eps, there exists h>0 and at least two probability densities which get above h infinitely often, but also get arbitrarily close to zero infinitely often.","Yun Kuen Cheung*,Max Planck Institute, Saarland Informatics Campus",marcodick@gmail.com,Theory/Game Theory and Computational Economics*; Algorithms/Dynamical Systems,Poster
1807,Why Is My Classifier Discriminatory?,"Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.","Irene Chen*,MIT;Fredrik Johansson,MIT;David Sontag,MIT",iychen@mit.edu;fredrikj@mit.edu;dsontag@csail.mit.edu,"Applications/Fairness, Accountability, and Transparency",Spotlight
1808,Multi-Layered Gradient Boosting Decision Trees,"Multi-layered representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments and visualizations confirmed the effectiveness of the model in terms of performance and representation learning ability.","Ji Feng*,Nanjing University;Yang Yu,Nanjing U;Zhi-Hua Zhou,Nanjing university",fengj@lamda.nju.edu.cn;yuy@lamda.nju.edu.cn;zhouzh@lamda.nju.edu.cn,Algorithms/Boosting and Ensemble Methods,Poster
1809,Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning,"Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.","Tom Zahavy*,Technion, Google;Matan Harush,Technion;Nadav Merlis,Technion;Daniel Mankowitz,Technion Israel Institute of Technology;Shie Mannor,",tomzahavy@gmail.com;matanharush@gmail.com;merlis.nadav@gmail.com;daniel.mankowitz@gmail.com;shiemannor@gmail.com,Reinforcement Learning and Planning*; Applications/Game Playing; Applications/Natural Language Processing,Poster
1817,Communication Efficient Parallel Algorithms for Optimization on Manifolds,"The last decade has witnessed an explosion in the development of models, theory and computational algorithms for ``big data'' analysis. In particular, distributed inference has served as a natural and dominating paradigm for statistical inference. However, the existing literature on parallel inference almost exclusively focuses on Euclidean data and parameters. While this assumption is valid for many applications, it is increasingly more common to encounter problems where the data or the parameters lie on a non-Euclidean space, like a manifold for example. Our work aims to fill a critical gap in the literature by generalizing parallel inference algorithms to optimization on manifolds. We show that our proposed algorithm is both communication efficient and carries theoretical convergence guarantees. In addition, we demonstrate the performance of our algorithm to the estimation of Fr\'echet means on simulated spherical data and the low-rank matrix completion problem over Grassmann manifolds applied to the Netflix prize data set.","Bayan Saparbayeva*,University of Notre Dame;Michael Zhang,Princeton University;Lizhen Lin,University of Notre Dame",bsaparba@nd.edu;mz8@cs.princeton.edu;lizhen.lin@nd.edu,Probabilistic Methods/Distributed Inference*; Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning; Applications/Recommender,Poster
1819,Neural Code Comprehension: A Learnable Representation of Code Semantics,"With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that with a single RNN architecture and pre-trained fixed embeddings, inst2vec outperforms specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.","Tal Ben-Nun*,ETH Zurich;Alice Shoshana Jakobovits,ETH Zurich;Torsten Hoefler,ETH Zurich",talbn@inf.ethz.ch;alicej@student.ethz.ch;htor@inf.ethz.ch,Applications*; Deep Learning/Embedding Approaches,Poster
1825,Tight Bounds for Collaborative PAC Learning via Multiplicative Weights,"We study the collaborative PAC learning problem recently proposed in Blum  et al.~\cite{BHPQ17}, in which we have $k$ players and they want to learn a target function collaboratively, such that the learned function approximates the target function well on all players' distributions simultaneously. The quality of the collaborative learning algorithm is measured by the ratio between the sample complexity of the algorithm and that of the learning algorithm for a single distribution (called the overhead).  We obtain a collaborative learning algorithm with overhead $O(\ln k)$, improving the one with overhead $O(\ln^2 k)$ in \cite{BHPQ17}.  We also show that an $\Omega(\ln k)$ overhead is inevitable when $k$ is polynomial bounded by the VC dimension of the hypothesis class.  Finally, our experimental study has demonstrated the superiority of our algorithm compared with the one in Blum  et al.~\cite{BHPQ17} on real-world datasets.","Jiecao Chen,Indiana University Bloomington;Qin Zhang*,Indiana University Bloomington;Yuan Zhou,Indiana University Bloomington",chenjiecao@gmail.com;qzhangcs@indiana.edu;yzhoucs@iu.edu,Theory/Learning Theory,Poster
1826,BinGAN: Learning Compact Binary Descriptors with a Regularized GAN,"In this paper, we propose a novel regularization method for Generative Adversarial Networks that allows the model to learn discriminative yet compact binary representations of image patches (image descriptors). We exploit the dimensionality reduction that takes place in the intermediate layers of the discriminator network and train the binarized penultimate layer's low-dimensional representation to mimic the distribution of the higher-dimensional preceding layers. To achieve this, we introduce two loss terms that aim at: (i) reducing the correlation between the dimensions of the binarized penultimate layer's low-dimensional representation (i.e. maximizing joint entropy)  and (ii) propagating the relations between the dimensions in the high-dimensional space to the low-dimensional space. We evaluate the resulting binary image descriptors on two challenging applications, image matching and retrieval, where they achieve state-of-the-art results.","Maciej Zieba*,Wroclaw University of Technology;Piotr Semberecki,Faculty of Electronics, Department of Computer Engineering, Wroc≈Çaw University of Technology;Tarek El-Gaaly,Voyage;Tomasz Trzcinski,Warsaw University of Technology",maciej.zieba@pwr.edu.pl;piotr.semberecki@pwr.edu.pl;tgaaly@gmail.com;t.trzcinski@ii.pw.edu.pl,Applications/Computer Vision*; Deep Learning/Generative Models,Poster
1827,Modern Neural Networks Generalize on Small Data Sets,"In this paper,  we  use a linear program to empirically decompose fitted neural networks into ensembles of low-bias sub-networks. We show that these sub-networks are relatively uncorrelated which leads to an  internal regularization process, very much like a random forest, which can explain why a neural network is surprisingly resistant to overfitting.  We then demonstrate this in practice by applying large neural networks, with hundreds of parameters per training observation, to a  collection of 116 real-world data sets from the UCI Machine Learning Repository.   This collection of data sets contains a much smaller number of training examples than the types of image classification tasks generally studied in the deep learning literature, as well as non-trivial label noise. We show  that even in this setting deep neural nets are capable of achieving superior classification accuracy without overfitting. ","Matthew Olson*,University of Pennsylvania;Abraham Wyner,;Richard Berk,",maolson@wharton.upenn.edu;ajw@wharton.upenn.edu;berkr@sas.upenn.edu,Algorithms/Classification*; Deep Learning,Poster
1830,Escaping Saddle Points in Constrained Optimization,"In this paper, we focus on escaping from saddle points in smooth nonconvex optimization problems subject to a convex set $\mathcal{C}$. We propose a generic framework that yields convergence to a second-order stationary point of the problem, if the convex set $\mathcal{C}$ is simple for a quadratic objective function. To be more precise, our results hold if one can find a $\rho$-approximate solution of a quadratic program subject to $\mathcal{C}$ in polynomial time, where $\rho<1$ is a positive constant that depends on the structure of the set $\mathcal{C}$. Under this condition, we show that the sequence of iterates generated by the proposed framework reaches an $(\epsilon,\gamma)$-second order stationary point (SOSP) in at most $\mathcal{O}(\max\{\epsilon^{-2},\rho^{-3}\gamma^{-3}\})$ iterations. We further characterize the overall arithmetic operations to reach an SOSP when the convex set $\mathcal{C}$ can be written as a set of quadratic constraints. Finally, we extend our results to the stochastic setting and characterize the number of stochastic gradient and Hessian evaluations to reach an $(\epsilon,\gamma)$-SOSP.","Aryan Mokhtari*,MIT;Asuman Ozdaglar,MIT;Ali Jadbabaie,",aryanm@mit.edu;asuman@mit.edu;jadbabai@mit.edu,Optimization/Non-Convex Optimization*; Theory/Computational Complexity,Spotlight
1837,Adversarial Attacks on Stochastic Bandits,"We study adversarial attacks that manipulate the reward signals to control the actions chosen by a stochastic multi-armed bandit algorithm.  We propose the first attack against two popular bandit algorithms: $\epsilon$-greedy and UCB, \emph{without} knowledge of the mean rewards.  The attacker is able to spend only logarithmic effort, multiplied by a problem-specific parameter that becomes smaller as the bandit problem gets easier to attack.  The result means the attacker can easily hijack the behavior of the bandit algorithm to promote or obstruct certain actions, say, a particular medical treatment.  As bandits are seeing increasingly wide use in practice, our study exposes a significant security threat.","Kwang-Sung Jun*,UW-Madison;Lihong Li,Google Inc.;Yuzhe Ma,University of Wisconsin-Madison;Xiaojin Zhu,University of Wisconsin-Madison",kjun@discovery.wisc.edu;lihongli.cs@gmail.com;ma234@wisc.edu;jerryzhu@cs.wisc.edu,"Applications/Privacy, Anonymity, and Security*; Algorithms/Bandit Algorithms",Poster
1842,Optimal Subsampling with Influence Functions,"Subsampling is a common and often effective method to deal with the computational challenges of large datasets. However, for most statistical models, there is no well-motivated approach for drawing a non-uniform subsample. We show that the concept of an asymptotically linear estimator and the associated influence function leads to asymptotically optimal sampling probabilities for a wide class of popular models. This is the only tight optimality result for subsampling we are aware of as other methods only provide probabilistic error bounds or optimal rates. 
Furthermore, for linear regression models, which have well-studied procedures for non-uniform subsampling, we empirically show our optimal influence function based method outperforms previous approaches even when using approximations to the optimal probabilities. ","Daniel Ting*,Tableau Software;Eric Brochu,Tableau Software",daniel.ting@gmail.com;ebrochu@tableau.com,Algorithms/Stochastic Methods*; Deep Learning/Efficient Training Methods; Theory/Frequentist Statistics,Poster
1847,A Bandit Approach to Sequential Experimental Design with False Discovery Control,"We propose a new adaptive sampling approach to multiple testing which aims to maximize statistical power while ensuring anytime false discovery control. We consider $n$ distributions whose means are partitioned by whether they are below or equal to a baseline (nulls), versus above the baseline (true positives). In addition, each distribution can be sequentially and repeatedly sampled. Using techniques from multi-armed bandits, we provide an algorithm that takes as few samples as possible to exceed a target true positive proportion (i.e. proportion of true positives discovered) while giving anytime control of the false discovery proportion (nulls predicted as true positives). Our sample complexity results match known information theoretic lower bounds and through simulations we show a substantial performance improvement over uniform sampling and an adaptive elimination style algorithm. Given the simplicity of the approach, and its sample efficiency, the method has promise for wide adoption in the biological sciences, clinical testing for drug discovery, and maximization of click through in A/B/n testing problems.","Kevin Jamieson*,U Washington;Lalit Jain,University of Washington",jamieson@cs.washington.edu;lalitj@cs.washington.edu,Algorithms/Bandit Algorithms*; Algorithms/Active Learning; Theory/Frequentist Statistics,Poster
1851,Equality of Opportunity in Classification: A Causal Approach,"Equalized Odds (EO) provides a sensible framework to reason about discrimination against a specified protected group (e.g., gender, race) in supervised learning. However, statistical tests based on the EO, as  acknowledged in (Hartz et al. 2016), are oblivious to the true data-generating mechanisms, and thus unable to capture fundamental notions of unfairness such as direct discrimination. This paper introduces a set of novel counterfactual measures that allows one to explain the disparities measured by EO over the underlying mechanisms in an arbitrary causal model. We operationalize these estimands through a practical procedure to obtain an efficient classifier compatible with basic human intuition about fairness.","Junzhe Zhang*,Purdue University;Elias Bareinboim,Purdue University",zhang745@purdue.edu;eb@purdue.edu,"Probabilistic Methods/Causal Inference*; Applications/Fairness, Accountability, and Transparency",Poster
1861,Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization,"Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) have been widely used in distributed machine learning, e.g., training large collaborative filtering systems and deep neural networks. Due to current technical limit, however, establishing convergence properties of Async-MSGD for these highly complicated nonoconvex problems is generally infeasible. Therefore, we propose to analyze the algorithm through a simpler but nontrivial nonconvex problems --- streaming PCA. This allows us to make progress toward understanding Aync-MSGD and gaining new insights for more general problems. Specifically, by exploiting the diffusion approximation of stochastic optimization, we establish the asymptotic rate of convergence of Async-MSGD for streaming PCA. Our results indicate a fundamental tradeoff between asynchrony and momentum: To ensure convergence and acceleration through asynchrony, we have to reduce the momentum (compared with Sync-MSGD). To the best of our knowledge, this is the first theoretical attempt on understanding Async-MSGD for distributed nonconvex stochastic optimization. Numerical experiments on both streaming PCA and training deep neural networks are provided to support our findings for Async-MSGD.","Tianyi Liu,Georgia Institute of Technolodgy;Shiyang Li,Harbin Institute of Technology;Jianping Shi,Sensetime Group Limited;Enlu Zhou,;Tuo Zhao*,Georgia Tech",tianyiliu@gatech.edu;lsydevin@gmail.com;shijianping@sensetime.com;enlu.zhou@isye.gatech.edu;tuo.zhao@isye.gatech.edu,Theory/Large Deviations and Asymptotic Analysis*; Algorithms/Stochastic Methods; Optimization/Non-Convex Optimization,Poster
1862,Unsupervised Attention-guided Image-to-Image Translation,"Current unsupervised image-to-image translation techniques struggle to focus their attention on individual objects without altering the background or the way multiple objects interact within a scene. Motivated by the important role of attention in human perception, we tackle this limitation by introducing unsupervised attention mechanisms which are jointly adversarially trained with the generators and discriminators. We empirically demonstrate that our approach is able to attend to relevant regions in the image without requiring any additional supervision, and that by doing so it achieves more realistic mappings compared to recent approaches.","Youssef Alami Mejjati*,University of Bath;Christian Richardt,University of Bath;James Tompkin,Brown University;Darren Cosker,University of Bath;Kwang In Kim,University of Bath",yam28@bath.ac.uk;christian@richardt.name;james_tompkin@brown.edu;D.P.Cosker@bath.ac.uk;K.Kim@bath.ac.uk,Deep Learning/Adversarial Networks*; Applications/Computer Vision; Deep Learning/Generative Models,Poster
1872,Inferring Networks From Random Walk-Based Node Similarities,"Digital presence in the world of online social media entails significant privacy risks \cite{korolova2008link,zheleva2012privacy}. In this work we consider a privacy threat to a social network in which an attacker has access to a subset of random walk-based node similarities, such as effective resistances (i.e., commute times) or personalized PageRank scores. Using these similarities, the attacker seeks to infer as much information as possible about the network, including unknown pairwise node similarities and edges.

For the effective resistance metric, we show that with just a small subset of measurements, one  can learn a large fraction of edges in a social network. We also show that it is possible to  learn a graph which accurately matches the underlying network on all other effective resistances. This second observation is interesting from a data mining perspective, since it can be expensive to compute all effective resistances or other random walk-based similarities. As an alternative, our graphs learned from just a subset of effective resistances can be used as surrogates in a range of applications that use effective resistances to probe graph structure, including for graph clustering, node centrality evaluation, and anomaly detection. 

We obtain our results by formalizing the graph learning objective mathematically, using two optimization problems. One formulation is convex and can be solved provably in polynomial time. The other is not, but we solve it efficiently with projected gradient and coordinate descent. We demonstrate the effectiveness of these methods on a number of social networks obtained from Facebook. We also discuss how our methods can be generalized to other random walk-based similarities, such as personalized PageRank scores.","Jeremy Hoskins,Yale University;Cameron Musco*,MIT;Christopher Musco,Mass Institute of Technology;Charalampos Tsourakakis,Boston University",jeremy.hoskins@yale.edu;cnmusco@mit.edu;cpmusco@mit.edu;ctsourak@bu.edu,Applications/Network Analysis*; Algorithms/Similarity and Distance Learning; Algorithms/Spectral Methods; Applications/Priv,Poster
1873,NEON 2: Finding Local Minima via First-Order Oracles,"(this is a theory paper)

We propose a reduction for non-convex optimization that can (1) turn an stationary-point finding algorithm into an local-minimum finding one, and (2) replace the Hessian-vector product computations with only gradient computations. It works both in the stochastic and the deterministic settings, without hurting the algorithm's performance. As applications, our reduction turns Natasha2 into a first-order method without hurting its theoretical performance. It also converts SGD, GD, and SCSG into algorithms finding approximate local minima, outperforming some best known results.","Zeyuan Allen-Zhu*,Microsoft Research AI;Yuanzhi Li,Princeton",zeyuan@csail.mit.edu;yuanzhil@cs.princeton.edu,Optimization/Non-Convex Optimization,Poster
1876,Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization,"As application demands for zeroth-order (gradient-free) optimization accelerate, the need for variance reduced and faster converging approaches is also intensifying. This paper addresses these challenges by presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order (ZO) optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models. Our theoretical analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the unbiased assumption on gradient estimates no longer holds. We prove that compared to its first-order counterpart, ZO-SVRG with a two-point random gradient estimator suffers an additional error of order $O(1/b)$, where $b$ the mini-batch size. To mitigate this error, we propose two accelerated versions of ZO-SVRG utilizing variance reduced gradient estimators, which achieve the best rate  known for ZO stochastic optimization (in terms of iterations). Our extensive experimental results show that our approaches outperform other state-of-the-art ZO algorithms, and strike a balance  between the convergence rate and the function query complexity.","Sijia Liu*,IBM;Bhavya Kailkhura,LLNL;Pin-Yu Chen,IBM Research;Pai-Shun Ting,UMICH;Shiyu Chang,IBM Research;Lisa Amini,IBM Research AI",sijia.liu@ibm.com;kailkhura1@llnl.gov;pin-yu.chen@ibm.com;paishun@umich.edu;shiyu.chang@ibm.com;lisa.amini@us.ibm.com,Optimization/Non-Convex Optimization*; Optimization,Poster
1877,Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting,"We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.","Hippolyt Ritter*,University College London;Aleksandar Botev,University College London;David Barber,UCL",j.ritter@cs.ucl.ac.uk;botevmg@gmail.com;david.barber@ucl.ac.uk,Algorithms/Online Learning*; Deep Learning/Supervised Deep Networks; Probabilistic Methods,Poster
1878,DeepProbLog:  Neural Probabilistic Logic Programming,"We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.","Robin Manhaeve*,KU Leuven;Sebastijan Dumancic,KU LEUVEN;Angelika Kimmig,Cardiff University;Thomas Demeester,Ghent University;Luc de Raedt,KU Leuven university",robin.manhaeve@cs.kuleuven.be;sebastijan.dumancic@cs.kuleuven.be;kimmiga@cardiff.ac.uk;thomas.demeester@ugent.be;luc.deraedt@cs.kuleuven.be,Algorithms/Relational Learning*; Deep Learning; Deep Learning/Program Induction; Probabilistic Methods,Spotlight
1881,Convergence of Cubic Regularization for Nonconvex Optimization under KL Property,"Cubic-regularized Newton's method (CR) is a popular algorithm that guarantees to produce a second-order stationary solution for solving nonconvex optimization problems. However, existing understandings of convergence rate of CR are conditioned on special types of geometrical properties of the objective function. In this paper, we explore the asymptotic convergence rate of CR by exploiting the ubiquitous Kurdyka-Lojasiewicz (KL) property of the nonconvex objective functions. In specific, we characterize the asymptotic convergence rate of various types of optimality measures for CR including function value gap, variable distance gap, gradient norm and least eigenvalue of the Hessian matrix. Our results fully characterize the diverse convergence behaviors of these optimality measures in the full parameter regime of the KL property. Moreover, we show that the obtained asymptotic convergence rates of CR are order-wise faster than those of first-order gradient descent algorithms under the KL property.","Yi Zhou*,The Ohio State University;Zhe Wang,Ohio State University;Yingbin Liang,The Ohio State University",zhou.1172@osu.edu;wang.10982@osu.edu;liang.889@osu.edu,Optimization*; Optimization/Non-Convex Optimization,Spotlight
1882,Direct Estimation of Differences in Causal Graphs,"We consider the problem of estimating the differences between two causal directed acyclic graph (DAG) models given i.i.d. samples from each model. This is of interest for example in genomics, where changes in the structure or edge weights of the underlying causal graphs reflect alterations in the gene regulatory networks. We here provide the first provably consistent method for directly estimating the differences in a pair of causal DAGs without separately learning two possibly large and dense DAG models and computing their difference. Our two-step algorithm first uses invariance tests between regression coefficients of the two data sets to estimate the skeleton of the difference graph and then orients some of the edges using invariance tests between regression residual variances. We demonstrate the properties of our method through a simulation study and apply it to the analysis of gene expression data from ovarian cancer and during T-cell activation. ","Yuhao Wang*,MIT;Chandler Squires,Massachusetts Institute of Technology;Anastasiya Belyaeva,Massachusetts Institute of Technology;Caroline Uhler,MIT",yuhaow@mit.edu;csquires@mit.edu;belyaeva@mit.edu;cuhler@mit.edu,Probabilistic Methods/Causal Inference*; Algorithms/Model Selection and Structure Learning; Applications/Computational Biology and Bioi,Poster
1883,Sublinear Time Low-Rank Approximation of Distance Matrices,"Let $\PP=\{ p_1, p_2, \ldots p_n \}$ and $\QQ = \{ q_1, q_2 \ldots q_m \}$ be two point sets in an arbitrary metric space. Let $\AA$ represent the $m\times n$ pairwise distance matrix with $\AA_{i,j} = d(p_i, q_j)$. Such distance matrices are commonly computed in software packages and have applications to learning image manifolds, handwriting recognition, and multi-dimensional unfolding, among other things. In an attempt to reduce their description size, we study low rank approximation of such matrices. Our main result is to show that for any underlying distance metric $d$, it is possible to achieve an additive error low rank approximation in sublinear time. We note that it is provably impossible to achieve such a guarantee in sublinear time for arbitrary matrices $\AA$, and our proof exploits special properties of distance matrices. We develop a recursive algorithm based on additive projection-cost preserving sampling. We then show that in general, relative error approximation in sublinear time is impossible for distance matrices, even if one allows for bicriteria solutions. Additionally, we show that if $\PP = \QQ$ and $d$ is the squared Euclidean distance, which is not a metric but rather the square of a metric, then a relative error bicriteria solution can be found in sublinear time. Finally, we empirically compare our algorithm with the SVD and input sparsity time algorithms. Our algorithm is several hundred times faster than the SVD, and about $8$-$20$ times faster than input sparsity methods on real-world and and synthetic datasets of size $10^8$. Accuracy-wise, our algorithm is only slightly worse than that of the SVD (optimal) and input-sparsity time algorithms.","Ainesh Bakshi*,Carnegie Mellon University;David Woodruff,Carnegie Mellon University",aineshbakshi@gmail.com;dwoodruf@cs.cmu.edu,Algorithms*; Theory/Computational Complexity,Spotlight
1889,Variational PDEs for Acceleration on Manifolds and Application to Diffeomorphisms,"We consider the optimization of cost functionals on manifolds and derive a variational approach to accelerated methods on manifolds. We demonstrate the methodology on the infinite-dimensional manifold of diffeomorphisms, motivated by registration problems in computer vision. We build on the variational approach to accelerated optimization by Wibisono, Wilson and Jordan, which applies in finite dimensions, and generalize that approach to infinite dimensional manifolds. We derive the continuum evolution equations, which are partial differential equations (PDE), and relate them to simple mechanical principles. Our approach can also be viewed as a generalization of the $L^2$ optimal mass transport problem. Our approach evolves an infinite number of particles endowed with mass, represented as a mass density. The density evolves with the optimization variable, and endows the particles with dynamics. This is different than current accelerated methods where only a single particle moves and hence the dynamics does not depend on the mass. We derive the theory, compute the PDEs for acceleration, and illustrate the behavior of this new accelerated optimization scheme.","Ganesh Sundaramoorthi*,Kaust;Anthony Yezzi,Georgia Tech",ganesh.sun@gmail.com;ayezzi@ece.gatech.edu,Optimization*; Applications/Computer Vision; Optimization/Non-Convex Optimization,Poster
1891,Bayesian Inference of Temporal Task Specifications from Demonstrations,"When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring true specifications with over 90% similarity between the inferred specification and the ground truth, both within a synthetic domain and a real-world table setting task.","Ankit Shah*,Massachusetts Institute of Technology;Pritish Kamath,MIT;Julie Shah,MIT;Shen Li,MIT",ankitshah91@gmail.com;pritish@mit.edu;julie_a_shah@csail.mit.edu;shenli@mit.edu,Applications*; Probabilistic Methods,Poster
1895,Data center cooling using model-predictive control,"Despite impressive recent advances in reinforcement learning (RL), its deployment in real-world physical systems is often complicated by unexpected events, limited data, and the potential for expensive failures. In this paper, we describe an application of RL ‚Äúin the wild‚Äù to the task of regulating temperatures and airflow inside a large-scale data center (DC). Adopting a data-driven, model-based approach, we demonstrate that an RL agent with little prior knowledge is able to effectively and safely regulate conditions on a server floor after just a few hours of exploration, while improving operational efficiency relative to existing PID controllers.","Nevena Lazic*,Google;Craig Boutilier,Google Research;Tyler Lu,Google;Eehern Wong,Google;Binz Roy,Google;Moonkyung Ryu,Google;Greg Imwalle,Google",nevena@google.com;cboutilier@google.com;tylerlu@google.com;ejwong@google.com;binzroy@google.com;mkryu@google.com;gregi@google.com,Applications*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
1896,Acceleration through Optimistic No-Regret Dynamics,"We consider the problem of minimizing a smooth convex function by reducing the optimization to computing the Nash equilibrium of a particular zero-sum convex-concave game. Zero-sum games can be solved using no-regret learning dynamics, and the standard approach leads to a rate of $O(1/T)$. But we are able to show that the game can be solved at a rate of $O(1/T^2)$, extending recent works of \cite{RS13,SALS15} by using \textit{optimistic learning} to speed up equilibrium computation. The optimization algorithm that we can extract from this equilibrium reduction coincides \textit{exactly} with the well-known \NA \cite{N83a} method, and indeed the same story allows us to recover several variants of the Nesterov's algorithm via small tweaks. This methodology unifies a number of different iterative optimization methods: we show that the \HB algorithm is precisely the non-optimistic variant of \NA, and recent prior work already established a similar perspective on \FW \cite{AW17,ALLW18}.","Jun-Kun Wang*,Georgia Tech;Jacob Abernethy,Georgia Institute of Technolog",jimwang@gatech.edu;prof@gatech.edu,Algorithms/Online Learning*; Optimization,Spotlight
1901,Lipschitz regularity of deep neural networks: analysis and efficient estimation,"Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications.  In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures.  First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-of-art methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function.  We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks.  Our experiments show that SeqLip can significantly improve on the existing upper bounds.  Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.  These results also hint at the difficulty to estimate the Lipschitz constant of deep networks.","Aladin Virmaux*,Huawei;Kevin Scaman,Huawei",aladin.virmaux@gmail.com;kevin.scaman@gmail.com,Deep Learning*; Algorithms; Deep Learning/CNN Architectures; Theory,Poster
1906,Minimax Estimation of Neural Net Distance,"An important class of distance metrics proposed for training generative adversarial networks (GANs) is the integral probability metric (IPM), in which the neural net distance captures the practical GAN training via two neural networks. This paper investigates the minimax estimation problem of the neural net distance based on samples drawn from the distributions. We develop the first known minimax lower bound on the estimation error of the neural net distance, and an upper bound tighter than an existing bound on the estimator error for the empirical neural net distance. Our lower and upper bounds match not only in the order of the sample size but also in terms of the norm of the parameter matrices of neural networks, which justifies the empirical neural net distance as a good approximation of the true neural net distance for training GANs in practice. ","Kaiyi Ji*,The Ohio State University;Yingbin Liang,The Ohio State University",ji.367@osu.edu;liang.889@osu.edu,Theory/Learning Theory*; Theory/Information Theory,Poster
1909,Leveraging the Exact Likelihood of Deep Latent Variable Models,"Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however, the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First, we investigate maximum likelihood estimation for DLVMs: in particular, we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates, and draw useful connections with nonparametric mixture models. Finally, we describe an algorithm for missing data imputation using the exact conditional likelihood of a deep latent variable model. On several data sets, our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs.","Pierre-Alexandre Mattei*,IT University of Copenhagen;Jes Frellsen,IT University of Copenhagen",pima@itu.dk;jefr@itu.dk,Deep Learning/Generative Models*; Algorithms/Missing Data; Probabilistic Methods/Latent Variable Models; Probabilistic Methods -,Poster
1910,Bipartite Stochastic Block Models with Tiny Clusters,"We study the problem of finding planted clusters in bipartite graphs. We present a simple two-step algorithm which provably finds even tiny clusters of size O(n^Œµ), where n is the number of vertices in the graph and Œµ > 0. Previous algorithms were only able to identify clusters of size Œ©( sqrt(n) ). We evaluated the algorithm on synthetic and on real-world data; the experiments show that the algorithm can find extremely small clusters even in presence of high destructive noise.","Stefan Neumann*,University of Vienna",stefan.neumann@univie.ac.at,Algorithms/Clustering*; Algorithms/Spectral Methods; Theory/Computational Complexity,Poster
1911,Learning sparse neural networks via sensitivity-driven regularization,"The ever-increasing number of parameters in deep neural networks poses challenges for memory-limited applications. Regularize-and-prune methods aim at meeting these challenges by sparsifying the network weights.
In this context we quantify the output sensitivity to the parameters (i.e. their relevance to the network output) and introduce a regularization term that gradually lowers the absolute value of parameters with low sensitivity. Thus, a very large fraction of the parameters approach zero and are eventually set to zero by simple thresholding. Our method surpasses recent techniques both in terms of sparsity and error rates. In some cases, the method reaches twice the sparsity obtained by other techniques at equal error rates.","Enzo Tartaglione*,Politecnico di Torino;Skjalg  Leps√∏y,Telecom Italia;Attilio Fiandrotti,POLITO;Gianluca Francini,Telecom Italia",enzo.tartaglione@polito.it;skjalg.lepsoy@telecomitalia.it;attilio.fiandrotti@polito.it;gianluca.francini@telecomitalia.it,Theory/Regularization*; Algorithms/Sparsity and Compressed Sensing; Deep Learning/Optimization for Deep Networks,Poster
1916,Faster Online Learning of Optimal Threshold for  Consistent F-measure Optimization,"In this paper, we consider online F-measure optimization (OFO). Unlike traditional performance metrics (e.g., classification error rate), F-measure is non-decomposable over training examples and is a non-convex function of model parameters, making it much more difficult to be optimized in an online fashion. Most existing results of OFO usually suffer from high memory/computational costs and/or lack  statistical consistency  guarantee for optimizing F-measure at the population level. To advance OFO, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing  a stochastic strongly convex function with unknown strong convexity parameter. A key component of the proposed method is  a novel stochastic algorithm with low memory and computational costs, which can enjoy a  convergence rate of $\widetilde O(1/\sqrt{n})$ for learning the optimal threshold under a mild condition on the convergence of the posterior probability,  where $n$ is the number of processed examples. It is provably  faster than its predecessor based on a heuristic for updating the threshold.   The experiments verify  the efficiency of the proposed algorithm in comparison with state-of-the-art OFO algorithms.","Xiaoxuan Zhang,University of Iowa;Mingrui Liu,The University of Iowa;Xun Zhou,University of Iowa;Tianbao Yang*,University of Iowa",xiaoxuan-zhang@uiowa.edu;mingrui-liu@uiowa.edu;xun-zhou@uiowa.edu;tianbao-yang@uiowa.edu,Algorithms/Classification*; Algorithms/Online Learning,Poster
1920,Direct Runge-Kutta Discretization Achieves Acceleration,"  We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation (ODE) related to the continuous limit of Nesterov's accelerated gradient method. When the function is smooth enough, we show that acceleration can be achieved by a stable discretization of this ODE using standard Runge-Kutta integrators. Specifically, we prove that under Lipschitz-gradient, convexity and order-$(s+2)$ differentiability assumptions, the sequence of iterates generated by discretizing the proposed second-order ODE converges to the optimal solution at a rate of $\mathcal{O}({N^{-2\frac{s}{s+1}}})$, where $s$ is the order of the Runge-Kutta numerical integrator. Furthermore, we introduce a new local flatness condition on the objective, under which rates even faster than $\mathcal{O}(N^{-2})$ can be achieved with low-order integrators and only gradient information. Notably, this flatness condition is satisfied by several standard loss functions used in machine learning. We provide numerical experiments that verify the theoretical rates predicted by our results.","Jingzhao Zhang*,MIT;Aryan Mokhtari,MIT;Suvrit Sra,Massachusetts Institute of Technology, USA;Ali Jadbabaie,",jzhzhang@mit.edu;aryanm@mit.edu;suvrit@mit.edu;jadbabai@mit.edu,Optimization/Convex Optimization*; Algorithms/Dynamical Systems,Spotlight
1934,Adversarial Examples that Fool both Computer Vision and Time-Limited Humans,"Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.","Gamaleldin Elsayed*,Google Brain;Shreya Shankar,Stanford;Brian Cheung,UC Berkeley;Nicolas Papernot,Google Brain;Alexey Kurakin,Google Brain;Ian Goodfellow,Google Brain;Jascha Sohl-Dickstein,Google Brain",gamaleldin.elsayed@gmail.com;shreya@cs.stanford.edu;bcheung@berkeley.edu;ngp5056@cse.psu.edu;kurakin@google.com;goodfellow@google.com;jaschasd@google.com,Deep Learning*; Applications/Computational Biology and Bioinformatics; Applications/Computer Vision; Deep Lear,Poster
1937,Stochastic Nested Variance Reduced Gradient Descent for Nonconvex Optimization,"We study finite-sum nonconvex optimization problems, where the objective function is an average of $n$ nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each epoch, our algorithm uses $K+1$ nested reference points to build an semi-stochastic gradient to further reduce its variance in each epoch. For smooth functions, the proposed algorithm converges to an approximate first order stationary point (i.e., $\|\nabla F(\xb)\|_2\leq \epsilon$) within $\tO(n\land \epsilon^{-2}+\epsilon^{-3}\land n^{1/2}\epsilon^{-2})$\footnote{$\tO(\cdot)$ hides the logarithmic factors} number of stochastic gradient evaluations, where $n$ is the number of component functions, and $\epsilon$ is the optimization error. This improves the best known gradient complexity of SVRG $O(n+n^{2/3}\epsilon^{-2})$ and the best gradient complexity of SCSG $O(\epsilon^{-5/3}\land n^{2/3}\epsilon^{-2})$. For gradient dominated functions, our algorithm achieves $\tO(n\land \tau\epsilon^{-1}+\tau\cdot (n^{1/2}\land (\tau\epsilon^{-1})^{1/2})$ gradient complexity, which again beats the existing best gradient complexity $\tO(n\land \tau\epsilon^{-1}+\tau\cdot (n^{1/2}\land (\tau\epsilon^{-1})^{2/3})$ achieved by SCSG. Thorough experimental results on different nonconvex optimization problems back up our theory.","Dongruo Zhou,UCLA;Pan Xu,UCLA;Quanquan Gu*,UCLA",drzhou@cs.ucla.edu;px3ds@virginia.edu;qg5w@virginia.edu,Optimization/Non-Convex Optimization,Spotlight
1941,Faster Neural Networks Straight from JPEG,"  The simple, elegant approach of training convolutional neural
  networks (CNNs) directly from RGB pixels has enjoyed overwhelming
  empirical success. But can more performance be squeezed out of
  networks by using different input representations?  In this paper we
  propose and explore a simple idea: train CNNs directly on the
  blockwise discrete cosine transform (DCT) coefficients computed and
  available in the middle of the JPEG codec. Intuitively, when
  processing JPEG images using CNNs, it seems unnecessary to
  decompress a blockwise frequency representation to an expanded pixel
  representation, shuffle it from CPU to GPU, and then process it with
  a CNN that will learn something similar to a transform back to
  frequency representation in its first layers. Why not skip both
  steps and feed the frequency domain into the network directly?  In
  this paper we modify \libjpeg to produce DCT coefficients directly,
  modify a ResNet-50 network to accommodate the differently sized and
  strided input, and evaluate performance on ImageNet. We find
  networks that are both faster and more accurate, as well as networks
  with about the same accuracy but 1.77x faster than ResNet-50.","Lionel Gueguen*,Uber;Alex Sergeev,Uber Technologies Inc,;Ben Kadlec,Uber;Rosanne Liu,Uber AI Labs;Jason Yosinski,Uber AI Labs",lgueguen@uber.com;asergeev@uber.com;bkadlec@uber.com;rosanne@uber.com;jason@yosinski.com,Deep Learning/Efficient Inference Methods*; Algorithms/Classification,Poster
1947,TopRank: A practical algorithm for online stochastic ranking,"Online learning to rank is a sequential decision-making problem where in each round the learning agent chooses a list of items and receives feedback in the form of clicks from the user. Many sample-efficient algorithms have been proposed for this problem that assume a specific click model connecting rankings and user behavior. We propose a generalized click model that encompasses many existing models, including the position-based and cascade models. Our generalization motivates a novel online learning algorithm based on topological sort, which we call TopRank. TopRank is (a) more natural than existing algorithms, (b) has stronger regret guarantees than existing algorithms with comparable generality, (c) has a more insightful proof that leaves the door open to many generalizations, (d) outperforms existing algorithms empirically.","Tor Lattimore*,DeepMind;Branislav Kveton,Google Research;Shuai Li,The Chinese University of Hong Kong;Csaba Szepesvari,DeepMind/University of Alberta",tor.lattimore@gmail.com;bkveton@google.com;shuaili@cse.cuhk.edu.hk;szepesva@ualberta.ca,Algorithms/Bandit Algorithms*; Algorithms/Online Learning; Algorithms/Ranking and Preference Learning,Poster
1949,Learning from discriminative feature feedback,"We consider the problem of learning a multi-class classifier from labels as well as simple explanations that we call ""discriminative features"". We show that such explanations can be provided whenever the target concept is a decision tree, or more generally belongs to a particular subclass of DNF formulas. We present an efficient online algorithm for learning from such feedback and we give tight bounds on the number of mistakes made during the learning process. These bounds depend only on the size of the target concept and not on the overall number of available features, which could be infinite. We also demonstrate the learning procedure experimentally.","Sanjoy Dasgupta*,UCSD;Sivan Sabato,Ben-Gurion Universtiy of the Negev;Nicholas Roberts,University of California San Diego;Akansha Dey,UCSD",dasgupta@eng.ucsd.edu;sabatos@cs.bgu.ac.il;n3robert@ucsd.edu;a1dey@ucsd.edu,Theory/Learning Theory*; Algorithms/Active Learning; Algorithms/Online Learning,Poster
1960,RetGK: Graph Kernels based on Return Probabilities of Random Walks,"Graph-structured data arise in wide applications, such as computer vision, bioinformatics, and social networks. Quantifying similarities among graphs is a fundamental problem. In this paper, we develop a framework for computing graph kernels, based on return probabilities of random walks. The advantages of our proposed kernels are that they can effectively exploit various node attributes, while being scalable to large datasets. We conduct extensive graph classification experiments to evaluate our graph kernels. The experimental results show that our graph kernels significantly outperform other state-of-the-art approaches in both accuracy and computational efficiency.","Zhen Zhang*,WASHINGTON UNIVERSITY IN ST.LOUIS;Mianzhi Wang,Washington University in St. Louis;Yijian Xiang,Washington University in St. Louis;Yan Huang,Washington University in St. Louis;Arye Nehorai,WASHINGTON UNIVERSITY IN ST.LOUIS",zhen.zhang@wustl.edu;mianzhi.wang@wustl.edu;yijian.xiang@wustl.edu;yanhuang640@wustl.edu;nehorai@wustl.edu,Algorithms/Kernel Methods*; Algorithms/Classification; Applications/Computational Biology and Bioinformatics; Applications,Poster
1973,Deep Generative Markov State Models,"We propose a deep generative Markov State Model (DeepGenMSM) learning framework for inference of metastable dynamical systems and prediction of trajectories. After unsupervised training on time series data, the model contains (i) a probabilistic encoder that maps from high-dimensional configuration space to a small-sized vector indicating the membership to metastable (long-lived) states, (ii) a Markov chain that governs the transitions between metastable states and facilitates analysis of the long-time dynamics, and (iii) a generative part that samples the conditional distribution of configurations in the next time step. The model can be operated in a recursive fashion to generate trajectories to predict the system evolution from a defined starting state and propose new configurations. The DeepGenMSM is demonstrated to provide accurate estimates of the long-time kinetics and generate valid distributions for molecular dynamics (MD) benchmark systems. Remarkably, we show that DeepGenMSMs are able to make long time-steps in molecular configuration space and generate physically realistic structures in regions that were not seen in training data.","Hao Wu,;Andreas Mardt,;Luca Pasquali,;Frank Noe*,FU Berlin",hwu@zedat.fu-berlin.de;andreasmardt@outlook.de;luca.pasquali.91@gmail.com;frank.noe@fu-berlin.de,Applications/Time Series Analysis*; Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning; Algorithms/Spectral Metho,Poster
1974,Early Stopping for Nonparametric Testing ,"Early stopping of iterative algorithms is an algorithmic regularization method to avoid over-fitting in estimation and classification. In this paper, we show that early stopping can also be applied to obtain the minimax optimal testing in a general non-parametric setup. Specifically, a Wald-type test statistic is obtained based on an iterated estimate produced by functional gradient descent algorithms in a reproducing kernel Hilbert space. A notable contribution is to establish a ``sharp'' stopping rule: when the number of iterations achieves an optimal order, testing optimality is achievable; otherwise, testing optimality becomes impossible. As a by-product, a similar sharpness result is also derived for minimax optimal estimation under early stopping. All obtained results hold for various kernel classes, including Sobolev smoothness classes and Gaussian kernel classes. ","Meimei Liu*,Duke University;Guang Cheng,Purdue University",meimei.liu@duke.edu;chengg@purdue.edu,Theory/Learning Theory*; Theory/Regularization,Poster
1983,Solving Non-smooth Constrained Programs with Lower Complexity than $\mathcal{O}(1/\varepsilon)$: A Primal-Dual Homotopy Smoothing Approach,"We propose a new primal-dual homotopy smoothing algorithm for a linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex. The best known iteration complexity solving such a non-smooth problem is $\mathcal{O}(\varepsilon^{-1})$. In this paper, 
we show that by leveraging a local error bound condition on the dual function, the proposed algorithm can achieve a better primal convergence time of  $\mathcal{O}\l(\varepsilon^{-2/(2+\beta)}\log_2(\varepsilon^{-1})\r)$, where $\beta\in(0,1]$ is a local error bound parameter. 
As an example application, we show that the distributed geometric median problem, which can be formulated as a constrained convex program, has its dual function non-smooth but satisfying the aforementioned local error bound condition with $\beta=1/2$, therefore enjoying a convergence time of $\mathcal{O}\l(\varepsilon^{-4/5}\log_2(\varepsilon^{-1})\r)$. This result improves upon the $\mathcal{O}(\varepsilon^{-1})$ convergence time bound achieved by existing distributed optimization algorithms. Simulation experiments also demonstrate the performance of our proposed algorithm. ","Xiaohan Wei*,USC;Hao Yu,Alibaba Group (US) Inc ;Qing Ling,Sun Yat-Sen University;Michael Neely,USC",ubimeteor@gmail.com;hao.yu@alibaba-inc.com;lingqing556@mail.sysu.edu.cn;mikejneely@gmail.com,Optimization/Convex Optimization*; Probabilistic Methods/Distributed Inference; Theory/Computational Complexity,Poster
1984,Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,"Recent work has shown that performing inference with fast, very-low-bitwidth
(e.g., 1 to 2 bits) representations of values can yield surprisingly accurate results.
When coupled with FPGAs or custom hardware, the performance of binary models
has been shown to be staggering. We seek to improve upon these designs by
leveraging custom hardware‚Äôs ability to perform true bitwise operations. To this
end, we introduce the ""middle-out"" algorithm, which allows us to jointly learn
the value of each parameter with it‚Äôs individual bitwidth, effectively allowing a
model to have a fractional bitwidth. We find that heterogeneous representations
are fundamentally more expressive than their integer counterparts. We verify this
finding by training several models on ImageNet and show that with an average of
1.4 bits we are able to out perform state-of-the-art 2-bit architectures.","Joshua Fromm*,University of Washington;Shwetak Patel,University of Washington;Matthai Philipose,Microsoft Research",jwfromm@uw.edu;shwetak@cs.washington.edu;matthaip@microsoft.com,Deep Learning/Efficient Inference Methods*; Applications/Hardware and Systems; Deep Learning/CNN Architectures,Poster
1985,Unsupervised Learning of Object Landmarks through Conditional Image Generation,"In this paper, we consider the problem of learning landmarks for object categories without any manual annotations. We cast this as the problem of conditionally generating an image of an object from another one, where the images differ by acquisition time and/or viewpoint. The process is aided by providing the generator with a keypoint-like representation extracted from the target image through a tight bottleneck. This encourages the representation to distil information about the object geometry, which changes from source to target, while the appearance, which is shared by the source and target, is read off from the source alone. Conditioning simplifies the generation task significantly, to the point that adopting a simple perceptual loss instead of more sophisticated approaches such as adversarial training is sufficient to learn landmarks.
We show that our method is applicable to a large variety of datasets  --- faces, people, 3D objects, and digits --- without any modifications. 
We further demonstrate that we can learn landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors.","Tomas Jakab*,University of Oxford;Ankush Gupta,University of Oxford;Hakan Bilen,University of Edinburgh;Andrea Vedaldi,Oxford University",tomj@robots.ox.ac.uk;ankush@robots.ox.ac.uk;hbilen@ed.ac.uk;vedaldi@robots.ox.ac.uk,Deep Learning*; Algorithms/Representation Learning; Algorithms/Unsupervised Learning; Applications/Body Pos,Poster
1997,Probabilistic Neural Programmed Networks for Scene Generation,"In this paper we address the text to scene image generation problem. Generative models that capture the variability in complicated scenes containing rich semantics is a grand goal of image generation. Complicated scene images contain rich visual elements, compositional visual concepts, and complicated relations between objects. Generative models, as an analysis-by-synthesis process, should encompass the following three core components: 1) the generation process that composes the scene; 2) what are the primitive visual elements and how are they composed; 3) the rendering of abstract concepts into their pixel-level realizations.  We propose PNP-Net, a variational auto-encoder framework that addresses these three challenges: it flexibly composes images with a dynamic network structure, learns a set of distribution transformers that can compose distributions based on semantics, and decodes samples from these distributions into realistic images.","Zhiwei Deng*,Simon Fraser University;Jiacheng Chen,Simon Fraser University;Yifang Fu,Simon Fraser University;Greg Mori,Simon Fraser University",zhiweid@sfu.ca;jca348@sfu.ca;yifangf@sfu.ca;mori@cs.sfu.ca,Deep Learning/Generative Models*; Applications/Computer Vision; Deep Learning/Deep Autoencoders,Spotlight
2000,The streaming rollout of deep networks - towards fully model-parallel execution,"Deep neural networks, and in particular recurrent networks, are promising candidates to control autonomous agents that interact in real-time with the physical world. However, this requires a seamless integration of temporal features into the
network‚Äôs architecture. For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. Conventionally during inference, the layers of a network are computed in a sequential manner resulting in sparse temporal integration of information and long response times. In this study, we present a theoretical framework to describe rollouts, the level of model-parallelization they induce, and demonstrate differences in solving specific tasks. We prove that certain rollouts, also for networks with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. The streaming rollout maximizes these properties and enables a fully parallel execution of the network reducing runtime on massively parallel devices. Finally, we provide an open-source toolbox to design, train, evaluate, and interact with streaming rollouts.","Volker Fischer*,Robert Bosch GmbH;Jan Koehler,Robert Bosch GmbH;Thomas Pfeil,Robert Bosch GmbH",volker.fischer@de.bosch.com;Jan.Koehler@de.bosch.com;Thomas.Pfeil@de.bosch.com,Theory*; Deep Learning/Efficient Inference Methods; Deep Learning/Recurrent Networks; Theory/Competi,Poster
2001,KONG: Kernels for ordered-neighborhood graphs,"We present novel graph kernels for graphs with node and edge labels that have ordered neighborhoods, i.e. when neighbor nodes follow an order. Graphs with ordered neighborhoods are a natural data representation for evolving graphs where edges are created over time, which induces an order. Combining convolutional subgraph kernels and string kernels, we design new scalable algorithms for generation of explicit graph feature maps using sketching techniques. We obtain precise bounds for the approximation accuracy and computational complexity of the proposed approaches and demonstrate their applicability on real datasets.  In particular, our experiments demonstrate that neighborhood ordering results in more informative features. For the special case of general graphs, i.e. graphs without ordered neighborhoods, the new graph kernels yield efficient and simple algorithms for the comparison of label distributions between graphs. ","Moez Draief,""Noah's Ark Lab, Huawei Technologies"";Konstantin Kutzkov*,London School of Economics;Kevin Scaman,Noah's Ark Lab, Huawei Technologies;Milan Vojnovic,London School of Economics",moez.draief@huawei.com;kutzkov@gmail.com;kevin.scaman@huawei.com;M.Vojnovic@lse.ac.uk,Algorithms*; Algorithms/Classification; Algorithms/Kernel Methods,Spotlight
2007,GumBolt: Extending Gumbel trick to Boltzmann priors,"Boltzmann machines (BMs) are appealing candidates as powerful priors in variational autoencoders (VAEs), as they are capable of capturing nontrivial and multi-modal distributions over discrete variables. However, indifferentiability of the discrete units prohibits using the reparameterization trick, essential for low-noise back propagation. The Gumbel trick resolves this problem in a consistent way by relaxing the variables and distributions, but it is incompatible with BM priors. Here, we propose the GumBolt, a model that extends the Gumbel trick to BM priors in VAEs. GumBolt is significantly simpler than the recently proposed methods with BM prior and outperforms them by a considerable margin. It achieves state-of-the-art performance on permutation invariant MNIST and OMNIGLOT datasets in the scope of models with only discrete latent variables.  Moreover, the performance can be further improved by allowing multi-sampled (importance-weighted) estimation of log-likelihood in training, which was not possible with previous works. ","Amir Khoshaman*,D-Wave systems Inc;Mohammad Amin,D-Wave Systems Inc",khoshaman@gmail.com;mhsamin@dwavesys.com,Deep Learning/Generative Models*; Deep Learning/Efficient Training Methods; Probabilistic Methods/Graphical Models; Probabilisti,Poster
2010,Neural Networks Trained to Solve Differential Equations Learn General Representations,"We introduce a technique based on the singular vector canonical correlation analysis (SVCCA) for measuring the generality of neural network layers across a continuously-parametrized set of tasks. We illustrate this method by studying generality in neural networks trained to solve parametrized boundary value problems based on the Poisson partial differential equation. We find that the first hidden layer is general, and that deeper layers are successively more specific. Next, we validate our method against an existing technique that measures layer generality using transfer learning experiments. We find excellent agreement between the two methods, and note that our method is much faster, particularly for continuously-parametrized problems. Finally, we visualize the general representations of the first layers, and interpret them as generalized coordinates over the input domain.","Martin Magill*,University of Ontario Institute of Technology;Faisal Qureshi,University of Ontario Institute of Technology, Canada;Hendrick de Haan,University of Ontario Institute of Technology",martin.magill1@uoit.net;Faisal.Qureshi@uoit.ca;hendrick.dehaan@uoit.ca,"Algorithms/Multitask and Transfer Learning*; Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA); Algorithms/Representation Learning;",Poster
2018,Beauty-in-averageness and its contextual modulations: A Bayesian statistical account,"Understanding how humans perceive the likability of high-dimensional ``objects'' such as faces is an important problem in both cognitive science and AI/ML. Existing models of human preferences generally assume these preferences to be fixed. However, human assessment of facial attractiveness have been found to be highly context-dependent. Specifically, the classical Beauty-in-Averageness (BiA) effect, whereby a face blended from two original faces is judged to be more attractive than the originals, is significantly diminished or reversed when the original faces are recognizable, or when the morph is mixed-race/mixed gender and the attractiveness judgment is preceded by a race/gender categorization. This effect, dubbed Ugliness-in-Averageness (UiA), has previously been attributed to a disfluency account, which is both qualitative and clumsy in explaining BiA. We hypothesize, instead, that these contextual influences on face processing result from the dependence of attractiveness perception on an element of statistical typicality, and from an attentional mechanism that restricts face representation to a task-relevant subset of features, thus redefining typicality within that subspace. Furthermore, we propose a principled explanation of why statistically atypical objects are less likable: they incur greater encoding or processing cost associated with a greater prediction error, when the brain uses predictive coding to compare the actual stimulus properties with those expected from its associated categorical prototype. We use simulations to show  our model provides a parsimonious, statistically grounded, and quantitative account of contextual dependence of attractiveness. We also validate our model using experimental data from a gender categorization task. Finally, we make model predictions for a proposed experiment that can disambiguate the previous disfluency account and our statistical typicality theory.","Chaitanya Ryali*,University of California, San Diego;Angela Yu,UC San Diego",rckrishn@ucsd.edu;ajyu@ucsd.edu,Neuroscience and Cognitive Science/Cognitive Science*; Neuroscience and Cognitive Science/Visual Perception,Poster
2029,Distributed Weight Consolidation: A Brain Segmentation Case Study,"Collecting the large datasets needed to train deep neural networks can be very difficult, particularly for the many applications for which sharing and pooling data is complicated by practical, ethical, or legal concerns. However, it may be the case that derivative datasets or predictive models developed within individual sites can be shared and combined with fewer restrictions. Training on distributed data and combining the resulting networks is often viewed as continual learning, but these methods require networks to be trained sequentially. In this paper, we introduce distributed weight consolidation (DWC), a continual learning method to consolidate the weights of separate neural networks, each trained on an independent dataset. We evaluated DWC with a brain segmentation case study, where we consolidated dilated convolutional neural networks trained on independent structural magnetic resonance imaging (sMRI) datasets from different sites. We found that DWC led to increased performance on test sets from the different sites, while maintaining generalization performance for a very large and completely independent multi-site dataset, compared to an ensemble baseline.","Patrick McClure*,NIMH;Charles Zheng,National Institute of Mental Health;Jakub Kaczmarzyk,MIT;John Rogers-Lee,NIMH;Satrajit Ghosh,MIT;Dylan Nielson,NIMH;Peter Bandettini,NIMH;Francisco Pereira,National Institute of Mental Health",patrick.s.mcclure@gmail.com;charles.zheng@nih.gov;jakubk@mit.edu;john.rodgers-lee@nih.gov;satra@mit.edu;dylan.nielson@nih.gov;bandettini@nih.gov;francisco.pereira@gmail.com,Neuroscience and Cognitive Science/Brain Segmentation*; Algorithms/Online Learning; Applications/Image Segmentation; Deep Learning; Probabilistic Meth,Poster
2034,Efficient Projection onto the Perfect Phylogeny Model,"Several algorithms build on the perfect phylogeny model to infer evolutionary trees. This problem is particularly hard, when evolutionary trees are inferred from the fraction of genomes that have mutations in different positions, across different samples. Existing algorithms require exhaustive searches over the space of possible trees. At the center of these algorithms is a projection problem that assigns a fitness cost to phylogenetic trees. In order to perform a wide search over the space of the trees, it is critical to solve this projection problem fast. In this paper, we use Moreau's decomposition for proximal operators, and a tree reduction scheme, to develop a new algorithm to compute this projection. Our algorithm terminates with an exact solution in a finite number of steps, and is extremely fast. In particular, it can search over all evolutionary trees with fewer than 11 nodes, a size relevant for several biological problems (more than 2 billion trees) in about 2 hours.","Bei Jia,Boston College; Surjyendu Ray,Boston College;Sam Safavi,Boston College;Jose Bento*,Boston College",jiabe@bc.edu;raysc@bc.edu;Safavisa@bc.edu;jose.bento@bc.edu,Optimization*; Algorithms; Applications/Computational Biology and Bioinformatics; Optimization/Convex Optimiz,Poster
2041,TETRIS: TilE-matching the TRemendous Irregular Sparsity,"Compressing neural networks by pruning weights with small magnitudes can significantly reduce the computation and storage cost. Although pruning makes the model smaller, it is difficult to get practical speedup in modern computing platforms such as CPU and GPU due to the irregularity. Recent efforts on hardware-friendly pruning involve structured sparsity with different granularity and dimensionality. 
Simply increasing the sparsity granularity can lead to better hardware utilization, but it will compromise the sparsity for maintaining accuracy. In this work, we propose a novel method, TETRIS, to achieve both better hardware utilization and higher sparsity. Just like a tile-matching game, we cluster the irregularly distributed weights with small value into structured blocks by reordering the input/output dimension and structurally prune them. Results show that it can achieve comparable sparsity with the irregular element-wise pruning and demonstrate negligible accuracy loss. Ideal speedup, proportional to the sparsity, is experimentally demonstrated. Our proposed method provides a new solution toward algorithm and architecture co-optimization for accuracy-efficiency trade-off.","Yu Ji*,Tsinghua University; UCSB;Ling Liang,UCSB;Lei Deng,UCSB;Youyang Zhang,Tsinghua University;Youhui Zhang,Tsinghua University;Yuan Xie,Nil",jiy15@mails.tsinghua.edu.cn;lingliang@umail.ucsb.edu;leideng@ece.ucsb.edu;youyang87@163.com;zyh02@tsinghua.edu.cn;yuanxie@ece.ucsb.edu,Deep Learning/Efficient Inference Methods*; Deep Learning/Optimization for Deep Networks,Poster
2048,Cooperative neural networks (CoNN): Exploiting prior independence structure for improved classification,"We propose a new approach, called cooperative neural networks (CoNN), which use a set of cooperatively trained neural networks to capture latent representations that exploit prior given independence structure. The model is more flexible than traditional graphical models based on exponential family distributions, but incorporates more domain specific prior structure than traditional deep networks or variational autoencoders. The framework is very general and can be used to exploit the independence structure of any graphical model. We illustrate the technique by showing that we can transfer the independence structure of the popular Latent Dirichlet Allocation (LDA) model to a cooperative neural network, CoNN-sLDA. Empirical evaluation of CoNN-sLDA on supervised text classification tasks demonstrate that the theoretical advantages of prior independence structure can be realized in practice - we demonstrate a 23 percent reduction in error on the challenging MultiSent data set compared to state-of-the-art.","Harsh Shrivastava*,Georgia Tech;Eugene Bart,PARC;Bob Price,PARC;Hanjun Dai,Georgia Tech;Bo Dai,Google Brain;Srinivas Aluru,Georgia Institute of Technology",hshrivastava3@gatech.edu;eugenebart@gmail.com;bob.price@parc.com;hanjundai@gatech.edu;bohr.dai@gmail.com;aluru@cc.gatech.edu,Algorithms/Classification*; Applications/Text Analysis; Deep Learning/Supervised Deep Networks,Poster
2051,Differentially Private Robust PCA,"In this paper, we initiate the study of the following problem: given a private matrix $A \in \R^{n \times d}$, output a rank-$k$ matrix $B$, while satisfying differential privacy, such that 
$ \norm{  A - B }_p \leq \alpha \mathsf{OPT}_k(A) + \gamma,$ where 
$\norm{  M }_p$ is the entry-wise $\ell_p$ norm 
and $\mathsf{OPT}_k(A):=\min_{\mathsf{rank}(X) \leq k} \norm{  A - X}_p$. 
It is well known that low-rank approximation w.r.t. entrywise $\ell_p$-norm, for $p \in [1,2)$, yields robustness to gross outliers in the data. 

We propose an algorithm that guarantees $\alpha=\widetilde{O}(k^2), \gamma=\widetilde{O}(k(n+d)/\varepsilon)$, runs in $\widetilde O((n+d)\poly~k)$ time and uses $O(k(n+d)\log k)$ space. This is an {\em exponential improvement}  in $\alpha$ over  known differentially private algorithms for $p=2$.
We also study extensions to the streaming setting where entries of the matrix arrive in an arbitrary order and output is produced at the very end or continually.  We also study the related problem of differentially private robust subspace learning that requires us to output a rank-$k$ projection matrix $\Pi$ such that 
$\norm{  A - A \Pi }_p \leq \alpha \mathsf{OPT}_k(A) + \tau.$ ","Raman Arora,Johns Hopkins University;Vladimir Braverman,Johns Hopkins University;Jalaj Upadhyay*,Johns Hopkins University",arora@cs.jhu.edu;vova@cs.jhu.edu;jalaj.kumar.upadhyay@gmail.com,"Applications/Privacy, Anonymity, and Security",Poster
2052,Meta-Learning MCMC Proposals,"Effective implementations of sampling-based probabilistic inference often require manually constructed, model-specific proposals. Inspired by recent progresses in meta-learning for training learning agents that can generalize to unseen environments, we propose a meta-learning approach to building effective and generalizable MCMC proposals. We parametrize the proposal as a neural network to provide fast approximations to block Gibbs conditionals. The learned neural proposals generalize to occurrences of common structural motifs across different models, allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required. We explore several applications including open-universe Gaussian mixture models, in which our learned proposals outperform a hand-tuned sampler, and a real-world named entity recognition task, in which our sampler yields higher final F1 scores than classical single-site Gibbs sampling.","Tongzhou Wang*,Facebook AI Research;Yi Wu,UC Berkeley;David Moore,University of California, Berk;Stuart Russell,UC Berkeley",tongzhou.wang.1994@gmail.com;jxwuyi@gmail.com;davmre@gmail.com;russell@berkeley.edu,Probabilistic Methods/MCMC*; Deep Learning/Meta-Learning; Probabilistic Methods/Graphical Models,Poster
2053,An Information-Theoretic Analysis for Thompson Sampling with Many Actions,"Information-theoretic Bayesian regret bounds of Russo and Van Roy capture the dependence of regret on prior uncertainty. However, this dependence is through entropy, which can become arbitrarily large as the number of actions increases.  We establish new bounds that depend instead on a notion of rate-distortion.  Among other things, this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit.  We also offer a bound for the logistic bandit that dramatically improves on the best previously available, though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.","Shi Dong*,Stanford University;Benjamin Van Roy,Stanford",sdong15@stanford.edu;bvr@stanford.edu,Algorithms/Bandit Algorithms*; Theory/Information Theory; Theory/Learning Theory,Poster
2054,Flexible and accurate inference and learning for deep generative models,"We introduce new approach to learning in hierarchical latent-variable generative
models called the ‚Äúdistributed distributional code Helmholtz machine‚Äù, which
emphasises flexibility and accuracy in the inferential process. In common with the
original Helmholtz machine and later variational autoencoder algorithms (but unlike
adverserial methods) our approach learns an explicit inference or ‚Äúrecognition‚Äù
model to approximate the posterior distribution over the latent variables. Unlike
in these earlier methods, the posterior representation is not limited to a narrow
tractable parameterised form (nor is it represented by samples). To train the
generative and recognition models we develop an extended wake-sleep algorithm
inspired by the original Helmholtz Machine. This makes it possible to learn
hierarchical latent models with both discrete and continuous variables, where
an accurate posterior representation is essential. We demonstrate that the new
algorithm outperforms current state-of-the-art methods on synthetic, natural image patch and the MNIST data sets.","Eszter Vertes*,Gatsby Unit, UCL;Maneesh Sahani,Gatsby Unit, UCL",vertes.eszter@gmail.com;maneesh@gatsby.ucl.ac.uk,Probabilistic Methods/Graphical Models*; Probabilistic Methods/Hierarchical Models; Probabilistic Methods/Latent Variable Models,Poster
2059,The Price of Privacy for Low-rank Factorization,"In this paper, we study what  price one has to pay to release \emph{differentially private low-rank factorization} of a matrix. We consider various settings that are close to the real world applications of low-rank factorization: (i) the manner in which matrices are updated (row by row or in an arbitrary manner), (ii) whether matrices are distributed or not, and (iii) how the output is produced (once at the end of all updates, also known as \emph{one-shot algorithms}  or continually). Even though these settings are well studied without privacy, surprisingly, there are no  private algorithm for these settings (except when a matrix is updated row by row). We present the first set of differentially private algorithms for all these settings.  

Our algorithms when private matrix is updated in an arbitrary manner promise differential privacy with respect to two stronger privacy guarantees than previously studied, use space and time \emph{comparable} to the non-private algorithm, and achieve \emph{optimal accuracy}. To complement our positive results, we also prove that the space required by our algorithms is optimal up to logarithmic factors. When data matrices are distributed over multiple servers, we give a non-interactive differentially private algorithm  with communication cost independent of dimension. In concise, we give algorithms that incur {\em optimal cost across all parameters of interest}. We also perform experiments  to verify that all our algorithms  perform well in practice and outperform the best known algorithm until now for large range of parameters. ","Jalaj Upadhyay*,Johns Hopkins University",jalaj.kumar.upadhyay@gmail.com,"Applications/Privacy, Anonymity, and Security",Poster
2065,Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator,"We consider adaptive control of the Linear Quadratic Regulator (LQR), where an
unknown linear system is controlled subject to quadratic costs. Leveraging recent
developments in the estimation of linear systems and in robust controller synthesis,
we present the first provably polynomial time algorithm that achieves sub-linear
regret on this problem. We further study the interplay between regret minimization
and parameter estimation by proving a lower bound on the expected regret in
terms of the exploration schedule used by any algorithm. Finally, we conduct a
numerical study comparing our robust adaptive algorithm to other methods from
the adaptive LQR literature, and demonstrate the flexibility of our proposed method
by extending it to a demand forecasting problem subject to state constraints.","Sarah Dean,;Horia Mania,UC Berkeley;Nikolai Matni,UC Berkeley;Benjamin Recht,UC Berkeley;Stephen Tu*,UC Berkeley",dean_sarah@berkeley.edu;hmania@berkeley.edu;nmatni@berkeley.edu;brecht@berkeley.edu;tu.stephenl@gmail.com,Theory/Control Theory*; Algorithms/Online Learning; Theory/Learning Theory,Poster
2068,Bilevel Distance Metric Learning for Robust Image Recognition,"Metric learning, aiming to learn a discriminative Mahalanobis distance matrix M that can effectively reflect the similarity between data samples, has been widely studied in various image recognition problems. Most of the existing metric learning methods input the features extracted directly from the original data in the preprocess phase. What's worse, these features usually take no consideration of the local geometrical structure of the data and the noise existed in the data, thus they may not be optimal for the subsequent metric learning task. In this paper, we integrate both feature extraction and metric learning into one joint optimization framework and propose a new bilevel distance metric learning model. Specifically,  the lower level characterizes the intrinsic data structure using graph regularized sparse coefficients, while the upper level forces the data samples from the same class to be close to each other and pushes those from different classes far away. 
 In addition, leveraging the KKT conditions and the alternating direction method (ADM), we derive an efficient algorithm to solve the proposed new model. Extensive experiments on various occluded datasets demonstrate the effectiveness and robustness of our method.","Jie Xu,Xidian University;Lei Luo,University of Pittsburgh;Cheng Deng,Xidian University;Heng Huang*,University of Pittsburgh",xujie@stu.xidian.edu.cn;luolei2017@pitt.edu;chdeng.xd@gmail.com;henghuanghh@gmail.com,Applications/Computer Vision*; Applications/Object Recognition,Poster
2070,Differentially Private Uniformly Most Powerful Tests for Binomial Data,"We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of Differential Privacy (DP), optimizing finite sample performance. We show that in general, DP hypothesis tests for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure, we prove a ‚ÄòNeyman-Pearson lemma‚Äô for binomial data under DP, where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a random variable, whose distribution we coin ‚ÄúTruncated-Uniform-Laplace‚Äù (Tulap), a generalization of the Staircase and discrete Laplace distributions. Furthermore, we obtain exact p-values, which are easily computed in terms of the Tulap random variable. We show that our results also apply to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that our tests have exact type I error, and are more powerful than current techniques.","Jordan Awan*,Penn State University;Aleksandra Slavkoviƒá,Pennsylvania State University",jaa5462@psu.edu;sesa@psu.edu,"Theory/Frequentist Statistics*; Applications/Privacy, Anonymity, and Security",Poster
2074,Scalable Coordinated Exploration in Concurrent Reinforcement Learning,"We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on the seed sampling concept introduced in Dimakopoulou and Van Roy (2018) and on a randomized value function learning algorithm from Osband et al. (2016). We demonstrate that, for simple tabular contexts, the approach is competitive with those previously proposed in Dimakopoulou and Van Roy (2018) and with a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.","Maria Dimakopoulou*,Stanford University;Ian Osband,Google Deepmind;Benjamin Van Roy,Stanford",madima@stanford.edu;iosband@google.com;bvr@stanford.edu,Reinforcement Learning and Planning/Exploration*; Reinforcement Learning and Planning/Reinforcement Learning,Poster
2077,Integrated accounts of behavioral and neuroimaging data using flexible recurrent neural network models,"Neuroscience studies of human decision-making abilities commonly involve subjects completing a decision-making task while BOLD signals are recorded using fMRI. Hypotheses are tested about which brain regions mediate the effect of past experience, such as rewards, on future actions. One standard approach to this is model-based fMRI data analysis, in which a model is fitted to the behavioral data, i.e., a subject's choices, and then the neural data are parsed to find brain regions whose BOLD signals are related to the model's internal signals. However, the internal mechanics of such purely behavioral models are not constrained by the neural data, and therefore might miss or mischaracterize aspects of the brain. To address this limitation, we introduce a new method using recurrent neural network models that are flexible enough to be fitted jointly to the behavioral and neural data. We trained a model so that its internal states were suitably related to neural activity during the task, while at the same time its output predicted the next action a subject would execute. We then used the fitted model to create a novel visualization of the relationship between the activity in brain regions at different times following a reward and the choices the subject subsequently made. Finally, we validated our method using a previously published dataset. We showed that the model was able to recover the underlying neural substrates that were discovered by explicit model engineering in the previous work, and also derived new results regarding the temporal pattern of brain activity.","Amir Dezfouli*,UNSW;Richard Morris,U Sydney;Fabio Ramos,U Sydney;Peter Dayan,Gatsby Computational Neuroscience Unit, UCL;Bernard Balleine,UNSW",akdezfuli@gmail.com;richard.morris@sydney.edu.au;fabio.ramos@sydney.edu.au;dayan@gatsby.ucl.ac.uk;bernard.balleine@unsw.edu.au,Neuroscience and Cognitive Science/Neuroscience*; Neuroscience and Cognitive Science/Brain Imaging; Neuroscience and Cognitive Science/Human or,Oral
2078,"BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training","In distributed machine learning (DML), the network performance between machines significantly impacts the speed of iterative training. In this paper we propose BML, a new gradient synchronization algorithm with higher network performance and lower network cost than the current practice. BML runs on BCube network, instead of using the traditional Fat-Tree topology. BML algorithm is designed in such a way that, compared to the parameter server (PS) algorithm on a Fat-Tree network connecting the same number of server machines, BML achieves theoretically1/k of the gradient synchronization time, with k/5 of switches (the typical number of k is 2‚àº4). Experiments of MNIST and VGG-19 benchmarks on a testbed with 9 dual-GPU servers show that, BML reduces the job completion time of DML training by up to 56.4%","Songtao Wang*,Tsinghua University;Dan Li,Tsinghua University;Yang Cheng,Tsinghua University;Jinkun Geng,Tsinghua University;Yanshu Wang,Tsinghua Univeristy;Shuai Wang,Tsinghua University;Shutao Xia,Tsinghua University;Jianping Wu,Tsinghua University",wangst12@mails.tsinghua.edu.cn;tolidan@tsinghua.edu.cn;cheng-y16@mails.tsinghua.edu.cn;steam1994@163.com;wangyanshu2016@gmail.com;s-wang17@mails.tsinghua.edu.cn;xiast@sz.tsinghua.edu.cn;jianping@cernet.edu.cn,Deep Learning/Efficient Training Methods,Poster
2083,Inexact trust-region algorithm on Riemannian manifolds,"We consider an inexact variant of the popular Riemannian trust-region algorithm for structured big-data minimization problems. The proposed algorithm approximates the gradient and the Hessian in addition to the solution of a (trust-region) sub-problem. We provide a total complexity bound to achieve $¬•epsilon$-approximate second-order optimality under mild conditions on inexact gradient and Hessian. Addressing large-scale finite-sum problems, a sub-sampled algorithm is proposed as a practical algorithm, where gradient and Hessian are generated by a random sampling technique. Numerical evaluations of the principal components analysis (PCA) and the matrix completion (MC) problem demonstrate that the proposed algorithm outperforms state-of-the-art Riemannian deterministic and stochastic gradient algorithms. ","Hiroyuki Kasai*,The University of Electro Communications;Bamdev Mishra,Microsoft",kasai@is.uec.ac.jp;bamdevm@gmail.com,Optimization/Non-Convex Optimization*; Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning,Poster
2089,Can We Gain More from Orthogonality Regularizations in Training Deep Networks?,"This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how we can enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on three state-of-the-art models: ResNet, WideResNet, and ResNeXt, on CIFAR-10 and CIFAR-100 datasets. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and accelerated and more stable convergences.","Nitin Bansal*,Texas A&M University;Xiaohan Chen,Texas A&M University;Zhangyang Wang,TAMU",bansa01@tamu.edu;chernxh@tamu.edu;atlaswang@tamu.edu,Deep Learning/CNN Architectures,Poster
2091,Binary Rating Estimation with Graph Side Information,"While rich experimental evidences show that one can better estimate users' unknown ratings with the aid of graph side information such as social graphs, its theoretical understanding is still lacking. In this work, we study the binary rating estimation problem to understand the fundamental value of graph side information. Considering a simple correlation model between a rating matrix and a graph, we characterize the sharp threshold on the number of observed entries required to recover the rating matrix (called the optimal sample complexity) as a function of the quality of graph side information (to be detailed). To the best of our knowledge, we are the first to reveal when and by how much the graph side information reduces sample complexity. Further, we propose a computationally efficient algorithm that achieves the limit. Our experimental results on synthetic and real-world data demonstrate the superiority of our algorithm over the state of the arts.","Kwangjun Ahn,KATUSA, ROK Army;Kangwook Lee*,EE at KAIST;Hyunseung Cha,Kakao Brain;Changho Suh,KAIST",kjahnkorea@kaist.ac.kr;kw1jjang@kaist.ac.kr;hs.cha@kakaobrain.com;chsuh@kaist.ac.kr,Theory/Information Theory*; Algorithms/Clustering; Algorithms/Collaborative Filtering; Algorithms/Spectral Methods; The,Poster
2093,SimplE Embedding for Link Prediction in Knowledge Graphs,"Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links between the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement (which we call SimplE) of CP to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge in terms of logical rules can be incorporated into these embeddings through weight tying. We prove SimplE is fully-expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques.","Seyed Mehran Kazemi*,University of British Columbia;David Poole,University of British Columbia",smkazemi@cs.ubc.ca;poole@cs.ubc.ca,Algorithms/Relational Learning*; Applications/Matrix and Tensor Factorization; Applications/Recommender Systems,Poster
2098,Differentially Private Contextual Linear Bandits,"We study the contextual linear bandit problem, a version of the standard stochastic multi-armed bandit (MAB) problem where a learner sequentially selects actions to maximize a reward which depends also on a user provided per-round context. Though the context is chosen arbitrarily or adversarially, the reward is assumed to be a stochastic function of a feature vector that encodes the context and selected action. Our goal is to devise private learners for the contextual linear bandit problem.

We first show that using the standard definition of differential privacy results in linear regret. So instead, we adopt the notion of joint differential privacy, where we assume that the action chosen on day t is only revealed to user t and thus needn't be kept private that day, only on following days. We give a general scheme converting the classic linear-UCB algorithm into a joint differentially private algorithm using the tree-based algorithm. We then apply either Gaussian noise or Wishart noise to achieve joint-differentially private algorithms and bound the resulting algorithms' regrets. In addition, we give the first lower bound on the additional regret any private algorithms for the MAB problem must incur.","Roshan Shariff*,University of Alberta;Or Sheffet,University of Alberta",roshan.shariff@ualberta.ca;osheffet@ualberta.ca,"Algorithms/Bandit Algorithms*; Applications/Privacy, Anonymity, and Security",Poster
2101,"Submodular Field Grammars: Representation, Inference, and Application to Image Parsing","Natural scenes contain many layers of part-subpart structure, and distributions over them are thus naturally represented by stochastic image grammars, with one production per decomposition of a part. Unfortunately, in contrast to language grammars, where the number of possible split points for a production $A \rightarrow BC$ is linear in the length of $A$, in an image there is an exponential number of ways to split a region into subregions. This makes parsing intractable and requires image grammars to be severely restricted in practice, for example by allowing only rectangular regions. In this paper, we address this problem by associating with each production a submodular Markov random field whose labels are the subparts and whose labeling segments the current object into these subparts. We call the result a submodular field grammar (SFG). Finding the MAP split of a region into subregions is now tractable, and by exploiting this we develop an efficient approximate algorithm for MAP parsing of images with SFGs. Empirically, we present promising improvements in accuracy when using SFGs for scene understanding, and show exponential improvements in inference time compared to traditional methods, while returning comparable minima.","Abram Friesen*,University of Washington;Pedro Domingos,University of Washington)",afriesen@cs.washington.edu;pedrod@cs.washington.edu,Probabilistic Methods/Graphical Models*; Algorithms/Structured Prediction; Applications/Computer Vision; Applications/Image Segmenta,Poster
2111,A Bridging Framework for Model Optimization and Deep Propagation,"Optimizing task-related mathematical model is one of the most fundamental methodologies in statistic and learning areas. However, generally designed schematic iterations may hard to investigate complex data distributions in real-world applications. Recently, training deep propagations (i.e., networks) has gained promising performance in some particular tasks. Unfortunately, existing networks are often built in heuristic manners, thus lack of principled interpretations and solid theoretical supports. In this work, we provide a new paradigm, named Propagation and Optimization based Deep Model (PODM), to bridge the gaps between these different mechanisms (i.e., model optimization and deep propagation). On the one hand, we utilize PODM as a deeply trained solver for model optimization. Different from these existing network based iterations, which often lack theoretical investigations, we provide strict convergence analysis for PODM in the challenging nonconvex and nonsmooth scenarios. On the other hand, by relaxing the model constraints and performing end-to-end training, we also develop a PODM based strategy to integrate domain knowledge (formulated as models) and real data distributions (learned by networks), resulting in a generic ensemble framework for challenging real-world applications. Extensive experiments verify our theoretical results and demonstrate the superiority of PODM against these state-of-the-art approaches.","Risheng Liu*,Dalian University of Technology;Shichao Cheng,Dalian University of Technology;Xiaokun Liu,Dalian University of Technology;Long Ma,School of Software Technology, Dalian University of Technology;Xin Fan,Dalian University of Technology;Zhongxuan Luo,DALIAN UNIVERSITY OF TECHNOLOGY",rsliu@dlut.edu.cn;shichao.cheng@outlook.com;falling47@hotmail.com;malone94319@gmail.com;xin.fan@dlut.edu.cn;zxluo@dlut.edu.cn,Deep Learning/Meta-Learning*; Applications/Computer Vision,Poster
2114,Completing State Representations using Spectral Learning,"A central problem in dynamical system modeling is state discovery---that is, finding a compact summary of the past that captures the information needed to predict the future. Predictive State Representations (PSRs) enable clever spectral methods for state discovery; however, while consistent in the limit of infinite data, these methods often suffer from poor performance in the low data regime. In this paper we develop a novel algorithm for incorporating domain knowledge, in the form of an imperfect state representation, as side information to speed spectral learning for PSRs. We prove theoretical results characterizing the relevance of a user-provided state representation, and design spectral algorithms that can take advantage of a relevant representation. Our algorithm utilizes principal angles to extract the relevant components of the representation, and is robust to mis-specification. Empirical evaluation on synthetic HMMs, an aircraft identification domain, and a gene splice dataset shows that, even with weak domain knowledge, the algorithm can significantly outperform standard PSR learning.","Nan Jiang*,University of Illinois at Urbana-Champaign;Alex Kulesza,Google Research;Satinder Singh,UMich",nanjiang@umich.edu;kulesza@google.com;baveja@umich.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Algorithms/Spectral Methods,Poster
2116,Optimization of Smooth Functions with Noisy Observations: Local Minimax Rates,"We consider the problem of global optimization of an unknown non-convex smooth function with noisy zeroth-order feedback. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations. We show that for functions with fast growth around their global minima, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries than worst-case global minimax theory predicts. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems. On the other hand, we show that in the worst case no algorithm can converge faster than the minimax rate of estimating an unknown functions in linf-norm. Finally, we show that non-adaptive algorithms, although optimal in a global minimax sense, do not attain the optimal local minimax rate.","Yining Wang*,CMU;Sivaraman Balakrishnan,Carnegie Mellon University;Aarti Singh,Carnegie Mellon University",ynwang.yining@gmail.com;siva@stat.cmu.edu;aarti@cs.cmu.edu,Theory/Information Theory*; Theory/Frequentist Statistics; Theory/Learning Theory,Poster
2125,Adding One Neuron Can Eliminate All Bad Local Minima,"One of the main difficulties in analyzing neural networks is the non-convexity of the loss function which may have many bad local minima. In this paper, we study the landscape of neural networks for binary classification tasks. Under mild assumptions, we prove that after adding one special neuron with a skip connection to the output, or one special neuron per layer, every local minimum is a global minimum. ","SHIYU LIANG*,UIUC;Ruoyu Sun,University of Illinois at Urbana-Champaign;Jason Lee,USC;Rayadurgam Srikant,UIUC",sliang26@illinois.edu;ruoyus@illinois.edu;jasonlee@marshall.usc.edu;rsrikant@illinois.edu,Optimization/Non-Convex Optimization*; Deep Learning/Optimization for Deep Networks,Poster
2132,Mean-field theory of graph neural networks in graph partitioning,"A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks, the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner, whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover, whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions, a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments. ","Tatsuro Kawamoto*,National Institute of Advanced Industrial Science and Technology;Masashi Tsubaki,AIST;Tomoyuki Obuchi,Tokyo Institute of Technology",kawamoto.tatsuro@gmail.com;tsubaki.masashi@aist.go.jp;obuchi@c.titech.ac.jp,Theory/Statistical Physics of Learning*; Algorithms/Clustering,Poster
2136,The Physical Systems Behind Optimization Algorithms,"We use differential equations based approaches to provide some {\it \textbf{physics}} insights into analyzing the dynamics of popular optimization algorithms in machine learning. In particular, we study gradient descent, proximal gradient descent, coordinate gradient descent, proximal coordinate gradient, and Newton's methods as well as their Nesterov's accelerated variants in a unified framework motivated by a natural connection of optimization algorithms to physical systems. Our analysis is applicable to more general algorithms and optimization problems {\it \textbf{beyond}} convexity and strong convexity, e.g. Polyak-\L ojasiewicz and error bound conditions (possibly nonconvex).","Lin Yang,Princeton University;Raman Arora,Johns Hopkins University;Vladimir Braverman,Johns Hopkins University;Tuo Zhao*,Georgia Tech",lin.yang@princeton.edu;arora@cs.jhu.edu;vova@cs.jhu.edu;tuo.zhao@isye.gatech.edu,Theory/Control Theory*; Optimization/Convex Optimization; Optimization/Non-Convex Optimization,Poster
2155,Top-k lists: Models and Algorithms,"The classic Mallows model is a widely-used tool to realize distributions on permutations.  Motivated by common practical situations, in this paper, we generalize Mallows to model distributions on \topk lists by using a suitable distance measure between \topk lists.  Unlike many earlier work, our model is both analytically tractable and computationally efficient.  We demonstrate this by studying two basic problems in this model, namely, sampling and reconstruction, from both algorithmic and practical points of view.","Flavio Chierichetti,Sapienza University;Anirban Dasgupta*,IIT Gandhinagar;Shahrzad Haddadan,Sapienza University, Rome, Italy;Ravi Kumar,Google;Silvio Lattanzi,Google",flavio@di.uniroma1.it;anirbandg@iitgn.ac.in;shahrzad.haddadan@gmail.com;ravi.k53@gmail.com;silviol@google.com,Algorithms/Ranking and Preference Learning*; Algorithms/Unsupervised Learning; Probabilistic Methods/MCMC,Poster
2160,Amortized Inference Regularization,"The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.","Rui Shu*,Stanford University;Hung Bui,Google;Shengjia Zhao,Stanford University;Mykel Kochenderfer,Stanford University;Stefano  Ermon,Stanford University",ruishu@stanford.edu;buih@google.com;sjzhao@stanford.edu;mykel@stanford.edu;ermon@cs.stanford.edu,Deep Learning/Generative Models*; Probabilistic Methods/Latent Variable Models; Probabilistic Methods/Variational Inference,Poster
2167,Maximum Causal Tsallis Entropy Imitation Learning,"In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. 
The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning
(MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Boltzmann Gibbs entropy is less effective. We validate the proposed method in two simulation studies and MCTEIL outperforms existing imitation learning methods in terms of average returns and learning multi-modal policies.","Kyungjae Lee,Seoul National University;Sungjoon Choi,Disney Research;Songhwai Oh*,Seoul National University",kyungjae.lee@cpslab.snu.ac.kr;sam.choi@kakaobrain.com;songhwai@snu.ac.kr,Reinforcement Learning and Planning*; Deep Learning/Adversarial Networks; Reinforcement Learning and Planning/Exploration; Reinforce,Poster
2171,Limited Memory Kelley's Method Converges for Composite Convex and Submodular Objectives,"The original simplicial method ({\sc OSM}), a variant of the classic Kelley's cutting plane method, has been shown to converge to the minimizer of composite convex and submodular objectives, though no rate of convergence for this method was known. Moreover, {\sc OSM} is required to solve subproblems in each iteration whose size grows linearly in the number of iterations. We propose a limited memory version of Kelley's method ({\sc L-KM}) that is a novel adaptation of {\sc OSM} and requires limited memory independent of the iteration (at most $n+1$ constraints for an $n$-dimensional problem), while maintaining convergence to the optimal solution. We further show that the dual method of {\sc L-KM} is a special case of the Fully-Corrective Frank-Wolfe ({\sc FCFW}) method with approximate correction, thereby deriving a limited memory version of {\sc FCFW} method and proving a rate of convergence for {\sc L-KM}. Though we propose {\sc L-KM} for minimizing composite convex and submodular objectives, our results on limited memory version of FCFW hold for general polytopes, which is of independent interest.","Song Zhou,Cornell University;Swati Gupta*,Georgia Institute of Technology;Madeleine Udell,Cornell University",sz557@cornell.edu;swatig@gatech.edu;udell@cornell.edu,Optimization/Convex Optimization*; Optimization/Submodular Optimization,Spotlight
2179,Semi-Supervised Learning with Declaratively Specified Entropy Constraints,"We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL).  The proposed method can be used to specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics.  Our technique achieves consistent improvements over prior frameworks for specifying SSL techniques on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.","Haitian Sun*,Carnegie Mellon University;William Cohen,Carnegie Mellon University;Lidong Bing,Tencent AI Lab",htsun@umich.edu;wcohen@cs.cmu.edu;binglidong@gmail.com,Algorithms/Semi-Supervised Learning*; Applications/Natural Language Processing,Poster
2187,End-to-end Symmetry Preserving Inter-atomic Potential Energy Model for Finite and Extended Systems,"Machine learning models are changing the paradigm of molecular modeling, which is a fundamental tool for material science, chemistry, and computational biology. Of particular interest is the inter-atomic potential energy surface (PES). Here we develop Deep Potential - Smooth Edition (DeepPot-SE), an end-to-end machine learning-based PES model, which is able to efficiently represent the PES for a wide variety of systems with the accuracy of ab initio quantum mechanics models. By construction, DeepPot-SE is extensive and continuously differentiable, scales linearly with system size, and preserves all the natural symmetries of the system. Further, we show that DeepPot-SE describes finite and extended systems including organic molecules, metals, semiconductors, and insulators with high fidelity.","Linfeng Zhang,Princeton University;Jiequn Han*,Princeton University;Han Wang,Institute of Applied Physics and Computational Mathematics;Wissam  Saidi,University of Pittsburgh;Roberto Car,Princeton University;Weinan E,Princeton University",linfengz@princeton.edu;frankhan91@gmail.com;wang_han@iapcm.ac.cn;alsaidi@pitt.edu;rcar@princeton.edu;weinan@math.princeton.edu,Deep Learning/Supervised Deep Networks*; Algorithms/Regression,Poster
2188,Sparsified SGD with Memory,"Nowadays machine learning applications require stochastic optimization algorithms that can be implemented on distributed systems. The communication overhead of the algorithms is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst this scheme shows good performance in practice it eluded theoretical analysis so far.

In this work we analyze a variant of Stochastic Gradient Descent (SGD) with k-sparsification (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD. That is, the communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and especially the better scalability for distributed applications.","Sebastian Stich,EPFL;Jean-Baptiste Cordonnier,EPFL;Martin Jaggi*,EPFL",sebastian.stich@epfl.ch;jean-baptiste.cordonnier@epfl.ch;martin.jaggi@epfl.ch,Optimization/Convex Optimization*; Deep Learning/Efficient Training Methods; Deep Learning/Optimization for Deep Networks,Poster
2189,Exponentiated Strongly Rayleigh Distributions,"  Strongly Rayleigh (SR) measures are discrete probability distributions over the subsets of a ground set. They enjoy strong negative dependence properties, as a result of which they assign higher probability to subsets of diverse elements. We introduce in this paper Exponentiated Strongly Rayleigh (ESR) measures, which sharpen (or smoothen) the negative dependence property of SR measures via a single parameter (the exponent) that can intuitively understood as an inverse temperature. We develop efficient MCMC procedures for approximate sampling from ESRs, and obtain explicit mixing time bounds for two concrete instances: exponentiated versions of Determinantal Point Processes and Dual Volume Sampling. We illustrate some of the potential of ESRs, by applying them to a few machine learning tasks; empirical results confirm that beyond their theoretical appeal, ESR-based models hold significant promise for these tasks.","Zelda Mariet*,Massachusetts Institute of Technology;Suvrit Sra,Massachusetts Institute of Technology, USA;Stefanie  Jegelka,MIT",zelda@csail.mit.edu;suvrit@mit.edu;stefje@csail.mit.edu,Probabilistic Methods/Graphical Models*; Probabilistic Methods/MCMC,Poster
2196,Importance Weighting and Varational Inference,"Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.","Justin Domke*,UMass Amherst;Daniel Sheldon,University of Massachusetts, Amherst",domke@cs.umass.edu;sheldon@cs.umass.edu,Probabilistic Methods/Variational Inference,Poster
2198,Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis,"We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.","Ye Jia*,Google;Yu Zhang,;Ron Weiss,Google, Inc.;Jonathan Shen,Google Brain;Yonghui Wu,Google;Zhifeng Chen,Google",jiayephy@gmail.com;ngyuzh@google.com;ronw@google.com;jonathanasdf@google.com;yonghui@google.com;zhifengc@google.com,Applications/Audio and Speech Processing*; Algorithms/Multitask and Transfer Learning; Deep Learning/Few-Shot Learning Approaches,Poster
2202,Expanding Holographic Embeddings for Knowledge Completion,"Neural models operating over structured spaces such as knowledge graphs require a continuous embedding of the discrete elements of this space (such as entities) as well as the relationships between them. Relational embeddings with high expressivity, however, have high model complexity, making them computationally difficult to train. We propose a new family of embeddings for knowledge graphs that interpolate between a method with high model complexity and one, namely Holographic embeddings, with low dimensionality and high training efficiency. This interpolation, termed HolEx, is achieved by concatenating several linearly perturbed copies of the original holographic embedding. We formally characterize the number of perturbed copies to provably recover the full relational interaction matrix between entities, leveraging ideas from Haar wavelets and compressed sensing. In practice, we find that using just a handful of perturbation vectors results in a much stronger knowledge completion system. On the Freebase FB15K dataset, HolEx outperforms original holographic embeddings by 13.7% on the HITS@10 metric, and the current state-of-the-art by 3.1% (absolute).","Yexiang Xue*,Purdue University;Yang Yuan,Cornell University;Zhitian Xu,Shanghai Jiaotong University;Ashish Sabharwal,AI2",yexiang@purdue.edu;yangyuan@cs.cornell.edu;voidxzt@sjtu.edu.cn;ashishs@allenai.org,Deep Learning/Embedding Approaches*; Algorithms/Relational Learning; Applications/Matrix and Tensor Factorization,Poster
2203,Lifelong Inverse Reinforcement Learning,"Methods for learning from demonstration (LfD) have shown success in acquiring behavior policies by imitating a user. However, even for a single task, LfD may require numerous demonstrations. For versatile agents that must learn many tasks via demonstration, this process would substantially burden the user if each task were learned in isolation. To address this challenge, we introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required.  As one solution to this problem, we propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance. ","Jorge Mendez*,University of Pennsylvania;Shashank Shivkumar,University of Pennsylvania;Eric Eaton,University of Pennsylvania",mendezme@seas.upenn.edu;shashs@seas.upenn.edu;eeaton@cis.upenn.edu,Algorithms/Multitask and Transfer Learning*; Reinforcement Learning and Planning/Reinforcement Learning,Poster
2204,Explaining Deep Learning Models -- A Bayesian Non-parametric Approach,"Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models. ","Wenbo Guo*,Pennsylvania State University;Sui Huang,Netflix Inc.;Yunzhe Tao,Columbia University;Xinyu Xing,The Pennsylvania State University;Lin Lin,The Pennsylvania State University",wzg13@ist.psu.edu;shuang@netflix.com;y.tao@columbia.edu;xxing@ist.psu.edu;llin@psu.edu,"Applications/Privacy, Anonymity, and Security*; Deep Learning/Visualization or Exposition Techniques for Deep Networks",Poster
2206,Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima,"We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems, by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently. More specifically, the proposed algorithm only needs $\tilde{O}(\epsilon^{-10/3})$ stochastic gradient evaluations to converge to an approximate local minimum $\mathbf{x}$, which satisfies $\|\nabla f(\mathbf{x})\|_2\leq\epsilon$ and $\lambda_{\min}(\nabla^2 f(\mathbf{x}))\geq -\sqrt{\epsilon}$ in the general stochastic optimization setting, where $\tilde{O}(\cdot)$ hides logarithm polynomial terms and constants. This improves upon the $\tilde{O}(\epsilon^{-7/2})$ gradient complexity achieved by the state-of-the-art stochastic local minima finding algorithms by a factor of $\tilde{O}(\epsilon^{-1/6})$. For the nonconvex finite-sum optimization, our algorithm also outperforms the best known algorithms in a certain regime. ","Yaodong Yu,University of Virginia;Pan Xu,UCLA;Quanquan Gu*,UCLA",yy8ms@virginia.edu;px3ds@virginia.edu;qg5w@virginia.edu,Optimization/Non-Convex Optimization,Poster
2212,COLA: Decentralized Linear Learning,"Decentralized machine learning is a promising emerging technique in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run on-device, on an arbitrary communication network, without a central coordinator.
We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our scheme overcomes many limitations of existing methods in the distributed setting, and achieves communication efficiency, scalability, as well as elasticity and resilience to changes in user's data and participating devices.","Lie He,EPFL;An Bian,ETH Zurich;Martin Jaggi*,EPFL",lie.he@epfl.ch;ybian@inf.ethz.ch;martin.jaggi@epfl.ch,Optimization/Convex Optimization*; Applications/Hardware and Systems,Poster
2222,MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare,"Deep learning models exhibit state-of-the-art performance for many predictive healthcare tasks using electronic health records (EHR) data, but these models typically require training data volume that exceeds the capacity of most healthcare systems.
External resources such as medical ontologies are used to bridge the data volume constraint, but this approach is often not directly applicable or useful because of inconsistencies with terminology.
To solve the data insufficiency challenge, we leverage the inherent multilevel structure of EHR data and, in particular, the encoded relationships among medical codes.
We propose Multilevel Medical Embedding (MiME) which learns the multilevel embedding of EHR data while jointly performing auxiliary prediction tasks that rely on this inherent EHR structure without the need for external labels. 
We conducted two prediction tasks, heart failure prediction and sequential disease prediction, where MiME outperformed baseline methods in diverse evaluation settings.
In particular, MiME consistently outperformed all baselines when predicting heart failure on datasets of different volumes, especially demonstrating the greatest performance improvement (15% relative gain in PR-AUC over the best baseline) on the smallest dataset, demonstrating its ability to effectively model the multilevel structure of EHR data.","Edward Choi*,Georgia Institute of Technology;Cao Xiao,IBM Research ;Walter  Stewart,No Affiliation;Jimeng Sun,CS",mp2893@gatech.edu;cxiao@us.ibm.com;wfs502000@yahoo.com;jsun@cc.gatech.edu,Applications*; Algorithms/Representation Learning; Deep Learning; Deep Learning/Predictive Models,Poster
2226,Adaptive Sampling Towards Fast Graph Representation Learning,"Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections.
Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.","Wenbing Huang,Tencent AI Lab;Tong  Zhang*,Australian National University;Yu Rong,Tencent AI Lab;Junzhou Huang,University of Texas at Arlington",hwenbing@126.com;tong.zhang@anu.edu.au;yu.rong@hotmail.com;jzhuang@uta.edu,Deep Learning*; Deep Learning/CNN Architectures,Poster
2229,Hunting for Discriminatory Proxies in Linear Regression Models,"A machine learning model may exhibit discrimination when used to make decisions involving people. One potential cause for such outcomes is that the model uses a statistical proxy for a protected demographic attribute. In this paper we formulate a definition of proxy use for the setting of linear regression and present algorithms for detecting proxies. Our definition follows recent work on proxies in classification models, and characterizes a model's constituent behavior that: 1) correlates closely with a protected random variable, and 2) is causally influential in the overall behavior of the model. We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program, and further extend this result to account for situations where the use of a certain input variable is justified as a ``business necessity''. Finally, we present empirical results on two law enforcement datasets that exhibit varying degrees of racial disparity in prediction outcomes, demonstrating that proxies shed useful light on the causes of discriminatory behavior in models.","Samuel Yeom*,Carnegie Mellon University;Anupam Datta,Carnegie Mellon University;Matt Fredrikson,Carnegie Mellon University",syeom@cs.cmu.edu;danupam@cmu.edu;mfredrik@cs.cmu.edu,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Regression",Poster
2233,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be vulnerable to the maliciously generated adversarial examples. In this paper, we present a novel training procedure and a thresholding test strategy, towards robust detection of adversarial examples. In training, we propose to minimize the reverse cross-entropy (RCE), which encourages a deep network to learn latent representations that better distinguish adversarial examples from normal ones. In testing, we propose to use a thresholding strategy as the detector to filter out adversarial examples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization. We apply our method to defend various attacking methods on the widely used MNIST and CIFAR-10 datasets, and achieve significant improvements on robust predictions under all the threat models in the adversarial setting.","Tianyu Pang*,Tsinghua University;Chao Du,Tsinghua University;Yinpeng Dong,Tsinghua University;Jun Zhu,Tsinghua University",pty17@mails.tsinghua.edu.cn;du-c14@mails.tsinghua.edu.cn;dyp17@mails.tsinghua.edu.cn;dcszj@mail.tsinghua.edu.cn,"Deep Learning/Efficient Training Methods*; Algorithms/Classification; Applications/Privacy, Anonymity, and Security",Spotlight
2234,Active Matting,"Image matting is an ill-posed problem. It requires a user input trimap or some  strokes to obtain an alpha matte of the foreground object. A fine user input is essential to obtain a good result, which is either time consuming or suitable for experienced users who know where to place the strokes. In this paper, we explore the intrinsic relationship between the user input and the matting algorithm to address the problem of where and when the user should provide the input. Our aim is to discover the most informative sequence of regions for user input in order to produce a good alpha matte with minimum labeling efforts. To this end, we propose an active matting method with recurrent reinforcement learning. The proposed framework involves human in the loop by sequentially detecting informative regions for trivial human judgement. Comparing to traditional matting algorithms, the proposed framework requires much less efforts, and can produce satisfactory results with just 10 regions. Through extensive experiments, we show that the proposed model reduces user efforts significantly and achieves comparable performance to dense trimaps in a user-friendly manner. We further show that the learned informative knowledge can be generalized across different matting algorithms.","Xin Yang,Dalian University of Technology;Ke Xu,Dalian University of Technology;City University of Hong Kong;Shaozhe Chen,Dalian University of Technology;Shengfeng He*,South China University of Technology;Baocai Yin Yin,Dalian University of Technology;Rynson Lau,City University of Hong Kong",xinyang@dlut.edu.cn;kkangwing@mail.dlut.edu.cn;csz@mail.dlut.edu.cn;shengfenghe7@gmail.com;ybc@dlut.edu.cn;rynson.lau@cityu.edu.hk,Applications/Computer Vision*; Applications/Image Segmentation; Deep Learning/Attention Models,Poster
2242,Learning filter widths of spectral decompositions with wavelets,"Time series classification using deep neural networks, such as convolutional neural networks (CNN), operate on the spectral decomposition of the time series computed using a preprocessing step. This step can include a large number of hyperparameters, such as window length, filter widths, and filter shapes, each with a range of possible values that must be chosen using time and data intensive cross-validation procedures. We propose the wavelet deconvolution (WD) layer as an efficient alternative to this preprocessing step that eliminates a significant number of hyperparameters. The WD layer uses wavelet functions with adjustable scale parameters to learn the spectral decomposition directly from the signal. Using backpropagation, we show the scale parameters can be optimized with gradient descent. Furthermore, the WD layer adds interpretability to the learned time series classifier by exploiting the properties of the wavelet transform. In our experiments, we show that the WD layer can automatically extract the frequency content used to generate a dataset. The WD layer combined with a CNN applied to the phone recognition task on the TIMIT database achieves a phone error rate of 18.1\%, a relative improvement of 4\% over the baseline CNN. Experiments on a dataset where engineered features are not available showed WD+CNN is the best performing method. Our results show that the WD layer can improve neural network based time series classifiers both in accuracy and interpretability by learning directly from the input signal.","Haidar Khan*,Rensselaer Polytechnic Institute;B√ºlent Yener,Rensselaer Polytechnic Institute",khanh2@rpi.edu;yener@rpi.edu,Applications/Signal Processing*; Algorithms/Classification; Algorithms/Representation Learning; Applications/Speech Recognit,Poster
2250,Optimal Byzantine-Resilient Stochastic Gradient Descent,"(this is a theory paper)

This paper studies the problem of distributed stochastic optimization in an adversarial setting where, out of $m$ machines which allegedly compute stochastic gradients every iteration, an $\alpha$-fraction are Byzantine, and can behave arbitrarily and adversarially. Our main result is a variant of stochastic gradient descent (SGD) which finds $\varepsilon$-approximate minimizers of convex functions in $T = \tilde{O}\big( \frac{1}{\varepsilon^2 m} + \frac{\alpha^2}{\varepsilon^2} \big)$ iterations. In contrast, traditional mini-batch SGD needs $T = O\big( \frac{1}{\varepsilon^2 m} \big)$ iterations, but cannot tolerate Byzantine failures. Further, we provide a lower bound showing that, up to logarithmic factors, our algorithm is information-theoretically optimal both in terms of sample complexity and time complexity.","Dan Alistarh,IST Austria & NeuralMagic;Zeyuan Allen-Zhu*,Microsoft Research AI;Jerry Li,MIT",dan.alistarh@ist.ac.at;zeyuan@csail.mit.edu;jerryzli@mit.edu,Algorithms/Online Learning*; Theory/Learning Theory,Poster
2253,PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits,"We address the problem of regret minimization in logistic contextual bandits, where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards. Using a fast inference procedure with Polya-Gamma distributed augmentation variables, we propose an improved version of Thompson Sampling, a Bayesian formulation of contextual bandits with near-optimal performance. Our approach, Polya-Gamma augmented Thompson Sampling (PG-TS), achieves state-of-the-art performance on simulated and real data. PG-TS explores the action space efficiently and exploits high-reward arms, quickly converging to solutions of low regret. Its explicit estimation of the posterior distribution of the context feature covariance leads to substantial empirical gains over approximate approaches. PG-TS is the first approach to demonstrate the benefits of Polya-Gamma augmentation in bandits and to propose an efficient Gibbs sampler for approximating the analytically unsolvable integral of logistic contextual bandits.","Bianca  Dumitrascu*,Princeton  University;Karen Feng,Princeton University;Barbara Engelhardt,Princeton University",biancad@princeton.edu;karenfeng.us@gmail.com;bee@cs.princeton.edu,Algorithms/Bandit Algorithms*; Algorithms/Regression; Applications/Recommender Systems,Poster
2257,Spectral Filtering for General Linear Dynamical Systems,"We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.","Elad Hazan,Princeton University and Google Brain;Holden Lee,Princeton University;Karan Singh,Princeton University;Cyril Zhang*,Princeton University;Yi Zhang,Princeton",ehazan@cs.princeton.edu;holdenl@princeton.edu;karans@princeton.edu;cyril.zhang@princeton.edu;yz7@cs.princeton.edu,Algorithms/Online Learning*; Theory/Control Theory,Oral
2269,On Learning Intrinsic Rewards for Policy Gradient Methods,"In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem, or close variants thereof, have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of Singh et al. that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. Our results show improved performance on most but not all of the domains.","Zeyu Zheng*,University of Michigan;Junhyuk Oh,U Michigan;Satinder Singh,UMich",zeyu@umich.edu;junhyuk@umich.edu;baveja@umich.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning,Poster
2272,Boolean Decision Rules via Column Generation,"This paper considers the learning of Boolean rules in either disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) or conjunctive normal form (CNF, AND-of-ORs) as an interpretable model for classification.  An integer program is formulated to optimally trade classification accuracy for rule simplicity.  Column generation (CG) is used to efficiently search over an exponential number of candidate clauses (conjunctions or disjunctions) without the need for heuristic rule mining.  This approach also bounds the gap between the selected rule set and the best possible rule set on the training data. To handle large datasets, we propose an approximate CG algorithm using randomization.  Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 datasets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate.","Sanjeeb Dash,IBM Research;Oktay Gunluk,IBM Research;Dennis Wei*,IBM Research",sanjeebd@us.ibm.com;gunluk@us.ibm.com;dwei@us.ibm.com,"Optimization/Combinatorial Optimization*; Algorithms/Classification; Applications/Fairness, Accountability, and Transparency",Spotlight
2273,Adversarial Text Generation via Feature-Mover's Distance,"Generative adversarial networks (GANs) have achieved significant success in generating real-valued data. However, the discrete nature of text hinders the application of GAN to text-generation tasks. Instead of using the standard GAN objective, we propose to improve text-generation GAN via a novel approach inspired by optimal transport. Specifically, we consider matching the latent feature distributions of real and synthetic sentences using a novel metric, termed the feature-mover's distance (FMD). This formulation leads to a highly discriminative critic and easy-to-optimize objective, overcoming the mode-collapsing and brittle-training problems in existing methods. Extensive experiments are conducted on a variety of tasks to evaluate the proposed model empirically, including unconditional text generation, style transfer from non-parallel text, and unsupervised cipher cracking. The proposed model yields superior performance, demonstrating wide applicability and effectiveness.","Liqun Chen*,Duke University;Shuyang Dai,Duke University;Chenyang Tao,Duke University;Haichao Zhang,Baidu Research;Zhe Gan,Microsoft;Dinghan Shen,Duke University;Yizhe Zhang,Microsoft;Guoyin Wang,Duke University;Ruiyi Zhang,Duke University;Lawrence Carin Duke,CS",liqun.chen@duke.edu;sd301@duke.edu;chenyang.tao@duke.edu;hczhang1@gmail.com;zhe.gan@microsoft.com;dinghan.shen@duke.edu;yizhe.zhang@microsoft.com;guoyin.wang@duke.edu;rz68@duke.edu;lcarin@duke.edu,Applications/Natural Language Processing*; Deep Learning/Generative Models,Poster
2283,Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions,"Error bound conditions (EBC) are properties that characterize the growth of an objective function when a point is moved away from the optimal set. They have  recently received increasing attention in the field  of optimization for developing optimization algorithms with fast convergence.  However,  the studies of EBC in statistical learning are hitherto still limited.  The main contributions of this paper are two-fold. First,  we develop fast and intermediate rates of  empirical risk minimization (ERM) under EBC for risk minimization with Lipschitz continuous, and  smooth  convex random functions. Second, we establish fast and intermediate rates of an efficient stochastic approximation (SA) algorithm for risk minimization  with Lipschitz continuous random functions, which requires only one pass of $n$ samples and adapts to EBC. For both approaches, the convergence rates span a full spectrum between $\widetilde O(1/\sqrt{n})$ and $\widetilde O(1/n)$ depending on the power constant in EBC, and could be even faster than $O(1/n)$ in special cases for ERM. Moreover, these  convergence rates are automatically adaptive without using any knowledge of EBC. Overall, this work not only strengthens the understanding of ERM for statistical learning but also brings new fast stochastic algorithms for solving a broad range of statistical learning problems. ","Mingrui Liu,The University of Iowa;Xiaoxuan Zhang,University of Iowa;Lijun Zhang,Nanjing University;rong jin,alibaba group;Tianbao Yang*,University of Iowa",mingrui-liu@uiowa.edu;xiaoxuan-zhang@uiowa.edu;zljzju@gmail.com;jinrong.jr@alibaba-inc.com;tianbao-yang@uiowa.edu,Theory/Learning Theory*; Optimization/Convex Optimization,Poster
2284,Learning Bounds for Greedy Approximation with Explicit Feature Maps from Multiple Kernels,"Nonlinear kernels can be approximated using finite-dimensional feature maps for efficient risk minimization. Due to the inherent trade-off between the dimension of the (mapped) feature space and the approximation accuracy, the key problem is to identify promising (explicit) features leading to a satisfactory out-of-sample performance. In this work, we tackle this problem by efficiently choosing such features from multiple kernels in a greedy fashion. Our method sequentially selects these explicit features from a set of candidate features using a correlation metric. We establish an out-of-sample error bound capturing the trade-off between the error in terms of explicit features (approximation error) and the error due to spectral properties of the best model in the Hilbert space associated to the combined kernel (spectral error). The result verifies that when the (best) underlying data model is sparse enough, i.e., the spectral error is negligible, one can control the test error with a small number of explicit features, that can scale poly-logarithmically with data. Our empirical results show that given a fixed number of explicit features, the method can achieve a lower test error with a smaller time cost, compared to the state-of-the-art in data-dependent random features. ","Shahin Shahrampour*,Harvard University;Vahid Tarokh,Duke University",shahin@seas.harvard.edu;vahid.tarokh@duke.edu,Algorithms/Kernel Methods*; Algorithms/Classification,Poster
2286,A Mathematical Model For Optimal Decisions In A Representative Democracy ,"  Direct democracy is a special
  case of an ensemble of classifiers,
  where every person (classifier) votes. This fails when the
  average voter competence (classifier accuracy) falls below 50\%, which 
  happens in
  noisy settings where voters have limited information.
  Representative democracy, where voters choose representatives to vote,
  is a specific way to improve the ensemble of classifiers.
  We introduce a mathematical model for
  studying representative democracy, in particular understanding the parameters
  of a representative democracy that gives maximum decision
  making capability. Our main result states that under general and natural
  conditions,
  \begin{enumerate}\itemsep-1pt
    \vspace*{-4pt}
  \item For fixed voting cost, the optimal number of representatives is
    \emph{linear}.
  \item For polynomial cost,
    the optimal number of representatives is
    \emph{logarithmic}.
  \end{enumerate}
  This work sets the mathematical
  foundation for studying the quality-quantity tradeoff
  in a representative democracy-type ensemble
  (fewer highly qualified representatives
  versus more less qualified representatives). ","Malik Magdon-Ismail*,RPI;Lirong Xia,RPI",magdon@gmail.com;xial@cs.rpi.edu,Theory/Game Theory and Computational Economics*; Applications/Computational Social Science,Poster
2287,Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making,"It is commonly believed that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a Pareto optimal policy, i.e., a policy that cannot be improved upon for one principal without making sacrifices for another. Harsanyi's theorem shows that when the principals have a common prior on the outcome distributions of all policies a Pareto optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals‚Äô utilities. In this paper, we derive a more precise generalization for the sequential decision setting in the case of principals with different priors on the dynamics of the environment. We refer to this generalization as the Negotiable Reinforcement Learning (NRL) framework. In this more general case, the relative weight given to each principal‚Äôs utility should evolve over time according to how well the agent‚Äôs observations conform with that principal‚Äôs prior. To gain insight into the dynamics of this new framework, we implement a simple NRL agent and empirically examine its behavior in a simple environment.","Nishant Desai*,UC Berkeley;Andrew Critch,UC Berkeley;Stuart Russell,UC Berkeley",nishantdesai@berkeley.edu;critch@berkeley.edu;russell@cs.berkeley.edu,Reinforcement Learning and Planning/Markov Decision Processes*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
2294,Non-metric Similarity Graphs for Maximum Inner Product Search,"In this paper we address the problem of Maximum Inner Product Search (MIPS) that is currently the computational bottleneck in a large number of machine learning applications. 
While being similar to the nearest neighbor search (NNS), the MIPS problem was shown to be more challenging, as the inner product is not a proper metric function. We propose to solve the MIPS problem with the usage of similarity graphs, i.e., graphs where each vertex is connected to the vertices that are the most similar in terms of some similarity function. Originally, the framework of similarity graphs was proposed for metric spaces and in this paper we naturally extend it to the non-metric MIPS scenario. We demonstrate that, unlike existing approaches, similarity graphs do not require any data transformation to reduce MIPS to the NNS problem and should be used for the original data. Moreover, we explain why such a reduction is detrimental for similarity graphs. By an extensive comparison to the existing approaches, we show that the proposed method is a game-changer in terms of the runtime/accuracy trade-off for the MIPS problem.","Stanislav Morozov*,Yandex;Artem Babenko,MIPT/Yandex",stanis-morozov@yandex.ru;artem.babenko@phystech.edu,Applications/Recommender Systems*; Algorithms,Poster
2295,Recurrently Controlled Recurrent Networks,"Recurrent neural networks (RNNs) such as long short-term memory and gated recurrent units are pivotal building blocks across a broad spectrum of sequence modeling problems. This paper proposes a recurrently controlled recurrent network (RCRN) for expressive and powerful sequence encoding. More concretely, the key idea behind our approach is to learn the recurrent gating functions using recurrent networks. Our architecture is split into two components - a controller cell and a listener cell whereby the recurrent controller actively influences the compositionality of the listener cell. We conduct extensive experiments on a myriad of tasks in the NLP domain such as sentiment analysis (SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment classification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading comprehension (NarrativeQA). Across all 26 datasets, our results demonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs, suggesting that our controller architecture might be a suitable replacement for the widely adopted stacked architecture. Additionally, RCRN achieves state-of-the-art results on several well-established datasets.","Yi Tay*,Nanyang Technological University;Anh Tuan Luu,Institute for Infocomm Research;Siu Cheung Hui,Nanyang Technological University",ytay017@e.ntu.edu.sg;at.luu@i2r.a-star.edu.sg;asschui@ntu.edu.sg,Deep Learning/Recurrent Networks*; Applications/Natural Language Processing; Applications/Text Analysis,Poster
2297,Fast greedy algorithms for dictionary selection with generalized sparsity constraints,"In dictionary selection, several atoms are selected from finite candidates that successfully approximate given data points in the sparse representation. We propose a novel efficient greedy algorithm for dictionary selection. Not only does our algorithm work much faster than the known methods, but it can also handle more complex sparsity constraints, such as average sparsity. Using numerical experiments, we show that our algorithm outperforms the known methods for dictionary selection, achieving competitive performances with dictionary learning algorithms in a smaller running time.","Kaito Fujii*,university of Tokyo;Tasuku Soma,University of Tokyo",kaito_fujii@mist.i.u-tokyo.ac.jp;tasuku_soma@mist.i.u-tokyo.ac.jp,Optimization/Submodular Optimization*; Optimization/Combinatorial Optimization,Spotlight
2298,Data-Efficient Model-based Reinforcement Learning with Deep Probabilistic Dynamics Models,"Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance, especially those with high-capacity parametric function
approximators, such as deep networks. In this paper, we study how to bridge this
gap, by employing uncertainty-aware dynamics models, proposing a new algorithm called probabilistic ensembles with trajectory sampling (PETS). We justify PETS with an empirical analysis of design decisions for both uncertainty-aware dynamics models and uncertainty propagation methods for planning. Our experimental comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples.","Kurtland Chua,UC Berkeley;Roberto Calandra*,UC Berkeley;Rowan McAllister,UC Berkeley;Sergey Levine,UC Berkeley",kchua@berkeley.edu;roberto.calandra@berkeley.edu;rmcallister@berkeley.edu;svlevine@eecs.berkeley.edu,Reinforcement Learning and Planning/Model-Based RL,Spotlight
2307,A Smoother Way to Train Structured Prediction Models,"We present a framework allowing one to perform smoothing on the inference 
used by structured prediction methods. Smoothing breaks the non-smoothness
inherent to structured prediction objectives, without the need to resort to convex duality, 
and paves the way to the use of fast primal gradient-based optimization algorithms. 

We illustrate the proposed framework by developing an novel primal incremental 
gradient-based optimization algorithm for the structural support vector machine. 
The algorithm blends an extrapolation scheme for acceleration and an adaptive smoothing scheme
for gradient-based optimization. We establish its worse-case complexity bounds. 

We present experiment results on two real-world problems, 
namely named entity recognition and visual object localization. Experimental results
show that the proposed framework allows one to develop competitive primal optimization 
algorithms for structured prediction efficiently leveraging inference routines. ","Krishna Pillutla*,University of Washington;Vincent Roulet,UW;Sham Kakade,University of Washington;Zaid Harchaoui,University of Washington",pillutla@cs.washington.edu;vroulet@uw.edu;sham@cs.washington.edu;zaid@uw.edu,Algorithms/Structured Prediction*; Optimization/Convex Optimization,Poster
2309,Context-dependent upper-confidence bounds for directed exploration,"  Directed exploration strategies for reinforcement learning are critical for learning an optimal policy in a minimal number of interactions with the environment. Many algorithms use optimism to direct exploration, either through visitation estimates or upper confidence bounds, as opposed to data-inefficient strategies like $\epsilon$-greedy that use random, undirected exploration. Most such data-efficient exploration methods, however, require significant computation, typically relying on a learned model to guide exploration. Least-squares methods have the potential to provide some of the data-efficiency benefits of model-based approaches---because they summarize past interactions---with the computation closer to that of model-free approaches.  In this work, we provide a novel computationally efficient, incremental exploration strategy, leveraging this property of least-squares temporal difference learning (LSTD). We derive upper confidence bounds on the action-values learned by LSTD, with context-dependent (or state-dependent) noise variance. Such context-dependent noise focuses exploration on a subset of variable states, and allows for reduced exploration in other states. We empirically demonstrate that our algorithm can converge more quickly than other incremental exploration strategies using confidence estimates on action-values.","Raksha Kumaraswamy,University of Alberta;Matthew Schlegel,University of Alberta;Adam White,DeepMind;Martha White*,University of Alberta",kumarasw@ualberta.ca;mkschleg@ualberta.ca;adamwhite@google.com;whitem@ualberta.ca,Reinforcement Learning and Planning/Exploration*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
2329,A Unified View of Piecewise Linear Neural Network Verification,"  The success of Deep Learning and its potential use in many safety-critical
  applications has motivated research on formal verification of Neural Network
  (NN) models. Despite the reputation of learned NN models to behave as black
  boxes and the theoretical hardness of proving their properties, researchers
  have been successful in verifying some classes of models by exploiting their
  piecewise linear structure and taking insights from formal methods such as
  Satisifiability Modulo Theory. These methods are however still far from
  scaling to realistic neural networks. To facilitate progress on this crucial
  area, we make two key contributions. First, we present a unified framework
  that encompasses previous methods. This analysis results in the identification
  of new methods that combine the strengths of multiple existing approaches,
  accomplishing a speedup of two orders of magnitude compared to the previous
  state of the art. Second, we propose a new data set of benchmarks which
  includes a collection of previously released testcases. We use the benchmark
  to provide the first experimental comparison of existing algorithms and
  identify the factors impacting the hardness of verification problems.","Rudy Bunel*,University of Oxford;Ilker Turkaslan,University of Oxford;Philip Torr,University of Oxford;Pushmeet Kohli,DeepMind;M. Pawan Kumar,University of Oxford",rudy@robots.ox.ac.uk;ilker.turkaslan@lmh.ox.ac.uk;philip.torr@eng.ox.ac.uk;pushmeet@google.com;pawan@robots.ox.ac.uk,"Algorithms*; Applications/Privacy, Anonymity, and Security; Data, Competitions, Implementations, and Software",Poster
2332,Hierarchical Graph Representation Learning with Differentiable Pooling,"Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets. ","Rex Ying*,Stanford University;Jiaxuan You,Stanford University;Christopher Morris,TU Dortmund University;Xiang Ren,University of Southern California;William Hamilton,Stanford University;Jure Leskovec,Stanford",rexying@stanford.edu;jiaxuan@stanford.edu;christopher.morris@tu-dortmund.de;xiangren@usc.edu;will.leif.hamilton@gmail.com;jure@cs.stanford.edu,Deep Learning/Supervised Deep Networks*; Algorithms/Representation Learning; Applications/Network Analysis,Spotlight
2334,Non-Ergodic Alternating Proximal  Augmented Lagrangian Algorithms with Optimal Rates,"We develop two new non-ergodic alternating proximal augmented Lagrangian algorithms (NEAPAL) to solve a class of nonsmooth constrained convex optimization problems. Our approach relies on a novel combination of the augmented Lagrangian framework,  alternating/linearization scheme, Nesterov's acceleration techniques, and adaptive strategy for parameters. Our algorithms have several new features compared to existing methods. Firstly, they have a Nesterov's acceleration step on the primal variables compared to the dual one in  several methods in the literature.
Secondly, they achieve non-ergodic optimal convergence rates under standard assumptions, i.e. an $\mathcal{O}\left(\frac{1}{k}\right)$ rate without any smoothness or strong convexity-type assumption, or an $\mathcal{O}\left(\frac{1}{k^2}\right)$ rate under only semi-strong convexity, where $k$ is the iteration counter. 
Thirdly, they preserve or have better per-iteration complexity compared to existing algorithms. Fourthly, they can be implemented in a parallel fashion.
Finally, all the parameters are adaptively updated without heuristic tuning.
We verify our algorithms on different numerical examples and compare them with some state-of-the-art methods.","Dinh Quoc Tran*,The University of North Carolina at Chapel Hill",quoctd@email.unc.edu,Optimization/Convex Optimization*; Theory/Computational Complexity,Poster
2336,Information-based Adaptive Stimulus Selection to Optimize Communication Efficiency in Brain-Computer Interfaces,"In stimulus-driven brain-computer interfaces (BCIs), the main hurdle that  limits the development of adaptive stimulus selection algorithms is the availability of objective functions that lead to tractable solutions to allow for algorithm implementation within the stringent time requirements of real-time BCI processing. In this work, we present a simple analytical solution of an information-based objective function by transforming the high-dimensional stimulus space into a one-dimensional space that parameterizes the objective function - the prior probability mass of the stimulus under consideration, irrespective of its contents. We demonstrate the utility of our adaptive stimulus selection algorithm in improving BCI performance with results from simulation and online human experiments.  ","Boyla Mainsah*,Duke University;Dmitry  Kalika,Johns Hopkins Applied Physics Laboratory;Leslie Collins,Duke University;Siyuan Liu,Duke University;Chandra  Throckmorton,Duke University",boyla.mainsah@duke.edu;dmitry.kalika@jhuapl.edu;leslie.collins@duke.edu;siyuan.liu@duke.edu;chandra.throckmorton@duke.edu,Neuroscience and Cognitive Science/Brain--Computer Interfaces and Neural Prostheses*; Optimization/Combinatorial Optimization,Poster
2338,Porcupine Neural Networks: Approximating Neural Network Landscapes,"Neural networks have been used prominently in several machine learning and statistics applications. In general, the underlying optimization of neural networks is non-convex which makes analyzing their performance challenging. In this paper, we take another approach to this problem by constraining the network such that the corresponding optimization landscape has good theoretical properties without significantly compromising performance. In particular, for two-layer neural networks we introduce Porcupine Neural Networks (PNNs) whose weight vectors are constrained to lie over a finite set of lines. We show that most local optima of PNN optimizations are global while we have a characterization of regions where bad local optimizers may exist. Moreover, our theoretical and empirical results suggest that an unconstrained neural network can be approximated using a polynomially-large PNN.","Soheil Feizi*,Stanford University;Hamid Javadi,Stanford University;Jesse Zhang,Stanford University;David Tse,Stanford University",sfeizi@stanford.edu;hrhakim@stanford.edu;jessez@stanford.edu;dntse@stanford.edu,Optimization/Non-Convex Optimization*; Theory/Spaces of Functions and Kernels,Poster
2341,Fairness Through Computationally-Bounded Awareness,"We study the problem of fair classification within the versatile framework of Dwork et al. [ITCS '12], which assumes the existence of a metric that measures similarity between pairs of individuals.  Unlike earlier work, we do not assume that the entire metric is known to the learning algorithm; instead, the learner can query this *arbitrary* metric a bounded number of times.  We propose a new notion of fairness called *metric multifairness* and show how to achieve this notion in our setting.
Metric multifairness is parameterized by a similarity metric d on pairs of individuals to classify and a rich collection C of (possibly overlapping) ""comparison sets"" over pairs of individuals.  At a high level, metric multifairness guarantees that *similar subpopulations are treated similarly*, as long as these subpopulations are identified within the class C.","Michael Kim*,Stanford University;Omer Reingold,Stanford University;Guy Rothblum,Weizmann Institute of Science",mpk@cs.stanford.edu;reingold@stanford.edu;rothblum@alum.mit.edu,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Classification; Algorithms/Metric Learning; Theory/Learning Theory",Poster
2344,Adaptive Negative Curvature Descent with Applications in Non-convex Optimization,"Negative curvature descent (NCD) method has been utilized to design deterministic or stochastic algorithms for non-convex optimization aiming at finding second-order stationary points or local minima. In existing studies, NCD needs to approximate the smallest eigen-value of the Hessian matrix with a sufficient precision (e.g., $\epsilon_2\ll 1$) in order to achieve a sufficiently accurate second-order stationary solution (i.e., $\lambda_{\min}(\nabla^2 f(\x))\geq -\epsilon_2)$.  One issue  with this approach is that the target precision $\epsilon_2$ is usually set to be very small in order to find a high quality solution, which increases the complexity for computing a negative curvature. To address this issue, we propose an adaptive NCD to allow for an adaptive error dependent on the current gradient's magnitude in approximating the smallest eigen-value of the Hessian, and to encourage competition between  a noisy NCD step and gradient descent step. We consider the applications of the proposed adaptive NCD for both deterministic and stochastic non-convex optimization, and demonstrate that it can help reduce the the overall complexity in computing the negative curvatures during the course of optimization without sacrificing the iteration complexity. ","Mingrui Liu*,The University of Iowa;Zhe Li,The University of Iowa ;Xiaoyu Wang,-;Jinfeng Yi,JD AI Research;Tianbao Yang,University of Iowa",mingrui-liu@uiowa.edu;zhe-li-1@uiowa.edu;fanghuaxue@gmail.com;jinfengyi.ustc@gmail.com;tianbao-yang@uiowa.edu,Optimization/Non-Convex Optimization*; Optimization,Poster
2366,Is Q-Learning Provably Efficient?,"Model-free reinforcement learning (RL) algorithms directly parameterize and update value functions or policies, bypassing the modeling of the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that they require large numbers of samples to learn.  The theoretical question of whether not model-free algorithms are in fact \emph{sample efficient} is one of the most fundamental questions in RL. The problem is unsolved even in the basic scenario with finitely many states and actions. We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret $\tlO(\sqrt{H^3 SAT})$ where $S$ and $A$ are the numbers of states and actions, $H$ is the number of steps per episode, and $T$ is the total number of steps. Our regret matches the optimal regret up to a single $\sqrt{H}$ factor.  Thus we establish the sample efficiency of a classical model-free approach. Moreover, to the best of our knowledge, this is the first model-free analysis to establish $\sqrt{T}$ regret \emph{without} requiring access to a ``simulator.''","Chi Jin*,UC Berkeley;Zeyuan Allen-Zhu,Microsoft Research AI;Sebastien Bubeck,Microsoft Research;Michael Jordan,UC Berkeley",chijin@cs.berkeley.edu;zeyuan@csail.mit.edu;sebubeck@microsoft.com;jordan@cs.berkeley.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Exploration,Poster
2367,"Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections","We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network's output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a first-order theorem can be proved efficiently by a solver using certain heuristics, and the final one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.","Xin Zhang*,MIT;Armando Solar-Lezama,MIT;Rishabh Singh,Google Brain",xzhang@csail.mit.edu;asolar@csail.mit.edu;rising@google.com,"Deep Learning/Visualization or Exposition Techniques for Deep Networks*; Applications/Fairness, Accountability, and Transparency",Poster
2372,Measures of distortion for machine learning,"Given data from a general metric space, one of the standard machine learning pipelines is to first embed the data into a Euclidean space and subsequently apply out of the box machine learning algorithms to analyze the data. The quality of such an embedding is typically described in terms of a distortion measure. In this paper, we show that many of the existing distortion measures behave in an undesired way, when considered from a machine learning point of view. We investigate desirable properties of distortion measures and formally prove that most of the existing measures fail to satisfy these properties. These theoretical findings are supported by simulations, which for example demonstrate that existing distortion measures are not robust to noise or outliers and cannot serve as good indicators for classification accuracy. As an alternative, we suggest a new measure of distortion, called $\sigma$-distortion. We can show both in theory and in experiments that it satisfies all desirable properties and is a better candidate to evaluate distortion in the context of machine learning. ","Leena Chennuru Vankadara*,Max Planck Institute for Intelligent Systems;Ulrike von Luxburg,University of T√ºbingen",leena.bitspilani@gmail.com;luxburg@informatik.uni-tuebingen.de,Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning*; Algorithms/Unsupervised Learning,Poster
2380,On the Local Minima of the Empirical Risk,"Population risk is always of primary interest in machine learning; however, learning algorithms only have access to the empirical risk. Even for applications with nonconvex non-smooth losses (such as modern deep networks), the population risk is generally significantly more well behaved from an optimization point of view than the empirical risk.  In particular, sampling can create many spurious local minima. We consider a general framework which aims to optimize a smooth nonconvex function $F$ (population risk) given only access to an approximation $f$ (empirical risk) that is pointwise close to $F$ (i.e., $\norm{F-f}_{\infty} \le \nu$). Our objective is to find the $\epsilon$-approximate local minima of the underlying function $F$ while avoiding the shallow local minima---arising because of the tolerance $\nu$---which exist only in $f$. We propose a simple algorithm based on stochastic gradient descent (SGD) on a smoothed version of $f$ that is guaranteed 
to achieve our goal as long as $\nu \le O(\epsilon^{1.5}/d)$. We also provide an almost matching lower bound showing that our algorithm achieves optimal error tolerance $\nu$ among all algorithms making a polynomial number of queries of $f$. As a concrete example, we show that our results can be directly used to give sample complexities for learning a ReLU unit.","Chi Jin,UC Berkeley;Lydia T. Liu*,University of California, Berk;Rong Ge,Duke University;Michael Jordan,UC Berkeley",chijin@cs.berkeley.edu;lydiatliu@berkeley.edu;rongge@cs.duke.edu;jordan@cs.berkeley.edu,Optimization/Non-Convex Optimization,Spotlight
2381,Densely Connected Attention Propagation for Reading Comprehension,"We propose DecaProp (Densely Connected Attention Propagation), a new densely connected neural architecture for reading comprehension (RC). There are two distinct characteristics of our model. Firstly, our model densely connects all pairwise layers of the network, modeling relationships between passage and query across all hierarchical levels. Secondly, the dense connectors in our network are learned via attention instead of standard residual skip-connectors. To this end, we propose novel Bidirectional Attention Connectors (BAC) for efficiently forging connections throughout the network. We conduct extensive experiments on four challenging RC benchmarks. Our proposed approach achieves state-of-the-art results on all four, outperforming existing baselines by up to 2.6%14.2% in absolute F1 score.","Yi Tay*,Nanyang Technological University;Anh Tuan Luu,Institute for Infocomm Research;Siu Cheung Hui,Nanyang Technological University;Jian Su,I2R, Singapore",ytay017@e.ntu.edu.sg;at.luu@i2r.a-star.edu.sg;asschui@ntu.edu.sg;sujian@i2r.a-star.edu.sg,Applications/Natural Language Processing*; Applications/Information Retrieval; Deep Learning/Attention Models,Poster
2384,Bandit Learning with Positive Externalities,"In many platforms, user arrivals exhibit a self-reinforcing behavior: future user arrivals are likely to have preferences similar to users who were satisfied in the past. In other words, arrivals exhibit {\em positive externalities}. We study multiarmed bandit (MAB) problems with positive externalities. We show that the self-reinforcing preferences may lead standard benchmark algorithms such as UCB to exhibit linear regret. We develop a new algorithm, Balanced Exploration (BE), which explores arms carefully to avoid suboptimal convergence of arrivals before sufficient evidence is gathered. We also introduce an adaptive variant of BE which successively eliminates suboptimal arms. We analyze their asymptotic regret, and establish optimality by showing that no algorithm can perform better.","Virag Shah*,Stanford University;Jose Blanchet,Stanford University;Ramesh  Johari,Stanford University",virag@stanford.edu;jblanche@stanford.edu;rjohari@stanford.edu,Algorithms/Bandit Algorithms*; Algorithms/Online Learning,Poster
2387,Learning Confidence Sets using Support Vector Machines,"The goal of confidence-set learning in the binary classification setting is to construct two sets, each with a specific probability guarantee to cover a class. An observation outside the overlap of the two sets is deemed to be from one of the two classes, while the overlap is an ambiguity region which could belong to either class. Instead of plug-in approaches, we propose a support vector classifier to construct confidence sets in a flexible manner. Theoretically, we show that the proposed learner can control the non-coverage rates and minimize the ambiguity with high probability. Efficient algorithms are developed and numerical studies illustrate the effectiveness of the proposed method.","Wenbo Wang*,Binghamton University;Xingye Qiao,Binghamton University",wwang75@binghamton.edu;qiao@math.binghamton.edu,Algorithms/Large Margin Methods*; Algorithms/Classification; Theory/Learning Theory,Poster
2388,Efficient Neural Network Robustness Certification with General Activation Functions,"Finding minimum distortion of adversarial examples and thus certifying robustness in neural networks classifiers is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for \textit{general} activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to the four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by \textit{adaptively} selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan. To the best of our knowledge, CROWN is the first framework that can efficiently certify non-trivial robustness for general activation functions in neural networks.","Huan Zhang*,UCLA;Lily Weng,MIT;Pin-Yu Chen,IBM Research;Cho-Jui Hsieh,UCLA, Google Research;Luca Daniel,MIT",huanzhang@ucla.edu;twweng@mit.edu;pin-yu.chen@ibm.com;chojui.hsieh@gmail.com;dluca@mit.edu,"Applications/Privacy, Anonymity, and Security*; Deep Learning",Poster
2391,Hessian-based Analysis of Large Batch Training and Robustness to Adversaries,"Large batch size training of Neural Networks has been shown to incur accuracy
loss when trained with the current methods.  The exact underlying reasons for
this are still not completely understood.  Here, we study large batch size
training through the lens of the Hessian operator and robust optimization. In
particular, we perform a Hessian based study to analyze exactly how the landscape of
the loss function changes when training with large batch size. We compute the
true Hessian spectrum, without approximation, by back-propagating the second
derivative. Extensive experiments on multiple networks show that saddle-points are
not the cause for generalization gap of large batch size training, and the results
consistently show that large batch converges to points with noticeably higher Hessian
spectrum. Furthermore, we show that robust training allows one to favor flat areas,
as points with large Hessian spectrum show poor robustness to adversarial perturbation.
We further study this relationship, and provide empirical and theoretical proof that the inner loop for robust training is a saddle-free optimization problem \textit{almost everywhere}.
We present detailed experiments with five
different network architectures, including a residual network, tested on MNIST, CIFAR-10, and CIFAR-100
datasets.","Zhewei Yao*,University of California, Berkeley;Amir Gholami,UC Berkeley;Kurt Keutzer,EECS, UC Berkeley;Michael Mahoney,""University of California, Berkeley"";Qi Lei,UT Austin",zheweiy@berkeley.edu;amirgh@berkeley.edu;keutzer@berkeley.edu;mmahoney@stat.berkeley.edu;leiqi@ices.utexas.edu,Deep Learning/Adversarial Networks*; Deep Learning/Supervised Deep Networks,Poster
2394,Neural Edit Operations for Biological Sequences," The evolution of biological sequences, such as proteins or DNAs, is driven by the three basic edit operations: substitution, insertion, and deletion. Motivated by the recent progress of neural network models for biological tasks, we implement two neural network architectures that can treat such edit operations. The first proposal is the edit invariant neural networks, based on differentiable Needleman-Wunsch algorithms. The second is the use of deep CNNs with concatenations. Our analysis shows that CNNs can recognize star-free regular expressions, and that deeper CNNs can recognize more complex regular expressions including the insertion/deletion of characters. The experimental results for the protein secondary structure prediction task suggest the importance of insertion/deletion. The test accuracy on the widely-used CB513 dataset is 71.5%, which is 1.2-points better than the current best result on non-ensemble models.","Satoshi Koide*,Toyota Central R&D Labs., Inc.;Keisuke Kawano,Toyota Central R&D Labs., Inc;Takuro Kutsuna,Toyota Central R&D Labs. Inc.",koide@mosk.tytlabs.co.jp;kawano@mosk.tytlabs.co.jp;kutsuna@mosk.tytlabs.co.jp,Deep Learning*; Applications/Computational Biology and Bioinformatics,Poster
2400,Objective and efficient inference for couplings in neuronal networks,"Inferring directional couplings from the spike data of networks is desired in various scientific fields such as neuroscience. Here, we apply a recently proposed objective procedure to the spike data obtained from the Hodgkin--Huxley type models and ¬•textit{in vitro} neuronal networks cultured in a circular structure. As a result, we succeed in reconstructing synaptic connections accurately from the evoked activity as well as the spontaneous one. To obtain the results, we invent an analytic formula approximately implementing a method of screening relevant couplings. This significantly reduces the computational cost of the screening method employed in the proposed objective procedure, making it possible to treat large-size systems as in this study.","Yu Terada*,RIKEN;Tomoyuki Obuchi,Tokyo Institute of Technology;Takuya Isomura,RIKEN;Yoshiyuki Kabashima,Tokyo Institute of Technology",yu.terada@riken.jp;obuchi@c.titech.ac.jp;takuya.isomura@riken.jp;kaba@c.titech.ac.jp,Neuroscience and Cognitive Science/Neural Coding*; Theory/Statistical Physics of Learning,Poster
2404,Learning from Group Comparisons: Exploiting Higher Order Interactions,"We study the problem of learning from group comparisons, with applications in predicting outcomes of sports and online games. Most of the previous works in this area focus on learning individual effects---they assume each player has an underlying score, and the ''ability'' of the team is modeled by the sum of team members' scores. Therefore, all the current approaches cannot model deeper interaction between team members: some players perform much better if they play together, and some players perform poorly together. In this paper, we propose a new model that takes the player-interaction effects into consideration. However, under certain circumstances, the total number of individuals can be very large, and number of player interactions grows quadratically, which makes learning intractable. In this case, we propose a latent factor model, and show that the sample complexity of our model is bounded under mild assumptions. Finally, we show that our proposed models have much better prediction power on several E-sports datasets, and furthermore can be used to reveal interesting patterns that cannot be discovered by previous methods.","Yao Li*,University of California, Davis;Minhao Cheng,University of California, Davis;Kevin Fujii,UC Davis Department of Statistics;Fushing Hsieh,UC Davis Department of Statistics;Cho-Jui Hsieh,UCLA, Google Research",yaoli@ucdavis.edu;mhcheng@ucdavis.edu;kmfujii@ucdavis.edu;fhsieh@ucdavis.edu;chojui.hsieh@gmail.com,Algorithms/Ranking and Preference Learning*; Applications/Matrix and Tensor Factorization,Poster
2413,Supervising Unsupervised Learning,"We introduce a framework to leverage knowledge acquired from a repository of (heterogeneous) supervised datasets to new unsupervised datasets. Our perspective avoids the subjectivity inherent in unsupervised learning by reducing it to supervised learning, and  provides a principled way to evaluate unsupervised algorithms. We demonstrate the versatility of our framework via rigorous agnostic bounds on a variety of unsupervised problems. In the context of clustering, our approach helps choose the number of clusters and the clustering algorithm,  remove the outliers, and provably circumvent the Kleinberg's  impossibility result.  Experimental results across hundreds of problems demonstrate significant improvements in performance on unsupervised data with simple algorithms, despite the fact that our problems come from heterogeneous domains. Additionally, our framework lets us leverage deep networks to learn common features from many such small datasets, and perform zero shot learning with massive performance gains. ","Vikas Garg*,MIT;Adam Kalai,Microsoft Research",vgarg@csail.mit.edu;adum@microsoft.com,Algorithms/Unsupervised Learning*; Algorithms/AutoML,Spotlight
2433,Nonparametric Bayesian Lomax delegate racing for survival analysis with competing risks,"We propose Lomax delegate racing (LDR) to explicitly model the mechanism of survival under competing risks and to interpret how the covariates accelerate or decelerate the time to event. LDR explains non-monotonic covariate effects by racing a potentially infinite number of sub-risks, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. Moreover, LDR is naturally able to model censoring and missing event times or types. For inference, we develop a Gibbs sampler under data augmentation for moderately sized data, along with a stochastic gradient descent maximum a posteriori inference algorithm for big data applications. Illustrative experiments are provided on both synthetic and real datasets, and comparison with various benchmark algorithms for survival analysis with competing risks demonstrates distinguished performance of LDR. ","Quan Zhang,University of Texas at Austin;Mingyuan Zhou*,University of Texas at Austin",quan.zhang@mccombs.utexas.edu;Mingyuan.Zhou@mccombs.utexas.edu,Probabilistic Methods/Hierarchical Models*; Probabilistic Methods/Bayesian Nonparametrics,Poster
2434,Adversarially Robust Generalization Requires More Data ,"Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high ""standard"" accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of ""standard"" learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.","Ludwig Schmidt,MIT;Shibani Santurkar,MIT;Dimitris Tsipras,MIT;Kunal Talwar*,Google;Aleksander Madry,MIT",ludwigs@mit.edu;shibani@mit.edu;tsipras@mit.edu;kunal@google.com;madry@mit.edu,"Theory/Learning Theory*; Applications/Privacy, Anonymity, and Security",Spotlight
2435,Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents,"Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.","Edoardo Conti,Facebook;Vashisht Madhavan*,;Felipe Petroski Such,Uber AI Labs;Joel Lehman,;Kenneth Stanley,Uber AI Labs;Jeff Clune,Uber AI Labs",edoardo.conti@gmail.com;vashisht@uber.com;felipe.such@uber.com;joel.lehman@uber.com;kstanley@uber.com;jeffclune@uber.com,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Exploration,Poster
2439,Practical exact algorithm for trembling-hand equilibrium refinements in games,"Nash equilibrium strategies have the known weakness that they do not prescribe rational play in situations that are reached with zero probability according to the strategies themselves, for example, if players have made mistakes. Trembling-hand refinements---such as extensive-form perfect equilibria and quasi-perfect equilibria---remedy this problem in sound ways. Despite their appeal, they have not received attention in practice since no known algorithm for computing them scales beyond toy instances. In this paper, we design an exact polynomial-time algorithm for finding trembling-hand equilibria in zero-sum extensive-form games. It is several orders of magnitude faster than the best prior ones, numerically stable, and quickly solves game instances with tens of thousands of nodes in the game tree. It enables, for the first time, the use of trembling-hand refinements in practice.","Gabriele Farina*,Carnegie Mellon University;Nicola Gatti,Politecnico di Milano;Tuomas Sandholm,Carnegie Mellon University",gfarina@cs.cmu.edu;nicola.gatti@polimi.it;sandholm@cs.cmu.edu,Theory/Game Theory and Computational Economics,Poster
2440,LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning,"This paper presents a new class of gradient methods for distributed 
machine learning that adaptively skip the gradient calculations to 
learn with reduced communication and computation. Simple rules 
are designed to detect slowly-varying gradients and, therefore, 
trigger the reuse of outdated gradients. The resultant gradient-based 
algorithms are termed Lazily Aggregated Gradient --- justifying our 
acronym \textbf{LAG} used henceforth. Theoretically, the merits of 
this contribution are: i) the convergence rate is the same as batch 
gradient descent in strongly-convex, convex, and nonconvex cases; 
and, ii) if the distributed datasets are heterogeneous (quantified by 
certain measurable constants), the communication rounds needed 
to achieve a targeted accuracy are reduced thanks to the adaptive 
reuse of \emph{lagged} gradients. Numerical experiments on both 
synthetic and real data corroborate a significant communication 
reduction compared to alternatives.","Tianyi Chen*,University of Minnesota;Georgios B. Giannakis,University of Minnesota;Tao Sun,National U. of Defense Tech.;Wotao Yin,University of California, Los Angeles",chen3827@umn.edu;georgios@umn.edu;nudtsuntao@163.com;wotaoyin@math.ucla.edu,Optimization*; Optimization/Convex Optimization,Spotlight
2447,Scalable Robust Matrix Factorization with Nonconvex Loss,"Robust matrix factorization (RMF), which uses the $\ell_1$-loss, often outperforms standard matrix factorization using the $\ell_2$-loss, particularly when outliers are present. The state-of-the-art RMF solver is the RMF-MM algorithm, which, however, cannot utilize data sparsity. Moreover, sometimes even the (convex) $\ell_1$-loss is not robust enough. In this paper, we propose the use of nonconvex loss to enhance robustness. To address the resultant difficult optimization problem, we use majorization-minimization (MM) optimization and propose a new MM surrogate. To improve scalability, we exploit data sparsity and optimize the surrogate via its dual with the accelerated proximal gradient algorithm. The resultant algorithm has low time and space complexities and is guaranteed to converge to a critical point. Extensive experiments demonstrate its superiority over the state-of-the-art in terms of both accuracy and scalability.","Quanming Yao*,4Paradigm;James Tin-Yau Kwok,The Hong Kong University of Science and Technology",qyaoaa@connect.ust.hk;jamesk@cse.ust.hk,Applications/Matrix and Tensor Factorization*; Optimization/Non-Convex Optimization,Poster
2449,Power-law efficient neural codes provide general link between perceptual bias and discriminability,"Recent work in theoretical neuroscience has shown that information-theoretic ""efficient"" neural codes, which allocate neural resources to maximize the mutual information between stimuli and neural responses, give rise to a lawful relationship between perceptual bias and discriminability that is observed across a wide variety of psychophysical tasks in human observers (Wei & Stocker 2017). Here we generalize these results to show that the same law arises under a much larger family of optimal neural codes, introducing a unifying framework that we call power-law efficient coding. Specifically, we show that the same lawful relationship between bias and discriminability arises whenever Fisher information is allocated proportional to any power of the prior distribution. This family includes neural codes that are optimal for minimizing Lp error for any p, indicating that the lawful relationship observed in human psychophysical data does not require information-theoretically optimal neural codes. Furthermore, we derive the exact constant of proportionality governing the relationship between bias and discriminability for different power laws (which includes information-theoretically optimal codes, where the power is 2, and so-called discrimax codes, where power is 1/2), and different choices of optimal decoder. As a bonus, our framework provides new insights into ""anti-Bayesian"" perceptual biases, in which percepts are biased away from the center of mass of the prior. We derive an explicit formula that clarifies precisely which combinations of neural encoder and decoder can give rise to such biases. ","Michael Morais*,Princeton University;Jonathan Pillow,Princeton University",mjmorais@princeton.edu;pillow@princeton.edu,Neuroscience and Cognitive Science/Neural Coding*; Neuroscience and Cognitive Science/Perception,Poster
2453,Active Geometry-Aware Visual Recognition in Cluttered Scenes,"Cross-object occlusions remain an important source of failures for current state-of-the-art object detectors. Actively selecting camera views for undoing  occlusions and recovering missing information has been identified as an important field of research since as early as 1980's, under the name active vision. Yet, 1980's active vision was not equipped with deep neural detectors, memory modules, or view selection policies, and often attempted tasks and imagery that would appear elementary with current detectors, even from a single camera view. On the other hand, the recent resurrection of active view selection policies has focused on reconstructing or classifying isolated objects. This work presents a paradigm for active object recognition under heavy occlusions, as was the original premise. It introduces a geometry-aware 3D neural memory that accumulates information of the full scene across multiple camera views into a 3D feature tensor in a geometrically consistent manner: information regarding the same 3D physical point is placed nearby in the tensor using egomotion-aware feature warping and (learned) depth-aware unprojection operations. Object detection, segmentation, and 3D reconstruction is then carried out directly using the accumulated 3D feature memory. The proposed model does not need to commit early to object detections, as current geometry-unaware object detection in 2D videos, and generalizes much better than geometry-unaware LSTM/GRU memories. The lack of geometric constraints on previous architectures appears to be the bottleneck for handling combinatorial explosions of visual data due to cross-object occlusions. The proposed model handles heavy occlusions even when trained with very little training data, by moving the head of the active observer between nearby views, seamlessly combining geometry with learning from experience.","Ricson Cheng*,Carnegie Mellon University;Ziyan Wang,CMU;Katerina Fragkiadaki,Carnegie Mellon University",ricsonc@andrew.cmu.edu;ziyanw1@andrew.cmu.edu;katef@cs.cmu.edu,Applications/Computer Vision*; Applications/Image Segmentation; Applications/Object Detection; Deep Learning/Recurrent Net,Poster
2454,Unsupervised Adversarial Invariance,"Data representations that contain all the information about target variables but are invariant to nuisance factors benefit supervised learning algorithms by preventing them from learning associations between these factors and the targets, thus reducing overfitting. We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. We describe an adversarial instantiation of this framework and provide analysis of its working. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation. Our method can be applied to any prediction task, eg., binary/multi-class classification or regression, without loss of generality.","Ayush Jaiswal*,University of Southern California;Rex Yue Wu,USC ISI;Wael Abd-Almageed,Information Sciences Institute;Prem Natarajan,USC ISI",ajaiswal@isi.edu;yue_wu@isi.edu;wamageed@isi.edu;pnataraj@isi.edu,Algorithms/Representation Learning*; Algorithms/Unsupervised Learning; Deep Learning/Adversarial Networks; Deep Learning/Supervi,Poster
2456,Content preserving text generation with attribute controls,"In this work, we address the problem of modifying textual attributes of sentences. Given an input sentence and a set of attribute labels, we attempt to generate sentences that are compatible with the conditioning information. To ensure that the model generates content compatible sentences, we introduce a reconstruction loss which interpolates between auto-encoding and back-translation loss components. We propose an adversarial loss to enforce generated samples to be attribute compatible and realistic. Through quantitative, qualitative and human evaluations we demonstrate that our model is capable of generating fluent sentences that better reflect the conditioning information compared to prior methods. We further demonstrate that the model is capable of simultaneously controlling multiple attributes.","Lajanugen Logeswaran*,University of Michigan;Honglak Lee,Google;Samy Bengio,Google Brain",llajan@umich.edu;honglak@google.com;bengio@google.com,Deep Learning*; Algorithms/Unsupervised Learning; Applications/Natural Language Processing; Deep Learning/A,Poster
2459,Multi-armed Bandits with Compensation,"We propose and study the known-compensation multi-arm bandit (KCMAB) problem, where a system controller offers a set of arms to many short-term players for $T$ steps. In each step, one short-term player arrives to the system. Upon arrival, the player greedily selects an arm with the current best average reward and receives a stochastic reward associated with the arm. In order to incentivize players to explore other arms,  the controller provides proper payment compensation to players. The objective of the controller is to maximize the total reward collected by players while minimizing the  compensation. We first give a compensation lower bound $\Theta(\sum_i {\Delta_i\log T\over KL_i})$, where $\Delta_i$ and $KL_i$ are the expected reward gap and Kullback-Leibler (KL) divergence between distributions of arm $i$ and the best arm, respectively. We then analyze three algorithms to solve the KCMAB problem, and obtain their regrets and compensations. We show that the algorithms all achieve $O(\log T)$ regret and $O(\log T)$ compensation that match the theoretical lower bound. Finally, we use experiments to show the behaviors of those algorithms.  ","Siwei Wang*,IIIS, Tsinghua University;Longbo Huang,IIIS, Tsinghua Univeristy",wangsw15@mails.tsinghua.edu.cn;longbohuang@tsinghua.edu.cn,Algorithms/Bandit Algorithms*; Algorithms/Online Learning; Reinforcement Learning and Planning/Reinforcement Learning,Poster
2461,GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training,"Data parallelism can boost the training speed of convolutional neural networks (CNN), but could suffer from significant communication costs caused by gradient aggregation. To alleviate this problem, several scalar quantization techniques have been developed to compress the gradients. But these techniques could perform poorly when used together with decentralized aggregation protocols like ring all-reduce (RAR), mainly due to their inability to directly aggregate compressed gradients. In this paper, we empirically demonstrate the strong linear correlations between CNN gradients, and propose a gradient vector quantization technique, named GradiVeQ, to exploit these correlations through principal component analysis (PCA) for substantial gradient dimension reduction. GradiveQ enables direct aggregation of compressed gradients, hence allows us to build a distributed learning system that parallelizes GradiveQ gradient compression and RAR communications. Extensive experiments on popular CNNs demonstrate that applying GradiveQ slashes the wall-clock gradient aggregation time of the original RAR by more than 5x without noticeable accuracy loss, and reduce the end-to-end training time by almost 50%. The results also show that \GradiveQ is compatible with scalar quantization techniques such as QSGD (Quantized SGD), and achieves a much higher speed-up gain under the same compression ratio.","Mingchao Yu*,University of Southern California;Zhifeng  Lin,University Of Southern California;Krishna Giri Narra,USC;Songze Li,University of Southern California;Youjie Li,UIUC;Nam Sung Kim,University of Illinois at Urbana-Champaign;Alexander Schwing,UIUC;Murali Annavaram,University of Southern California;Salman Avestimehr,University of Southern California",fishermanymc@gmail.com;zhifeng@usc.edu;narra@usc.edu;songzeli@usc.edu;li238@illinois.edu;nskim@illinois.edu;aschwing@illinois.edu;annavara@usc.edu;avestimehr@gmail.com,Applications/Communication- or Memory-Bounded Learning*; Deep Learning/Efficient Training Methods,Poster
2462,Multi-agent Online Learning with Asynchronous Feedback Loss,"We consider a game-theoretical multi-agent learning problem where the feedback information can be lost and rewards are given by a broad class of games known as variationally stable games. We propose a simple variant of the online gradient descent algorithm, called reweighted online gradient descent (ROGD) and show that in variationally stable games, if each agent adopts reweighted online gradient descent learning dynamics, then almost sure convergence to the set of Nash equilibria is guaranteed, even when the feedback loss is asynchronous and arbitrarily corrrelated among agents.
We then extend the framework to deal with unknown feedback loss probabilities by using an estimator (constructed from past data) in its replacement.
Finally, we further extend the framework to accommodate both asynchronous loss and stochastic rewards and establish that multi-agent ROGD learning still converges to the set of Nash equilibria in such settings. 
Together, we make meaningful progress towards the broad open problem of convergence of no-regret algorithms to Nash in general continuous games and contribute to the broad landscape of multi-agent online learning under imperfect information.","Zhengyuan Zhou*,Stanford University;Panayotis Mertikopoulos,CNRS;Susan Athey,;Nicholas Bambos,;Peter Glynn,Stanford University;Yinyu Ye,",zyzhou@stanford.edu;panayotis.mertikopoulos@imag.fr;athey@susanathey.com;bambos@stanford.edu;glynn@stanford.edu;yinyu-ye@stanford.edu,Theory/Game Theory and Computational Economics*; Theory/Learning Theory,Poster
2468,Scalable methods for 8-bit training of neural networks,"Quantized Neural Networks (QNNs) are often used to improve network efficiency during the inference phase, i.e. after the network has been trained. Extensive research in the field suggests many different quantization schemes. Still, the number of bits required, as well as the best quantization scheme, are yet unknown. Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision.  Armed with this knowledge, we quantize the model parameters,  activations and layer gradients to 8-bit, leaving at higher precision only the final step in the computation of the weight gradients. Additionally, as QNNs require batch-normalization to be trained at high precision, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity. Our simulations show that Range BN is equivalent to the traditional batch norm if a precise scale adjustment, which can be approximated analytically, is applied. To the best of the authors' knowledge, this work is the first to quantize the weights, activations, as well as a substantial volume of the gradients stream, in all layers (including batch normalization) to 8-bit while showing state-of-the-art results over the ImageNet-1K dataset. ","Ron Banner*,Intel;Itay Hubara,Technion;Elad Hoffer,Technion;Daniel Soudry,Technion",ron.banner@intel.com;itayhubara@gmail.com;elad.hoffer@gmail.com;daniel.soudry@gmail.com,Deep Learning/Efficient Training Methods*; Deep Learning/Efficient Inference Methods,Poster
2471,Dropping Symmetry for Fast Symmetric Nonnegative Matrix Factorization,"Symmetric nonnegative matrix factorization (NMF)---a special but important class of the general NMF---is demonstrated to be useful for data analysis and in particular for various clustering tasks. Unfortunately, designing fast algorithms for Symmetric NMF is not as easy as for the nonsymmetric counterpart, the later admitting the splitting property that allows  efficient alternating-type algorithms. To overcome this issue, we transfer the symmetric NMF to a nonsymmetric one, then we can  adopt the idea from the state-of-the-art algorithms for nonsymmetric NMF to design fast algorithms solving symmetric NMF.  We rigorously establish that solving nonsymmetric reformulation returns a solution for symmetric NMF and then apply  fast alternating based algorithms for the corresponding reformulated problem. Furthermore, we show these fast algorithms admit strong convergence guarantee in the sense that the generated sequence is convergent at least at a sublinear rate and it converges globally to a critical point of the symmetric NMF.  We conduct experiments on both synthetic data and image clustering to support our result.	","Zhihui Zhu,Johns Hopkins University;Xiao Li*,The Chinese University of Hong Kong;Kai Liu,Colorado School of Mines;Qiuwei Li,Colorado school of mines",zhihuizhu90@gmail.com;xli@ee.cuhk.edu.hk;liukaizhijia@gmail.com;qiuli@mines.edu,Applications/Matrix and Tensor Factorization*; Algorithms/Clustering; Algorithms/Unsupervised Learning; Applications/Signal Processing; Ap,Poster
2478,Link Prediction Based on Graph Neural Networks,"Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and (often) scalability. However, heuristic methods have strong assumptions on when two nodes are likely to have a link, which limits their effectiveness in networks where these assumptions fail. In this regard, a more reasonable way should be learning suitable ``heuristics'' from networks instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a ``heuristic'' that suits the current network. In this paper, we study this heuristic learning problem for link prediction. We first propose a $\gamma$-decaying heuristic theory. By unifying a wide range of heuristics into a single framework, we prove that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. ","Muhan Zhang*,Washington University in St. Louis;Yixin Chen,Washington University in St. Louis",muhan@wustl.edu;chen@cse.wustl.edu,Applications/Network Analysis*; Algorithms/Relational Learning,Spotlight
2482,Why so gloomy? A Bayesian explanation of human pessimism bias in the multi-armed bandit task,"How humans make repeated choices among options with imperfectly known reward outcomes is an important problem in psychology and neuroscience. This is often studied using multi-armed bandits, which are also frequently studied in machine learning. We present data from a human stationary bandit experiment, in which we vary the abundance and variability of reward availability (mean and variance of reward rate distributions). Surprisingly, subjects have significantly underestimated prior mean of reward rates -- elicited at the end of a bandit game, when they are asked to estimate the reward rates of arms never chosen during the game. Previously, human learning in the bandit task was found to be well captured by a Bayesian ideal learning model, the Dynamic Belief Model (DBM), albeit under an incorrect generative assumption of the temporal structure -- humans assume reward rates can change over time even when they are actually fixed. We find that the ""pessimism bias"" in the bandit task is well captured by the prior mean of DBM when fitted to human choices; but it is poorly captured by the prior mean of the Fixed Belief Model (FBM), an alternative Bayesian model that (correctly) assumes reward rates to be constant. This pessimism bias is also incompletely captured by a simple reinforcement learning model (RL) commonly used in neuroscience and psychology, in terms of fitted initial option values. While it seems highly sub-optimal, and thus mysterious, that humans have an underestimated prior reward expectation, our simulations show that an underestimated prior mean helps to maximize long-term gain, if the observer assumes volatility when reward rates are actually stable and uses a softmax decision policy instead of the optimal one (obtainable by dynamic programming). This raises the intriguing possibility that the brain underestimates reward rates to compensate for the incorrect non-stationarity assumption in the generative model and a suboptimal decision policy.","Dalin Guo*,UC San Diego;Angela Yu,UC San Diego",dag082@ucsd.edu;ajyu@ucsd.edu,Neuroscience and Cognitive Science/Cognitive Science*; Neuroscience and Cognitive Science/Human or Animal Learning; Reinforcement Learning and Planning,Poster
2486,Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model,"In this paper we consider the problem of computing an $\epsilon$-optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in $O(1)$ time. Given such a DMDP with states $\states$, actions $\actions$, discount factor $\gamma\in(0,1)$, and rewards in range $[0, 1]$ we provide an algorithm which computes an $\epsilon$-optimal policy with probability $1 - \delta$ where {\it both} the run time spent and number of sample taken is upper bounded by 
\[
O\left[\frac{|\cS||\cA|}{(1-\gamma)^3 \epsilon^2} \log \left(\frac{|\cS||\cA|}{(1-\gamma)\delta \epsilon}
		\right) 
		\log\left(\frac{1}{(1-\gamma)\epsilon}\right)\right] ~.
\]
For fixed values of $\epsilon$, this improves upon the previous best known bounds by a factor of $(1 - \gamma)^{-1}$ and matches the sample complexity lower bounds proved in \cite{azar2013minimax} up to logarithmic factors. 
We also extend our method to computing $\epsilon$-optimal policies for finite-horizon MDP with a generative model and provide a nearly matching sample complexity lower bound. ","Aaron Sidford,Stanford;Mengdi Wang,Princeton University;Xian Wu,Stanford University;Lin Yang*,Princeton University;Yinyu  Ye,Standord",sidford@stanford.edu;mengdiw@princeton.edu;xwu20@stanford.edu;lin.yang@princeton.edu;yyye@stanford.edu,Reinforcement Learning and Planning/Markov Decision Processes*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
2487,ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions,"Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. ChannelNets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolutional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods.","Hongyang Gao,Texas A&M University;Zhengyang Wang,Texas A&M University;Shuiwang Ji*,Texas A&M University",hongyang.gao@tamu.edu;zhengyang.wang2@wsu.edu;sji@eecs.wsu.edu,Deep Learning/Efficient Inference Methods*; Deep Learning/CNN Architectures; Deep Learning/Predictive Models; Deep Learning/Supervised,Poster
2490,Causal Inference and Mechanism Clustering of A Mixture of Additive Noise Models,"The inference of the causal relationship between a pair of observed variables is a fundamental problem in science, and most existing approaches are based on one single causal model. In practice, however, observations are often collected from multiple sources with heterogeneous causal models due to certain uncontrollable factors, which renders causal analysis results obtained by a single model skeptical. In this paper, we generalize the Additive Noise Model (ANM) to a mixture model, which consists of a finite number of ANMs, and provide the condition of its causal identifiability. To conduct model estimation, we propose Gaussian Process Partially Observable Model (GPPOM), and incorporate independence enforcement into it to learn latent parameter associated with each observation. Causal inference and clustering according to the underlying generating mechanisms of the mixture model are addressed in this work. Experiments on synthetic and real data demonstrate the effectiveness of our proposed approach.","Shoubo Hu*,The Chinese University of Hong Kong;Zhitang Chen,Huawei Noah‚Äôs Ark Lab;Vahid Partovi Nia,Huawei Montreal Research Center;Laiwan Chan,The Chinese Unversity of Hong Kong;Yanhui Geng,Huawei Noah‚Äôs Ark Lab",shoubo.hu@gmail.com;chenzhitang2@huawei.com;vahid.partovinia@polymtl.ca;lwchan@cse.cuhk.edu.hk;geng.yanhui@huawei.com,Probabilistic Methods/Causal Inference*; Algorithms/Clustering; Probabilistic Methods/Gaussian Processes; Probabilistic Methods/Late,Poster
2491,Contour location via entropy reduction leveraging multiple information sources,"We introduce an algorithm to locate contours of functions that are expensive to evaluate. The problem of locating contours arise in many applications, including classification, constrained optimization, and analysis of performance of mechanical 
and dynamical systems (reliability, probability of failure, stability, etc.). Our algorithm locates contours using information from multiple sources, which are available in the form of relatively inexpensive, biased, and possibly noisy approximations to the original function. Considering multiple information sources can lead to significant cost savings. We also introduce the concept of contour entropy, a formal measure of uncertainty about the location of the zero contour of a function approximated by a statistical surrogate model. Our algorithm locates contours efficiently by maximizing the reduction of contour entropy per unit cost.","Alexandre Marques*,Massachusetts Institute of Technology;R√©mi Lam,MIT;Karen Willcox,Massachusetts Institute of Technology",noll@mit.edu;rlam@mit.edu;kwillcox@mit.edu,Algorithms/Classification*; Algorithms/Active Learning,Spotlight
2500,Assessing Generative Models via Precision and Recall,"Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods such as Frechet Inception Distance (FID) correlate well with the perceived quality of samples and they also show sensitivity to dropping modes from the target distribution. However, these metrics are unable to distinguish different failure cases since they inherently only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach, we perform an empirical study on several variants of Generative Adversarial Networks and the Variational Autoencoder. In an extensive set of experiments, we show that the proposed metric is able to disentangle the quality of samples from the coverage of the target distribution.","Mehdi S. M. Sajjadi*,Max Planck Institute for Intelligent Systems;Olivier Bachem,Google Brain;Mario Lucic,Google Brain;Olivier Bousquet,Google;Sylvain Gelly,Google",msajjadi@tue.mpg.de;bachem@google.com;lucic@google.com;obousquet@google.com;sylvaingelly@google.com,Deep Learning/Generative Models*; Deep Learning; Deep Learning/Adversarial Networks,Poster
2501,Multiple-Step Greedy Policies in Approximate and Online Reinforcement Learning," Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning, via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work (Efroni et al., 2018), multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work, we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty, arising with soft-policy updates: even in the absence of approximations, and contrary to the 1-step-greedy case, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty, we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator. ","Yonathan Efroni*,Technion;Gal Dalal,Technion;Bruno Scherrer,Inria;Shie Mannor,Technion",jonathan.efroni@gmail.com;gald@tx.technion.ac.il;bruno.scherrer@inria.fr;shie@ee.technion.ac.il,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Decision and Control,Spotlight
2512,A Convex Duality Framework for GANs,"Generative adversarial network (GAN) is a minimax game between a generator mimicking the true  model and a discriminator distinguishing the samples produced by the generator from the real training samples. Given an unconstrained discriminator able to approximate any function, this game reduces to finding the generative model minimizing a divergence measure, e.g. the Jensen-Shannon divergence, to the data distribution. However, in practice the discriminator is constrained to be in a smaller class F such as neural nets. Then, a natural question is how the minimum divergence interpretation changes as we constrain F. In this work, we address this question by developing a convex duality framework for analyzing GANs. For a convex set F, this duality framework interprets f-GANs as finding the generative model with the minimum f-divergence to the distributions penalized to match the moments of the data distribution, with the moments specified by the discriminators in F. We show a similar result also holds for Wasserstein GANs. As a byproduct, we apply this duality framework to a specific mixture of an f-divergence and a Wasserstein distance. Unlike f-divergences we prove this mixed distance enjoys a continuous behavior in the generative model. This result suggests regularizing the discriminator in f-GANs by either constraining its Lipschitz constant or by adversarially training it using  Wasserstein risk minimization. We provide numerical experiments supporting our theoretical results.","Farzan Farnia*,Stanford University;David Tse,Stanford University",farnia@stanford.edu;dntse@stanford.edu,Deep Learning/Adversarial Networks*; Deep Learning/Generative Models; Optimization/Convex Optimization,Poster
2513,Horizon-Independent Minimax Linear Regression,"We consider a linear regression game: at each round, an adversary reveals a covariate vector, the learner predicts a real value, the adversary reveals a label, and the learner suffers the squared prediction error. The aim is to minimize the difference between the cumulative loss and that of the linear predictor that is best in hindsight. Previous work demonstrated that the minimax optimal strategy is easy to compute recursively from the end of the game; this requires the entire sequence of covariate vectors in advance. We show that, once provided with a measure of the scale of the problem, we can invert the recursion and play the minimax strategy without knowing the future covariates. Further, we show that this forward recursion remains optimal even against adaptively chosen labels and covariates, provided that the adversary adheres to a set of constraints that prevent misrepresentation of the scale of the problem. This strategy is horizon-independent, i.e. it incurs no more regret than the optimal strategy that knows in advance the number of rounds of the game. We also provide an interpretation of the minimax algorithm as a follow-the-regularized-leader strategy with a data-dependent regularizer, and obtain an explicit expression for the minimax regret. ","Alan Malek*,MIT;Peter Bartlett,""University of California, Berkeley""",alan.malek@gmail.com;bartlett@cs.berkeley.edu,Algorithms/Online Learning*; Theory/Learning Theory,Poster
2521,Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression,"In this paper, we obtain improved running times for regression and top eigenvector computation for numerically sparse matrices. Given a data matrix $\mat{A} \in \R^{n \times d}$ where every row $a \in \R^d$ has $\|a\|_2^2 \leq L$ and numerical sparsity $\leq s$, i.e. $\|a\|_1^2 / \|a\|_2^2 \leq s$, we provide faster algorithms for these problems for many parameter settings.

For top eigenvector computation, when $\gap > 0$ is the relative gap between the top two eigenvectors of $\mat{A}^\top \mat{A}$ and $r$ is the stable rank of $\mat{A}$ we obtain a running time of $\otilde(nd + r(s + \sqrt{r s}) / \gap^2)$ improving upon the previous best unaccelerated running time of $O(nd + r d / \gap^2)$. As $r \leq d$ and $s \leq d$ our algorithm everywhere improves or matches the previous bounds for all parameter settings.

For regression, when $\mu > 0$ is the smallest eigenvalue of $\mat{A}^\top \mat{A}$ we obtain a running time of $\otilde(nd + (nL / \mu) \sqrt{s nL / \mu})$ improving upon the previous best unaccelerated running time of $\otilde(nd + n L d / \mu)$. This result expands when regression can be solved in nearly linear time from when $L/\mu = \otilde(1)$ to when $L / \mu = \otilde(d^{2/3} / (sn)^{1/3})$.

Furthermore, we obtain similar improvements even when row norms and numerical sparsities are non-uniform and we show how to achieve even faster running times by accelerating using approximate proximal point \cite{frostig2015regularizing} / catalyst \cite{lin2015universal}. Our running times depend only on the size of the input and natural numerical measures of the matrix, i.e. eigenvalues and $\ell_p$ norms, making progress on a key open problem regarding optimal running times for efficient large-scale learning.","Neha Gupta*,Stanford University;Aaron Sidford,Stanford",nehagupta@cs.stanford.edu;sidford@stanford.edu,"Algorithms/Regression*; Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA)",Poster
2527,Experimental Design for Cost-Aware Learning of Causal Graphs,"We consider the minimum cost intervention design problem: Given the essential graph of a causal graph and a cost to intervene on a variable, identify the set of interventions with minimum total cost that can learn any causal graph with the given essential graph. We first show that this problem is NP-hard. We then prove that we can achieve a constant factor approximation to this problem. We then constrain the sparsity of each intervention. We create an algorithm that returns an intervention design that is nearly optimal in terms of size for sparse graphs with sparse interventions and we discuss how to use it when there are costs on the vertices.","Erik Lindgren*,UT Austin;Murat Kocaoglu,UT Austin;Alex Dimakis,UT Austin;Sriram Vishwanath,University of Texas at Austin",erikml@utexas.edu;mkocaoglu@utexas.edu;dimakis@austin.utexas.edu;sriram@austin.utexas.edu,Probabilistic Methods/Causal Inference*; Optimization/Combinatorial Optimization,Poster
2528,Task-Driven Convolutional Recurrent Models of the Visual System,"Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classification tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain's visual system.  However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas.  Here we explored the role of recurrence in improving classification performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, custom cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identified novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs explained the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain's recurrent connections in performing difficult visual behaviors.","Aran Nayebi*,Stanford University;Daniel Bear,Stanford University;Jonas Kubilius,Massachusetts Institute of Technology;Kohitij Kar,MIT;Surya Ganguli,;David Sussillo,Google Brain;James DiCarlo,Massachusetts Institute of Technology;Daniel Yamins,Stanford University",anayebi@stanford.edu;dbear@stanford.edu;qbilius@mit.edu;kohitij@mit.edu;sganguli@stanford.edu;sussillo@google.com;dicarlo@mit.edu;yamins@stanford.edu,Neuroscience and Cognitive Science*; Deep Learning/Biologically Plausible Deep Networks; Deep Learning/CNN Architectures; Deep Lear,Poster
2533,Meta-Reinforcement Learning of Structured Exploration Strategies,"Exploration is a fundamental challenge in reinforcement learning (RL). Many current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we study how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.","Abhishek Gupta*,UC Berkeley;Russell Mendonca,University of California, Berkeley;YuXuan Liu,UC Berkeley;Pieter Abbeel,UC Berkeley;Sergey Levine,UC Berkeley",abhigupta@berkeley.edu;russellm@berkeley.edu;yuxuanliu@berkeley.edu;pabbeel@cs.berkeley.edu;svlevine@eecs.berkeley.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Deep Learning/Meta-Learning; Reinforcement Learning and Planning/Exploration,Spotlight
2542,Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method for Stochastic Sparse Linear Regression with Limited Attribute Observation,"We develop new stochastic gradient methods for efficiently solving sparse linear regression in a partial attribute observation setting, where learners are only allowed to observe a fixed number of actively chosen attributes per example at training and prediction times. It is shown that the methods achieve essentially a sample complexity of $O(1/\varepsilon)$ to attain an error of $\varepsilon$ under a variant of restricted eigenvalue condition, and the rate has better dependency on the problem dimension than existing methods. Particularly, if the smallest magnitude of the non-zero components of the optimal solution is not too small, the rate of our proposed {\it Hybrid} algorithm can be boosted to near the minimax optimal sample complexity of {\it full information} algorithms. The core ideas are (i) efficient construction of an unbiased gradient estimator by the iterative usage of the hard thresholding operator for configuring an exploration algorithm; and (ii) an adaptive combination of the exploration and an exploitation algorithms for quickly identifying the support of the optimum and efficiently searching the optimal parameter in its support. Experimental results are presented to validate our theoretical findings and the superiority of our proposed methods.","Tomoya Murata*,NTT DATA Mathematical Systems Inc.;Taiji Suzuki,University of Tokyo / RIKEN",taramurh@gmail.com;taiji@mist.i.u-tokyo.ac.jp,Algorithms*; Algorithms/Sparsity and Compressed Sensing; Optimization/Convex Optimization,Poster
2545,Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance,"Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements  on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.","Neal Jean*,Stanford University;Sang Xie,Stanford University;Stefano  Ermon,Stanford University",nealjean@stanford.edu;xie@cs.stanford.edu;ermon@cs.stanford.edu,Algorithms/Semi-Supervised Learning*; Algorithms/Regression,Poster
2556,Generalizing to Unseen Domains via Adversarial Data Augmentation,"We are concerned with learning models that generalize well to different unseen domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from the source domain, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is ""hard"" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers (e.g., ridge or lasso) that regularize towards zero. On digit recognition and semantic segmentation tasks, we empirically observe that our method learns models that improve performance across a priori unknown data distributions. ","Riccardo  Volpi*,Istituto Italiano di Tecnologia;Hongseok Namkoong,Stanford University;Ozan Sener,Intel Labs;John Duchi,Stanford University;Vittorio Murino,Istituto Italiano di Tecnologia;Silvio Savarese,Stanford University",riccardo.volpi@iit.it;hnamk@stanford.edu;ozan.sener@intel.com;jduchi@stanford.edu;vittorio.murino@iit.it;ssilvio@stanford.edu,Deep Learning*; Deep Learning/Adversarial Networks,Poster
2562,Hyperbolic Neural Networks,"Hyperbolic spaces have recently gained momentum in the context of machine learning due to their high capacity and tree-likeliness properties. However, the representational power of hyperbolic geometry is not yet on par with Euclidean geometry, firstly because of the absence of corresponding hyperbolic neural network layers. Here, we bridge this gap in a principled manner by combining the formalism of M√∂bius gyrovector spaces with the Riemannian geometry of the Poincar√© model of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep learning tools: multinomial logistic regression, feed-forward and recurrent neural networks. This allows to embed sequential data and perform classification in the hyperbolic space. Empirically, we show that, even if hyperbolic optimization tools are limited, hyperbolic sentence embeddings either outperform or are on par with their Euclidean variants on textual entailment and noisy-prefix recognition tasks.","Octavian Ganea*,ETH;Gary Becigneul,ETHZ;Thomas Hofmann,ETH Zurich",octavian.ganea@inf.ethz.ch;gary.becigneul@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,Deep Learning*; Applications/Natural Language Processing,Spotlight
2567,Breaking the Curse of Horizon: Infinite-Horizon Off-policy Estimation,"We consider the off-policy estimation problem of estimating the expected reward of a target policy using samples collected by a different behavior policy. Importance sampling (IS) has been a key technique to derive (nearly) unbiased estimators, but is known to suffer an excessively high variance in long-horizon problems.  In the extreme case where one tries to estimate the average per-step reward in infinite-horizon problems, the variance of an IS-based estimator may even be unbounded. In this paper, we propose a new off-policy estimation method that applies IS directly on the stationary state-visitation distributions to avoid the exploding variance issue faced by existing estimators.Our key contribution is a novel approach to estimating the density ratio of two stationary distributions, with data sampled from only the behavior distribution. We develop a mini-max loss function for the estimation problem, and derive a closed-form solution for the case of RKHS. We support our method with both theoretical  and empirical analysis. ","Qiang Liu,UT Austin;Lihong Li,Google Inc.;Ziyang Tang*,The University of Texas at Austin;Denny Zhou,Google",lqiang@cs.utexas.edu;lihongli.cs@gmail.com;ztang@cs.utexas.edu;dennyzhou@google.com,Reinforcement Learning and Planning/Reinforcement Learning*; Algorithms/Density Estimation; Reinforcement Learning and Planning,Spotlight
2577,Learning Task Specifications from Demonstrations,"  Real world applications often naturally decompose into several
  sub-tasks. In many settings (e.g., robotics) demonstrations provide
  a natural way to specify the sub-tasks. However, most methods for
  learning from demonstrations either do not provide guarantees that
  the artifacts learned for the subtasks can be safely recombined or
  limit the types of composition available.  Motivated by this
  deficit, we consider the problem of inferring binary non-Markovian
  rewards, also known as logical trace properties or
  \emph{specifications}, from demonstrations provided by an agent
  operating in an uncertain, stochastic environment. Crucially,
  specifications admit well-defined composition rules that are
  typically easy to interpret.  In this paper, we formulate the
  specification inference task as a maximum a posteriori (MAP)
  probability inference problem, apply the principle of maximum
  entropy to derive an analytic demonstration likelihood model and
  give an efficient approach to search for the most likely
  specification in a large candidate pool of a specifications. In our
  experiments, we demonstrate how learning specifications can help
  avoid common reward hacking bugs that often occur due to ad-hoc
  reward composition.","Marcell Vazquez-Chanlatte*,University of California, Berkeley;Susmit Jha,SRI International;Ashish Tiwari,SRI International;Mark Ho,Brown;Sanjit Seshia,UC Berkeley",marcell.vc@eecs.berkeley.edu;jha@csl.sri.com;tiwari@csl.sri.com;mark_ho@brown.edu;sseshia@eecs.berkeley.edu,Reinforcement Learning and Planning/Markov Decision Processes*; Algorithms/Relational Learning,Poster
2580,Learning an olfactory topography from neural activity in piriform cortex,"A major difficulty afflicting the study of olfactory perception is the lack of any obvious spatial organization or topography governing the relationship between odorants or the percepts they elicit. Here we develop a Gaussian process latent variable model to extract such a topography directly from olfactory responses measured in piriform cortex. Our approach seeks to map odorants to points in a low-dimensional embedding space, where the distance between points in this embedding space relates to the similarity of population responses they elicit. The model is specified by an explicit continuous mapping from a latent embedding space to the space of high-dimensional neural population activity patterns via a set of nonlinear neural tuning curves, each parametrized by a Gaussian process, followed by a low-rank model of correlated, odor-dependent Gaussian noise. We apply this model to large-scale calcium fluorescence imaging measurements of population activity in layers 2 and 3 of mouse piriform cortex following presentation of a diverse set of odorants. We show that we can learn a low-dimensional embedding of each odor, and a smooth tuning curve over the latent embedding space that accurately captures each neuron's response to different odorants. The model captures both signal and noise correlations across more than 500 neurons. We perform a co-smoothing analysis to show that the model can accurately predict responses of a population held-out neurons to test odorants.","Anqi Wu*,Princeton neuroscience institute; Stan Pashkovski,harvard university;Sandeep Datta,harvard university	;Jonathan Pillow,Princeton University",anqiw@princeton.edu;stanlp86@gmail.com;bob.datta@gmail.com;pillow@princeton.edu,Neuroscience and Cognitive Science/Neuroscience*; Neuroscience and Cognitive Science/Neural Coding; Probabilistic Methods/Gaussian Processes; Pr,Poster
2582,Fully Understanding The Hashing Trick,"Feature hashing, also known as {\em the hashing trick}, introduced by Weinberger et al. (2009), is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking, feature hashing uses a random sparse projection matrix $A : \mathbb{R}^n \to \mathbb{R}^m$ (where $m \ll n$) in order to reduce the dimension of the data from $n$ to $m$ while approximately preserving the Euclidean norm. Every column of $A$ contains exactly one non-zero entry, equals to either $-1$ or $1$.

Weinberger et al. showed tail bounds on $\|Ax\|_2^2$. Specifically they showed that for every $\varepsilon, \delta$, if $\|x\|_{\infty} / \|x\|_2$ is sufficiently small, and $m$ is sufficiently large, then 
\begin{equation*}\Pr[ \; | \;\|Ax\|_2^2 - \|x\|_2^2\; | < \varepsilon \|x\|_2^2 \;] \ge 1 - \delta \;.\end{equation*}
These bounds were later extended by Dasgupta et al. (2010) and most recently refined by Dahlgaard et al. (2017), however, the true nature of the performance of this key technique, and specifically the correct tradeoff between the pivotal parameters $\|x\|_{\infty} / \|x\|_2, m, \varepsilon, \delta$ remained an open question.

We settle this question by giving tight asymptotic bounds on the exact tradeoff between the central parameters, thus providing a complete understanding of the performance of feature hashing. We complement the asymptotic bound with empirical data, which shows that the constants ""hiding"" in the asymptotic notation are, in fact, very close to $1$, thus further illustrating the tightness of the presented bounds in practice.","Lior Kamma*,Aarhus University;Casper Freksen,Aarhus University;Kasper Green Larsen,Aarhus University, MADALGO",lior.kamma@cs.au.dk;cfreksen@cs.au.dk;larsen@cs.au.dk,Algorithms/Similarity and Distance Learning*; Algorithms/Sparse Coding and Dimensionality Expansion; Algorithms/Sparsity and Compressed Sens,Spotlight
2590,Evolved Policy Gradients,"We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.","Rein Houthooft*,OpenAI;Richard Chen,OpenAI;Phillip Isola,OpenAI;Bradly Stadie,Universtiy of California, Berkeley;Filip Wolski,OpenAI;Jonathan Ho,OpenAI, UC Berkeley;Pieter Abbeel,UC Berkeley",rein.houthooft@gmail.com;richardchen100@gmail.com;phillip@openai.com;bstadie@berkeley.edu;filip@openai.com;hoj@openai.com;pabbeel@cs.berkeley.edu,Deep Learning/Meta-Learning,Spotlight
2592,The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network,"An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface, very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work, we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity, the spectrum can be tuned so as to improve the efficiency of first-order optimization methods.","Jeffrey Pennington*,Google Brain;Pratik Worah,Google",jpennin@google.com;pworah@google.com,Theory*; Deep Learning/Optimization for Deep Networks,Poster
2598,Learning Concave Conditional Likelihood Models for Improved Analysis of Tandem Mass Spectra,"The most widely used technology to identify the proteins present in a complex biological sample is tandem mass spectrometry, which quickly produces a large collection of spectra representative of the peptides (i.e., protein subsequences) present in the original sample. In this work, we greatly expand the parameter learning capabilities of a dynamic Bayesian network (DBN) peptide-scoring algorithm, Didea, by deriving emission distributions for which its conditional log-likelihood scoring function remains concave. We show that this class of emission distributions, called Convex Virtual Emissions (CVEs), naturally generalizes the log-sum-exp function while rendering both maximum likelihood estimation and conditional maximum likelihood estimation concave for a wide range of Bayesian networks. Utilizing CVEs in Didea allows efficient learning of a large number of parameters while ensuring global convergence, in stark contrast to Didea‚Äôs previous parameter learning framework (which could only learn a single parameter using a costly grid search) and other trainable models (which only ensure convergence to local optima). The newly trained scoring function substantially outperforms the state-of-the-art in both scoring function accuracy and downstream Fisher kernel analysis. Furthermore, we significantly improve Didea‚Äôs runtime performance through successive optimizations to its message passing schedule and derive explicit connections between Didea‚Äôs new concave score and related MS/MS scoring functions.","John Halloran*,University of California, Davis;David Rocke,University of California, Davis",jthalloran@ucdavis.edu;dmrocke@ucdavis.edu,Applications/Computational Biology and Bioinformatics*; Probabilistic Methods/Graphical Models,Poster
2600,Differentially Private k-Means with Constant Multiplicative Error,"We design new differentially private algorithms for the Euclidean k-means problem, both in the centralized model and in the local model of differential privacy. In both models, our algorithms achieve significantly improved error guarantees than the previous state-of-the-art. In addition, in the local model, our algorithm significantly reduces the number of interaction rounds.

Although the problem has been widely studied in the context of differential privacy, all of the existing constructions achieve only super constant approximation factors. We present, for the first time, efficient private algorithms for the problem with constant multiplicative error. Furthermore, we show how to modify our algorithms so they compute private corsets for k-means clustering in both models.","Uri Stemmer*,Weizmann Institute;Haim Kaplan,",uri.stemmer@gmail.com;haimk@post.tau.ac.il,"Applications/Privacy, Anonymity, and Security*; Algorithms/Clustering; Algorithms/Unsupervised Learning",Spotlight
2613,Policy Optimization via Importance Sampling,"Policy optimization is an effective reinforcement learning approach to solve continuous control tasks. Recent achievements have shown that alternating on-line and off-line optimization is a successful choice for efficient trajectory reuse. However, deciding when to stop optimizing and collect new trajectories is non-trivial as it requires to account for the variance of the objective function estimate. In this paper, we propose a novel model-free policy search algorithm, POIS, applicable in both control-based and parameter-based settings. We first derive a high-confidence bound for importance sampling estimation and then we define a surrogate objective function which is optimized off-line using a batch of trajectories. Finally, the algorithm is tested on a selection of continuous control tasks, with both linear and deep policies, and compared with the state-of-the-art policy optimization methods.","Alberto Maria Metelli*,Politecnico di Milano;Matteo Papini,Politecnico di Milano;Francesco Faccio,Politecnico di Milano;Marcello Restelli,Politecnico di Milano",albertomaria.metelli@polimi.it;matteo.papini@polimi.it;francesco.faccio@mail.polimi.it;marcello.restelli@polimi.it,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Markov Decision Processes; Reinforcement Learning and Plannin,Oral
2618,Adversarial Logit Pairing,"We introduce adversarial logit pairing (ALP), a new regularization technique designed to increase robustness to adversarial examples.
We first demonstrate that ALP is a generalization of weight decay. Then, using the ImageNet dataset, we empirically show that the performance of the state of the art adversarial training defense from Madry et al. degrades on high dimensional input spaces. Next, we show that ALP achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%.
Unlike previous work on adversarial training, we achieve this improvement without an increase in model size.
Finally, we show that examples generated from an ALP-trained model are the current state-of-the-art transfer attack. This transfer attack successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al.), dropping its accuracy from 66.6% to 47.1%.
With this new accuracy drop, adversarial logit pairing ties with Tramer et al. for the state of the art on black box attacks on ImageNet.","Harini Kannan*,Google Brain;Ian Goodfellow,Google Brain;Alexey Kurakin,Google Brain",hkannan@google.com;goodfellow@google.com;kurakin@google.com,Deep Learning*; Deep Learning/Adversarial Networks,Poster
2620,Estimating Learnability in the Sublinear Data Regime,"We consider the problem of estimating how well a model class is capable of fitting a distribution of labeled data.  We show that it is often possible to accurately estimate this ``learnability'' even when given an amount of data that is too small to reliably learn any accurate model.   Our first result applies to the setting where the data is drawn from a $d$-dimensional distribution with isotropic covariance, and the label of each datapoint is an arbitrary noisy function of the datapoint.  In this setting, we show that with $O(\sqrt{d})$ samples, one can accurately estimate the fraction of the variance of the label that can be explained via the best linear function of the data. 
We extend these techniques to a binary classification, and show that the prediction error of the best linear classifier can be accurately estimated given $O(\sqrt{d})$ labeled samples.  For comparison, in both the linear regression and binary classification settings, even if there is no noise in the labels, a sample size linear in the dimension, $d$, is required to \emph{learn} any function correlated with the underlying model.  We further extend our estimation approach to the setting where the data distribution has an (unknown) arbitrary covariance matrix, allowing these techniques to be applied to settings where the model class consists of a linear function applied to a nonlinear embedding of the data.  We demonstrate the practical viability of our approaches on synthetic and real data.  This ability to estimate the explanatory value of a set of features (or dataset), even in the regime in which there is too little data to realize that explanatory value, may be relevant to the scientific and industrial settings for which data collection is expensive and there are many potentially relevant feature sets that could be collected. ","Weihao Kong*,Stanford University;Gregory Valiant,Stanford University",kweihao@gmail.com;gvaliant@cs.stanford.edu,Algorithms*; Algorithms/Classification; Algorithms/Model Selection and Structure Learning; Algorithms/Re,Poster
2621,Algorithmic Assurance: An Active Approach to Algorithmic Testing using Bayesian Optimisation,"We introduce algorithmic assurance, the problem of testing whether machine learning algorithms are conforming to their intended design goal. We address this problem by proposing an efficient framework for algorithmic testing. To provide assurance, we need to efficiently discover scenarios where an algorithm decision deviates maximally from its intended gold standard. We mathematically formulate this task as an optimisation problem of an expensive, black-box function. We use an active learning approach based on Bayesian optimisation to solve this optimisation problem. We extend this framework to algorithms with vector-valued outputs by making appropriate modification in Bayesian optimisation via the Hedge algorithm. We theoretically analyse our methods for convergence. Using two real-world applications, we demonstrate the efficiency of our methods. The significance of our problem formulation and initial solutions is that it will serve as the foundation in assuring humans about machines making complex decisions.","Shivapratap Gopakumar,Deakin University;Sunil Gupta*,Deakin University, Australia;Santu Rana,Deakin University, Australia;Vu Nguyen,Deakin University;Svetha Venkatesh,Deakin University",shivapratap@gmail.com;sunil.gupta@deakin.edu.au;santu.rana@deakin.edu.au;v.nguyen@deakin.edu.au;svetha.venkatesh@deakin.edu.au,Algorithms/Active Learning*; Algorithms/Classification; Algorithms/Regression; Deep Learning/Predictive Models; Probabil,Poster
2637,Community Exploration: From Offline Optimization to Online Learning,"We introduce the community exploration problem that has various real-world applications such as online advertising. In the problem, an explorer allocates limited budget to explore communities so as to maximize the number of members he could meet. We provide a systematic study of the community exploration problem, from offline optimization to online learning. For the offline setting where the sizes of communities are known, we prove that the greedy methods for both of non-adaptive exploration and adaptive exploration are optimal. For the online setting where the sizes of communities are not known and need to be learned from the multi-round explorations, we propose an ``upper confidence'' like algorithm that achieves the logarithmic regret bounds. By combining the feedback from different rounds, we can achieve a constant regret bound. ","Xiaowei Chen*,The Chinese University of Hong Kong;Weiran Huang,Huawei Noah's Ark Lab;Wei Chen,Microsoft;John C. S. Lui,The Chinese University of Hong Kong",xwchen@cse.cuhk.edu.hk;huang.inbox@outlook.com;weic@microsoft.com;cslui@cse.cuhk.edu.hk,Algorithms/Bandit Algorithms*; Algorithms/Adaptive Data Analysis,Poster
2639,A Dual Framework for Low-rank Tensor Completion,"One of the popular approaches for low-rank tensor completion is to use the latent trace norm regularization. However, most existing works in this direction learn a sparse combination of tensors. In this work, we fill this gap by proposing a variant of the latent trace norm that helps in learning a non-sparse combination of tensors. We develop a dual framework for solving the proposed low-rank tensor completion problem. In this framework, we first show a novel characterization of the solution space with an interesting factorization of the optimal solution. This allows to propose two scalable optimization formulations. The problems are shown to lie on a Cartesian product of Riemannian spectrahedron manifolds. We exploit the versatile Riemannian optimization framework for proposing computationally efficient trust region algorithms. The experiments illustrate the efficacy of the proposed algorithms on several real-world datasets across applications.","Madhav Nimishakavi,Indian Institute of Science;Pratik Jawanpuria*,Microsoft;Bamdev Mishra,Microsoft",cse.madhav@gmail.com;pratik.iitb@gmail.com;bamdevm@gmail.com,Applications/Matrix and Tensor Factorization*; Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning; Optimization/Non-Convex O,Poster
2641,Low-rank Interaction with Sparse Additive Effects Model for Large Data Frames,"Many applications of machine learning involve the analysis of large data frames -- matrices collecting heterogeneous measurements (binary, numerical, counts, etc.) across samples -- with missing values. Low-rank models, as studied by Udell et al. (2016), are popular in this framework for tasks such as visualization, clustering and missing value imputation. Yet, available methods with statistical guarantees and efficient optimization do not allow explicit modeling of main additive effects such as row and column, or covariate effects. In this paper, we introduce a low-rank interaction and sparse additive effects (LORIS) model which combines matrix regression on a dictionary and low-rank design, to estimate main effects and interactions simultaneously. We provide statistical guarantees in the form of upper bounds on the estimation error of both components. Then, we introduce a mixed coordinate gradient descent (MCGD) method which provably converges sub-linearly to an optimal solution and is computationally efficient for large scale data sets. We show on simulated and survey data that the method has a clear advantage over current practices.","Genevi√®ve Robin*,√âcole Polytechnique;Hoi-To Wai,Arizona State University;Julie Josse,√âcole Polytechnique;Olga Klopp,ESSEC Business School;Eric Moulines,Ecole Polytechnique",genevieve.robin@polytechnique.edu;htwai@asu.edu;julie.josserennes@gmail.com;kloppolga@math.cnrs.fr;eric.moulines@polytechnique.edu,Algorithms/Missing Data*; Optimization/Convex Optimization,Spotlight
2646,Inference Aided Reinforcement Learning for Incentive Mechanism Design in Crowdsourcing,"Incentive mechanisms for crowdsourcing are designed to incentivize financially self-interested workers to generate and report high-quality labels. Existing mechanisms are often developed as one-shot static solutions, assuming a certain level of knowledge about worker models (expertise levels, costs for exerting efforts, etc.). In this paper, we propose a novel inference aided reinforcement mechanism that acquires data sequentially and requires no such prior assumptions. Specifically, we first design a Gibbs sampling augmented Bayesian inference algorithm to estimate workers' labeling strategies from the collected labels at each step. Then we propose a reinforcement incentive learning (RIL) method, building on top of the above estimates, to uncover how workers respond to different payments. RIL dynamically determines the payment without accessing any ground-truth labels. We theoretically prove that RIL is able to incentivize rational workers to provide high-quality labels both at each step and in the long run. Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models. Besides, the payments offered by RIL are more robust and have lower variances compared to existing one-shot mechanisms.","Zehong Hu*,Alibaba Group;Yitao Liang,UCLA;Jie Zhang,Nanyang Technological University;Zhao Li,Alibaba Group;Yang Liu,UC Santa Cruz/Harvard University",HUZE0004@e.ntu.edu.sg;yliang@cs.ucla.edu;zhangj@ntu.edu.sg;lizhao.lz@alibaba-inc.com;yangl@seas.harvard.edu,Theory/Game Theory and Computational Economics*; Probabilistic Methods/Bayesian Nonparametrics; Reinforcement Learning and Planning/Markov Deci,Poster
2652,Middle-Out Decoding,"Despite being virtually ubiquitous, sequence-to-sequence models are challenged by their lack of diversity and inability to be externally controlled. In this paper, we speculate that a fundamental shortcoming of sequence generation models is that the decoding is done strictly from left-to-right, meaning that outputs values generated earlier have a profound effect on those generated later. To address this issue, we propose a novel middle-out decoder architecture that begins from an initial middle-word and simultaneously expands the sequence in both directions. To facilitate information flow and maintain consistent decoding, we introduce a dual self-attention mechanism that allows us to model complex dependencies between the outputs. We illustrate the performance of our model on the task of video captioning, as well as a synthetic sequence de-noising task. Our middle-out decoder achieves significant improvements on de-noising and competitive performance in the task of video captioning, while quantifiably improving the caption diversity. Furthermore, we perform a qualitative analysis that demonstrates our ability to effectively control the generation process of our decoder.","Shikib Mehri*,Carnegie Mellon University;Leonid Sigal,University of British Columbia",mehrishikib@gmail.com;lsigal@cs.ubc.ca,Deep Learning/Recurrent Networks*; Applications/Computer Vision; Deep Learning/Attention Models; Deep Learning/Supervised Deep,Poster
2653,First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time,"(This is a theory paper) In this paper, we consider first-order methods for solving stochastic non-convex optimization problems. The key building block of the proposed algorithms is first-order procedures to extract negative curvature from the Hessian matrix through a principled sequence starting from noise, which are referred to {\it NEgative-curvature-Originated-from-Noise or NEON} and are of independent interest. Based on this building block, we design purely first-order stochastic algorithms for escaping from non-degenerate saddle points with a much better time complexity (almost linear time in  the problem's dimensionality). In particular, we develop a general framework of {\it first-order stochastic algorithms} with a second-order convergence guarantee based on our new technique and existing algorithms that may only converge to a first-order stationary point. For finding a nearly {\it second-order stationary point} $\x$ such that $\|\nabla F(\x)\|\leq \epsilon$ and $\nabla^2 F(\x)\geq -\sqrt{\epsilon}I$ (in high probability), the best time complexity of the presented algorithms is $\widetilde O(d/\epsilon^{3.5})$, where $F(\cdot)$ denotes the objective function and $d$ is the dimensionality of the problem. To the best of our knowledge, this is the first theoretical result of first-order stochastic algorithms with an almost linear time in terms of problem's dimensionality for finding second-order stationary points, which is  even competitive with  existing stochastic algorithms hinging on the second-order information.","Yi Xu,The University of Iowa;rong jin,alibaba group;Tianbao Yang*,University of Iowa",yi-xu@uiowa.edu;jinrong.jr@alibaba-inc.com;tianbao-yang@uiowa.edu,Optimization/Non-Convex Optimization*; Algorithms/Stochastic Methods,Poster
2654,To Trust Or Not To Trust A Classifier,"Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists a considerably more effective alternative.

We propose a new score, called the {\it trust score}, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines.

Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis. ","Heinrich Jiang*,Google Research;Been Kim,Google;Melody Guan,Stanford University;Maya Gupta,Google",heinrichj@google.com;beenkim@google.com;melodieguan@gmail.com;mayagupta@google.com,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Density Estimation; Theory/Frequentist Statistics",Poster
2657,Reparameterization Gradient for Non-differentiable Models,"We present a new algorithm for stochastic variational inference that targets at models with non-differentiable densities. One of the key challenges in stochastic variational inference is to come up with a low-variance estimator of the gradient of a variational objective. We tackle the challenge by generalizing the reparameterization trick, one of the most effective techniques for addressing the variance issue for differentiable models, so that the trick works for non-differentiable models as well. Our algorithm splits the space of latent variables into regions where the density of the variables is differentiable, and their boundaries where the density may fail to be differentiable. For each differentiable region, the algorithm applies the standard reparameterization trick and estimates the gradient restricted to the region. For each potentially non-differentiable boundary, it uses a form of manifold sampling and computes the direction for variational parameters that, if followed, would increase the boundary‚Äôs contribution to the variational objective. The sum of all the estimates becomes the gradient estimate of our algorithm. Our estimator enjoys the reduced variance of the reparameterization gradient while remaining unbiased even for non-differentiable models. The experiments with our preliminary implementation confirm the benefit of reduced variance and unbiasedness.","Wonyeol Lee*,KAIST;Hangyeol Yu,KAIST;Hongseok Yang,KAIST",wonyeol@kaist.ac.kr;yhk1344@kaist.ac.kr;hongseok00@gmail.com,Probabilistic Methods/Variational Inference*; Algorithms/Stochastic Methods,Poster
2665,A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization,"We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth finite-sum problems.
In particular, the objective function is given by the summation of a differentiable (possibly nonconvex) component, together with a possibly non-differentiable but convex component.
We propose a proximal stochastic gradient algorithm based on variance reduction,
called ProxSVRG+.
Our main contribution lies in the analysis of ProxSVRG+.
It recovers several existing convergence results (in terms of the number of stochastic gradient oracle calls and proximal operations), and improves/generalizes them.
In particular, ProxSVRG+ generalizes the best results given by the SCSG (stochastically controlled stochastic gradient) algorithm, recently proposed by [Lei et al., NIPS'17] for the smooth nonconvex case.
ProxSVRG+ is more straightforward than SCSG and yields simpler analysis.
Moreover, ProxSVRG+ outperforms the deterministic proximal gradient descent (ProxGD) for a wide range of minibatch sizes, which partially solves an open problem proposed in [Reddi et al., NIPS'16].
Also, ProxSVRG+ uses much less proximal oracle calls than ProxSVRG [Reddi et al., NIPS'16] if minibatch size $b<n^{2/3}$.
Besides, for nonconvex functions satisfied Polyak-\L{}ojasiewicz condition,
we show that ProxSVRG+ achieves global linear convergence rate without restart unlike ProxSVRG.
ProxSVRG+ also improves ProxGD and ProxSVRG/SAGA, and generalizes the results of SCSG in this case.
Finally, we conduct several numerical experiments and the experimental results are consistent with the theoretical results.","Zhize Li*,Tsinghua University;Jian Li,"" Tsinghua University, China""",zz-li14@mails.tsinghua.edu.cn;lijian83@mail.tsinghua.edu.cn,Optimization*; Algorithms/Stochastic Methods; Deep Learning/Efficient Training Methods; Optimization/Non-C,Spotlight
2666,Multimodal Generative Models for Scalable Weakly-Supervised Learning,"Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations. Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder (MVAE) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the MVAE on four datasets and show that we match state-of-the-art performance using many fewer parameters. In addition, we show that the MVAE is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider a case study of learning image transformations‚Äîedge detection, colorization, facial landmark segmentation, etc.‚Äîas a set of modalities. We find appealing results across this range of tasks.","Mike Wu*,Stanford University;Noah Goodman,Stanford University",wumike@cs.stanford.edu;ngoodman@stanford.edu,Deep Learning/Generative Models*; Algorithms/Semi-Supervised Learning,Poster
2671,How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery?,"When the linear measurements of an instance of low-rank matrix recovery
satisfy a restricted isometry property (RIP) --- i.e. they
are approximately norm-preserving --- the problem is known
to contain no spurious local minima, so exact recovery is guaranteed.
In this paper, we show that moderate RIP is not enough to eliminate
spurious local minima, so existing results can only hold for near-perfect
RIP. In fact, counterexamples are ubiquitous: every $x$ is the spurious
local minimum of a rank-1 instance of matrix recovery that satisfies
RIP. One specific counterexample has RIP constant $\delta=1/2$, but
causes randomly initialized stochastic gradient descent (SGD) to fail
12\% of the time. SGD is frequently able to avoid and escape spurious
local minima, but this empirical result shows that it can occasionally
be defeated by their existence. Hence, while exact recovery guarantees
will likely require a proof of no spurious local minima, arguments
based solely on norm preservation will only be applicable to a narrow
set of nearly-isotropic instances.","Richard Zhang*,UC Berkeley;Cedric Josz,UC Berkeley;Somayeh Sojoudi,UC Berkeley;Javad Lavaei,UC Berkeley",ryz@berkeley.edu;cedric.josz@gmail.com;sojoudi@berkeley.edu;lavaei@berkeley.edu,Optimization/Non-Convex Optimization,Spotlight
2676,Impossibility of deducing preferences and rationality from human policy,"Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings.
However, there has been little analysis of the general problem of inferring the reward of a human of unknown rationality.
The observed behavior can, in principle, be decomposed into two components: a reward function and a planning algorithm, both of which have to be inferred from behavior.
This paper presents a No Free Lunch theorem, showing that, without making `normative' assumptions beyond the data, nothing about the human reward function can be deduced from human behavior.
Unlike most No Free Lunch theorems, this cannot be alleviated by regularising with simplicity assumptions.
We show that the simplest hypotheses which explain the data are generally degenerate.","Stuart Armstrong*,Oxford University;S√∂ren Mindermann,Oxford University",stuart.armstrong@philosophy.ox.ac.uk;soeren.mindermann@gmail.com,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Markov Decision Processes,Poster
2690,Manifold Structured Prediction,"Structured prediction provides a general framework to deal with supervised problems where the outputs have semantically rich structure. While classical approaches consider finite, albeit potentially huge, output spaces, in this paper we discuss how structured prediction can be extended to a continuous scenario. Specifically, we study a structured prediction approach to manifold-valued regression. We characterize a class of problems for which the considered approach is statistically consistent and study how geometric optimization can be used to compute the corresponding estimator. Promising experimental results on both simulated and real data complete our study.","Alessandro Rudi*,INRIA, Ecole Normale Superieure;Carlo Ciliberto,;Gian Marconi,iit;Lorenzo Rosasco,",alessandro.rudi@inria.fr;cciliber@gmail.com;gmmarconi@gmail.com;lrosasco@mit.edu,Algorithms/Structured Prediction*; Theory/Spaces of Functions and Kernels,Poster
2695,Fast Greedy MAP Inference for Determinantal Point Process to Improve Recommendation Diversity,"The determinantal point process (DPP) is an elegant probabilistic model of repulsion with applications in various machine learning tasks including summarization and search. However, the maximum a posteriori (MAP) inference for DPP which plays an important role in many applications is NP-hard, and even the popular greedy algorithm can still be too computationally expensive to be used in large-scale real-time scenarios. To overcome the computational challenge, in this paper, we propose a novel algorithm to greatly accelerate the greedy MAP inference for DPP. In addition, our algorithm also adapts to scenarios where the repulsion is only required among nearby few items in the result sequence. We apply the proposed algorithm to generate relevant and diverse recommendations. Experimental results show that our proposed algorithm is significantly faster than state-of-the-art competitors, and provides a better relevance-diversity trade-off on several public datasets, which is also confirmed in an online A/B test.","Laming Chen*,Hulu LLC;Guoxin Zhang,Kwai Inc.;Hanning Zhou,Hulu LLC",laming.chen@hulu.com;zhangguoxin@kuaishou.com;ericzhouh@gmail.com,Applications/Recommender Systems*; Optimization/Submodular Optimization,Poster
2711,Learning Others' Intentional Models in Multi-Agent Settings Using Interactive POMDPs,"Interactive partially observable Markov decision processes (I-POMDPs) provide a principled framework for planning and acting in a partially observable, stochastic and multi-agent environment. It extends POMDPs to multi-agent settings by including models of other agents in the state space and forming a hierarchical belief structure. In order to predict other agents' actions using I-POMDPs, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others' intentional models which ascribe to them beliefs, preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than other methods. Our approach serves as a generalized Bayesian learning algorithm that learns other agents' beliefs, and transition, observation and reward functions. It also effectively mitigates the belief space complexity due to the nested belief hierarchy. ","Yanlin Han*,UIC;Piotr Gmytrasiewicz,UIC",yhan37@uic.edu;piotr@uic.edu,Reinforcement Learning and Planning/Multi-Agent RL*; Probabilistic Methods/MCMC; Reinforcement Learning and Planning/Decision and Control; Reinforc,Poster
2713,Contextual Pricing for Lipschitz Buyers,"We investigate the problem of learning a Lipschitz function from binary
  feedback. In this problem, a learner is trying to learn a Lipschitz function
  $f:[0,1]^d \rightarrow [0,1]$ over the course of $T$ rounds. On round $t$, an
  adversary provides the learner with an input $x_t$, the learner submits a
  guess $y_t$ for $f(x_t)$, and learns whether $y_t > f(x_t)$ or $y_t \leq
  f(x_t)$. The learner's goal is to minimize their total loss $\sum_t\ell(f(x_t),
  y_t)$ (for some loss function $\ell$). The problem is motivated by \textit{contextual dynamic pricing},
  where a firm must sell a stream of differentiated products to a collection of
  buyers with non-linear valuations for the items and observes only whether the
  item was sold or not at the posted price.

  For the symmetric loss $\ell(f(x_t), y_t) = \vert f(x_t) - y_t \vert$,  we
  provide an algorithm for this problem achieving total loss $O(\log T)$
  when $d=1$ and $O(T^{(d-1)/d})$ when $d>1$, and show that both bounds are
  tight (up to a factor of $\sqrt{\log T}$). For the pricing loss function
  $\ell(f(x_t), y_t) = f(x_t) - y_t {\bf 1}\{y_t \leq f(x_t)\}$ we show a regret
  bound of $O(T^{d/(d+1)})$ and show that this bound is tight. We present
  improved bounds in the special case of a population of linear buyers.","Jieming Mao,Princeton University;Renato Paes Leme,Google;Jon Schneider*,Princeton University",jiemingm@cs.princeton.edu;renatoppl@google.com;js44@cs.princeton.edu,Theory/Game Theory and Computational Economics*; Algorithms/Bandit Algorithms; Algorithms/Online Learning,Poster
2718,Online Improper Learning with an Approximation Oracle,"We study the following question: given an efficient approximation algorithm for an optimization problem, can we learn efficiently in the same setting? We give a formal affirmative answer to this question in the form of a reduction from online learning to offline approximate optimization using an efficient algorithm that guarantees near optimal regret. The algorithm is efficient in terms of the number of oracle calls to a given approximation oracle ‚Äì it makes only logarithmically many such calls per iteration. This resolves an open question by Kalai and Vempala, and by Garber. Furthermore, our result applies to the more general improper learning problems.","Elad Hazan,Princeton University and Google Brain;Wei Hu,Princeton University;Yuanzhi Li,Princeton;Zhiyuan Li*,Princeton University",ehazan@cs.princeton.edu;huwei@cs.princeton.edu;yuanzhil@cs.princeton.edu;zhiyuanli@cs.princeton.edu,Algorithms/Online Learning,Poster
2736,Bandit Learning in Concave N-Person Games,"This paper examines the long-run behavior of learning with bandit feedback in non-cooperative concave games. The bandit framework accounts for extremely low-information environments where the agents may not even know they are playing a game; as such, the agents‚Äô most sensible choice in this setting would be to employ a no-regret learning algorithm. In general, this does not mean that the players' behavior stabilizes in the long run: no-regret learning may lead to cycles, even with perfect gradient information. However, if a standard monotonicity condition is satisfied, our analysis shows that no-regret learning based on mirror descent with bandit feedback converges to Nash equilibrium with probability 1. We also derive an upper bound for the convergence rate of the process that nearly matches the best attainable rate for single-agent bandit stochastic optimization.","Mario Bravo,University of Santiago, Chile;David Leslie,Lancaster University;Panayotis Mertikopoulos*,CNRS",mario.a.bravo@gmail.com;d.leslie@lancaster.ac.uk;panayotis.mertikopoulos@imag.fr,Theory/Game Theory and Computational Economics*; Algorithms/Bandit Algorithms; Theory/Learning Theory,Poster
2739,On Fast Leverage Score Sampling and Optimal Learning,"Leverage score sampling provides an appealing way to perform approximate computations for large matrices.
Indeed, it allows to derive  faithful approximations with a  complexity  adapted to the  problem at hand. 
Yet, performing leverage scores  sampling  is a challenge in its own right and further approximations  are typically needed. 
In this paper, we study the problem of leverage score sampling for positive definite matrices defined by a kernel. 
Our contribution is twofold. First we provide a novel   algorithm for leverage score sampling.
We provide theoretical guarantees as well as empirical results proving that the 
proposed algorithm is currently the fastest and most accurate solution to this problem.
Second, we analyze the properties of the proposed method in a downstream supervised learning task.
Combining several algorithmic ideas, we derive the fastest solver for kernel ridge regression and Gaussian process regression  currently available.
Also in this case, theoretical findings   are corroborated by experimental results.","Alessandro Rudi*,INRIA, Ecole Normale Superieure;Daniele Calandriello,IIT;Luigi Carratino,unige;Lorenzo Rosasco,",alessandro.rudi@inria.fr;calandriello.daniele@gmail.com;luigicarratino@gmail.com;lrosasco@mit.edu,Algorithms/Kernel Methods*; Theory/Learning Theory; Theory/Regularization,Poster
2740,Unsupervised Video Object Segmentation for Deep Reinforcement Learning,"We present a new technique for deep reinforcement learning that automatically
detects moving objects and uses the relevant information for action selection. The
detection of moving objects is done in an unsupervised way by exploiting structure
from motion. Instead of directly learning a policy from raw images, the agent first
learns to detect and segment moving objects by exploiting flow information in
video sequences.  The learned representation is then used to focus the policy of
the agent on the moving objects. Over time, the agent identifies which objects are
critical for decision making and gradually builds a policy based on relevant moving
objects. This approach, which we call Motion-Oriented REinforcement Learning
(MOREL), is demonstrated on a suite of Atari games where the ability to detect
moving objects reduces the amount of interaction needed with the environment
to obtain a good policy.  Furthermore, the resulting policy is more interpretable
than policies that directly map images to actions or values with a black box neural
network. We can gain insight into the policy by inspecting the segmentation and
motion of each object detected by the agent. This allows practitioners to confirm
whether a policy is making decisions based on sensible information.","Vikash Goel*,University of Waterloo;Jameson Weng,University of Waterloo;Pascal Poupart,University of Waterloo",vikash.a.goel@gmail.com;weng.jameson126@gmail.com;ppoupart@uwaterloo.ca,Reinforcement Learning and Planning/Reinforcement Learning*; Algorithms/Representation Learning,Poster
2741,Efficient inference for time-varying behavior during learning,"The process of learning new behaviors is of great interest to various domains of neuroscience and artificial intelligence. However, most standard analyses of training data either treat behavior as fixed or track only coarse performance statistics (e.g., accuracy, choice bias), providing limited insight into evolving behavioral strategies. To overcome these limitations, we propose a dynamic psychophysical model that efficiently tracks trial-to-trial changes in behavior over the course of training. Based on a dynamic logistic regression model, our model infers a high-dimensional time-varying weight vector that expresses the dynamic dependencies of behavior on task stimuli and common task-irrelevant variables including choice history, sensory history, reward history, and choice bias. Our implementation scales to the largest behavioral datasets, allowing us to infer 500K parameters (e.g. 10 weights over 50K trials) in a few hours on a desktop computer. We optimize hyperparameters with the decoupled Laplace approximation, an efficient method for maximizing marginal likelihood that allows us to estimate directly from the data how quickly each weight evolves. To illustrate performance and utility, we apply our method to psychophysical data from both human subjects and rats learning the same delayed sensory discrimination task. We successfully track the dynamics of psychophysical weights during training, capturing day-to-day and trial-to-trial fluctuations that underly changes in performance, choice bias, and dependencies on task history. Finally, we leverage the flexibility of our model to investigate why rats frequently make mistakes on easy trials, demonstrating that this lapse phenomenon occurs due to sub-optimal weighting of task covariates.","Nicholas Roy*,Princeton Neuroscience Institute;Ji Hyun Bak,KIAS;Athena Akrami,Princeton University;Carlos Brody,Princeton University;Jonathan Pillow,Princeton University",nroy@princeton.edu;jhbak@kias.re.kr;aakrami@princeton.edu;brody@princeton.edu;pillow@princeton.edu,Neuroscience and Cognitive Science/Human or Animal Learning*; Neuroscience and Cognitive Science/Neuroscience,Poster
2743,Learning convex polytopes with margin,"We present a near-optimal algorithm for properly learning convex polytopes in the realizable PAC setting from data with a margin. Our first contribution is to identify distinct generalizations of the notion of {\em margin} from hyperplanes to polytopes and to understand how they relate geometrically; this result may be of interest beyond the learning setting. Our novel learning algorithm constructs a consistent polytope as an intersection of about $t \log t$ halfspaces in time polynomial in $t$ (where $t$ is the number of halfspaces forming an optimal polytope). This is an exponential improvement over the state of the art [Arriaga and Vempala, 2006]. We also improve over the super-polynomial-in-$t$ algorithm of Klivans and Servedio [2008], while achieving a better sample complexity. Finally, we provide the first nearly matching hardness-of-approximation lower bound, whence our claim of near optimality.","Lee-Ad  Gottlieb,Ariel University;Eran Kaufman,Ariel University;Aryeh Kontorovich*,Ben-Gurion University of the Negev;Gabriel Nivasch,Ariel University	",leead@ariel.ac.il;erankfmn@gmail.com;karyeh@cs.bgu.ac.il;gabrieln@ariel.ac.il,Algorithms*; Theory/Learning Theory,Poster
2766,Critical initialisation for deep signal propagation in noisy rectifier neural networks,"Stochastic regularisation is an important weapon in the arsenal of a deep learning practitioner. However, despite recent theoretical advances, our understanding of how noise influences signal propagation in deep neural networks remains limited. By extending recent work based on mean field theory, we develop a new framework for signal propagation in stochastic regularised neural networks. Our noisy signal propagation theory can incorporate several common noise distributions, including additive and multiplicative Gaussian noise as well as dropout. We use this framework to investigate initialisation strategies for noisy ReLU networks. We show that no critical initialisation strategy exists using additive noise, with signal propagation exploding regardless of the selected noise distribution. For multiplicative noise (e.g. dropout), we identify alternative critical initialisation strategies that depend on the second moment of the noise distribution. Simulations confirm that our proposed initialisation is able to stably propagate signals in deep networks, while using an initialisation disregarding noise fails to do so. In experiments on real-world data, the impact of this is that noisy ReLU models using dropout with our initialisation are able to train on MNIST and CIFAR-10 at large depths  where the standard initialisation strategy fails.","Arnu Pretorius*,Stellenbosch University;Elan Van Biljon,Stellenbosch University;Steve Kroon,Stellenbosch University;Herman Kamper,Stellenbosch University",arnupretorius@gmail.com;elanvanbiljon@gmail.com;kroon@sun.ac.za;kamperh@sun.ac.za,Theory/Regularization*; Deep Learning/Deep Autoencoders; Deep Learning/Efficient Training Methods; Deep Learning/Op,Poster
2770,Insights on representational similarity in neural networks with canonical correlation,"Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method. We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.","Ari Morcos*,DeepMind;Maithra Raghu,Google Brain/ Cornell;Samy Bengio,Google Brain",arimorcos@gmail.com;maithrar@gmail.com;bengio@google.com,Deep Learning/Visualization or Exposition Techniques for Deep Networks*; Algorithms/Representation Learning; Deep Learning/CNN Architectures; Deep Learning/Optimiza,Poster
2774,Variational Inference with Tail Adapted f-Divergence,"Variational inference with alpha-divergences has been widely used in modem probabilistic machine learning. Compared to Kullback-Leibler (KL) divergence, a major advantage of using alpha-divergences is their mass-covering property. However, alpha-divergences require importance sampling to estimate and optimize, which can be extremely ineffective when the importance weights have a heavy tail. In this paper, we propose new variants of f-divergences that adaptively change with the tail of the importance ratios. Compared to alpha-divergences, our approach theoretically guarantees finite mean of importance weights and simultaneously produce overdispersed approximations. We test our methods on Bayesian neural networks and Reinforcement learning in which our method is applied to improve a recent soft actor-critic (SAC) algorithm. Our results show that our approach yields significant advantages compared with classic KL and alpha-divergence based VI. ","Dilin Wang*,UT Austin;Hao Liu,UESTC;Qiang Liu,UT Austin",dilin@cs.utexas.edu;uestcliuhao@gmail.com;lqiang@cs.utexas.edu,Probabilistic Methods/Variational Inference,Oral
2777,Mental Sampling in Multimodal Representations,"Both resources in the natural environment and concepts in a semantic space are distributed ""patchily"", with large gaps in between the patches. To describe people's internal and external foraging behavior, various random walk models have been proposed. In particular, internal foraging has been modeled as sampling: in order to gather relevant information for making a decision, people draw samples from a mental representation using random-walk algorithms such as Markov chain Monte Carlo (MCMC). However, two common empirical observations argue against people using simple sampling algorithms such as MCMC for internal foraging. First, the distance between samples is often best described by a Levy flight distribution: the probability of the distance between two successive locations follows a power-law on the distances. Second, humans and other animals produce long-range, slowly decaying autocorrelations characterized as 1/f-like fluctuations, instead of the 1/f^2 fluctuations produced by random walks. We propose that mental sampling is not done by simple MCMC, but is instead adapted to multimodal representations and is implemented by Metropolis-coupled Markov chain Monte Carlo (MC3), one of the first algorithms developed for sampling from multimodal distributions. MC3 involves running multiple Markov chains in parallel but with target distributions of different temperatures, and it swaps the states of the chains whenever a better location is found. Heated chains more readily traverse valleys in the probability landscape to propose moves to far-away peaks, while the colder chains make the local steps that explore the current peak or patch. We show that MC3 generates distances between successive samples that follow a Levy flight distribution and produce 1/f-like autocorrelations, providing a single mechanistic account of these two puzzling empirical phenomena of internal foraging.","Jianqiao Zhu*,University of Warwick;Adam Sanborn,University of Warwick;Nick Chater,Warwick Business School",j.zhu@warwick.ac.uk;a.n.sanborn@warwick.ac.uk;nick.chater@wbs.ac.uk,Neuroscience and Cognitive Science/Cognitive Science*; Neuroscience and Cognitive Science/Reasoning,Poster
2782,Adversarially Robust Optimization with Gaussian Processes,"In this paper, we consider the problem of Gaussian process (GP) optimization with an added robustness requirement: The returned point may be perturbed by an adversary, and we require the function value to remain as high as possible even after this perturbation. This problem is motivated by settings in which the underlying functions during optimization and implementation stages are different, or when one is interested in finding an entire region of good inputs rather than only a single point.  We show that standard GP optimization algorithms do not exhibit the desired robustness properties, and provide a novel confidence-bound based algorithm StableOpt for this purpose.  We rigorously establish the required number of samples for StableOpt to find a near-optimal point, and we complement this guarantee with an algorithm-independent lower bound.  We experimentally demonstrate several potential applications of interest using real-world data sets, and we show that StableOpt consistently succeeds in finding a stable maximizer where several baseline methods fail.","Ilija Bogunovic*,EPFL Lausanne;Jonathan Scarlett,National University of Singapore;Stefanie  Jegelka,MIT;Volkan Cevher,EPFL",ilija.bogunovic@epfl.ch;scarlett@comp.nus.edu.sg;stefje@csail.mit.edu;volkan.cevher@epfl.ch,Probabilistic Methods/Gaussian Processes*; Algorithms/Bandit Algorithms,Spotlight
2784,Learning to Multitask,"Multitask learning has shown promising performance in many applications and many multitask models have been proposed. In order to identify an effective multitask model for a given multitask problem, we propose a learning framework called learning to multitask (L2MT). To achieve the goal, L2MT exploits historical multitask experience which is organized as a training set consists of several tuples, each of which contains a multitask problem with multiple tasks, a multitask model and the relative test error. Based on such training set, L2MT first uses a proposed layerwise graph neural network to learn task embeddings for all the tasks in a multitask problem and then learns an estimation function to estimate the relative test error based on task embeddings and the representation of the multitask model based on a unified formulation. Given a new multitask problem, the estimation function is used to identify a suitable multitask model. Experiments on benchmark datasets show the effectiveness of the proposed L2MT method.","Yu Zhang*,HKUST;Ying Wei,Tencent AI Lab;Qiang Yang,Hong Kong UST",yu.zhang.ust@gmail.com;judywei@tencent.com;qyang@cse.ust.hk,Algorithms/Multitask and Transfer Learning,Poster
2786,Loss Functions for Multiset Prediction,"We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.","Sean Welleck*,New York University Shanghai;Zixin Yao,New York University;Yu Gai,New York University;Jialin Mao,New York University;Zheng Zhang,New York University Shanghai;Kyunghyun Cho,New York University",wellecks@nyu.edu;zixin.yao@nyu.edu;yg1246@nyu.edu;jialin.mao@nyu.edu;zz@nyu.edu;kyunghyun.cho@nyu.edu,Algorithms/Structured Prediction*; Algorithms/Classification; Deep Learning/Recurrent Networks,Poster
2789,Computing Kantorovich-Wasserstein Distances on $d$-dimensional histograms using $(d+1)$-partite graphs,"This paper presents a novel method to compute the exact Kantorovich-Wasserstein distance between a pair of $d$-dimensional histograms having $n$ bins each. We prove that this problem is equivalent to an uncapacitated minimum cost flow problem on a $(d+1)$-partite graph with $(d+1)n$ nodes and $dn^{\frac{d+1}{d}}$ arcs, whenever the cost is separable along the principal $d$-dimensional directions. We show numerically the benefits of our approach by computing the Kantorovich-Wasserstein distance of order 2 among two sets of instances: gray scale images and $d$-dimensional biomedical histograms. On these types of instances, our approach is competitive with state-of-the-art optimal transport algorithms.","Gennaro Auricchio*,University of Pavia;Federico Bassetti,Politecnico di Milano;Stefano Gualandi,University of Pavia;Marco Veneroni,University of Pavia",gennaro.auricchio01@universitadipavia.it;federico.bassetti@polimi.it;stefano.gualandi@unipv.it;marco.veneroni@unipv.it,Optimization/Combinatorial Optimization*; Applications/Computer Vision,Poster
2797,Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability,"Neural networks are known to model statistical interactions, but they entangle the interactions at intermediate hidden layers for shared representation learning. We propose a framework, DI, that Disentangles Interactions by  counteracting the shared learning across different interactions to obtain their intrinsic lower-order and interpretable structure. This is done through a novel regularizer that directly penalizes interaction order. We show that disentangling interactions reduces a feedforward neural network to a generalized additive model with interactions, which can lead to transparent models that perform comparably to the state-of-the-art models. DI is also flexible and efficient; it can learn generalized additive models with maximum K-order interactions by training only $O(1)$ models.","Michael Tsang*,University of Southern California;Hanpeng Liu,USC;Sanjay Purushotham,University of Southern California;Pavankumar Murali,IBM;Yan Liu,USC",tsangm@usc.edu;hanpengl@usc.edu;spurusho@usc.edu;pavanm@us.ibm.com;yanliu.cs@usc.edu,Deep Learning/Visualization or Exposition Techniques for Deep Networks,Poster
2815,CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces,"In this paper, we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end, we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes.  We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace, and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class.  With low dimensionality of capsule subspace as well as an iterative method to estimate the matrix inverse, only a small negligible computing overhead is incurred to train the network. Experiment results on image datasets show the presented network can greatly improve the performance of state-of-the-art Resnet backbones by $10-20\%$ with almost the same computing cost.","Liheng Zhang,University of Central Florida;Marzieh Edraki,University of Central Florida;Guo-Jun Qi*,University of Central Florida",lihengzhang1993@knights.ucf.edu;m.edraki@knights.ucf.edu;guojunq@gmail.com,Deep Learning/CNN Architectures*; Algorithms/Classification,Poster
2816,Gamma-Poisson Dynamic Matrix Factorization Embedded with Metadata Influence,"A conjugate Gamma-Poisson model for Dynamic Matrix Factorization incorporated with metadata influence (mGDMF for short) is proposed to effectively and efficiently model massive, sparse and dynamic data in recommendations. Modeling recommendation problems with a massive amount of ratings and very sparse or even no ratings on some users/items in a dynamic setting is very demanding and poses critical challenges for well-studied matrix factorization models due to the large-scale, sparse and dynamic nature of the data. Our proposed mGDMF tackles these challenges by introducing three strategies: (1) constructing a stable Gamma-Markov chain model that smoothly drifts over time by combining both static and dynamic latent features of data; (2) incorporating the user/item metadata into the model to tackle sparse ratings; and (3) undertaking stochastic variational inference to  efficiently handle massive data. mGDMF is conjugate, dynamic and scalable. Experiments show that mGDMF significantly (both effectively and efficiently) outperforms the state-of-the-art static and dynamic models on large, sparse and dynamic data.","Trong Dinh Thac Do*,University of Technology Sydney;Longbing Cao,University of Technology Sydney",thacdtd@gmail.com;longbing.cao@uts.edu.au,Applications/Recommender Systems*; Algorithms/Collaborative Filtering; Applications/Matrix and Tensor Factorization; Probabilisti,Poster
2820,Masking: A New Perspective of Noisy Supervision,"It is important to learn classifiers under noisy labels due to their ubiquities. As noisy labels are corrupted from ground-truth labels by an unknown noise transition matrix, the accuracy of classifiers can be improved by estimating this matrix, without introducing either sample-selection or regularization biases. However, such estimation is often inexact, which inevitably degenerates the accuracy of classifiers. The inexact estimation is due to either a heuristic trick, or the brutal-force learning by deep networks under a finite dataset. In this paper, we present a human-assisted approach called ``\textit{masking}''. The masking conveys human cognition of invalid class transitions, and naturally speculates the structure of the noise transition matrix. Given the structure information, we only learn the sparse noise transition probability to reduce the estimation burden. To instantiate this approach, we derive a structure-aware probabilistic model, which incorporates a structure prior. During the model realization, we solve the challenges from structure extraction and alignment in principle. Empirical results on benchmark datasets with three noise structures show that, our approach can improve the robustness of classifiers significantly.","Bo Han*,RIKEN / University of Technology Sydney;Jiangchao Yao,Cooperative Medianet Innovation Center, Shang hai Jiao Tong University;Gang Niu,RIKEN;Mingyuan Zhou,University of Texas at Austin;Ivor Tsang,University of Technology Sydney;Ya Zhang,Cooperative Medianet Innovation Center, Shang hai Jiao Tong University;Masashi Sugiyama,RIKEN/The University of Tokyo",bo.han@riken.jp;sunarker@sjtu.edu.cn;gang.niu@riken.jp;Mingyuan.Zhou@mccombs.utexas.edu;ivor.tsang@uts.edu.au;ya_zhang@sjtu.edu.cn;sugi@k.u-tokyo.ac.jp,Algorithms/Semi-Supervised Learning*; Algorithms/Classification,Poster
2822,On GANs and GMMs,"A longstanding problem in machine learning is to find unsupervised methods that can learn the statistical structure of high dimensional signals. In recent years, GANs have gained much attention as a possible solution to the problem, and in particular have shown the ability to generate remarkably realistic high resolution sampled images. At the same time, many authors have pointed out that GANs may fail to model the full distribution (""mode collapse"") and that using the learned models for anything other than generating samples may be very difficult.
In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the  Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require  approximating intractable computations.  Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to  train them on full sized images despite the high dimensionality. Our results show that  GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution which  GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. 
Finally, we discuss how a pix2pix network can be used to add high-resolution details to GMM samples while maintaining the basic diversity.","Eitan Richardson*,The Hebrew University;Yair Weiss,Hebrew University",eitan.richardson@gmail.com;yweiss@cs.huji.ac.il,Deep Learning/Generative Models*; Deep Learning/Adversarial Networks; Probabilistic Methods/Latent Variable Models,Spotlight
2825,Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance,"Applications of optimal transport have recently gained remarkable attention as a result of the computational advantages of entropic regularization. However, in most situations the  Sinkhorn approximation to the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn distance, proving that it enjoys the same smoothness of its regularized version and we explicitly provide an efficient algorithm to compute its gradient. We show that this result benefits both theory and applications: on one hand, high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand, the gradient formula allows to efficiently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis. ","Giulia Luise*,University College London;Alessandro Rudi,INRIA, Ecole Normale Superieure;Massimiliano Pontil,IIT;Carlo Ciliberto,",g.luise.16@ucl.ac.uk;alessandro.rudi@inria.fr;massimiliano.pontil@gmail.com;cciliber@gmail.com,Algorithms/Structured Prediction*; Algorithms/Kernel Methods; Theory/Learning Theory,Poster
2833,Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching,"End-to-end deep-learning networks recently demonstrated extremely good performance for stereo matching. However, existing networks are difficult to use for practical applications since (1) they are memory-hungry and unable to process even modest-size images, (2) they have to be fully re-trained to handle a different disparity range.

The Practical Deep Stereo (PDS) network that we propose addresses both issues: First, its architecture relies on novel bottleneck modules that drastically reduce the memory footprint in inference, and additional design choices allow to handle greater image size during training. This results in a model that leverages large image context to resolve matching ambiguities. Second, a novel sub-pixel cross-entropy loss combined with a MAP estimator make this network less sensitive to ambiguous matches, and applicable to any disparity range without re-training.

We compare PDS to state-of-the-art methods published over the recent months, and demonstrate its superior performance on FlyingThings3D and KITTI sets.","Stepan Tulyakov*,;Anton Ivanov,EPFL;Francois Fleuret,Idiap Research Institute",stepan.tulyakov@epfl.ch;anton.ivanov@epfl.ch;francois.fleuret@idiap.ch,Deep Learning*; Applications/Computer Vision; Deep Learning/CNN Architectures; Deep Learning/Supervised Dee,Poster
2835,A Bayes-Sard Cubature Method,"This paper focusses on the formulation of numerical integration as an inferential task. To date, research effort has largely focussed on the development of Bayesian cubature, whose distributional output provides uncertainty quantification for the integral. However, the point estimators associated to Bayesian cubature can be inaccurate and acutely sensitive to the prior when the domain is high-dimensional. To address these drawbacks we introduce Bayes‚ÄìSard cubature, a probabilistic framework that combines the flexibility of Bayesian cubature with the robustness of classical cubatures which are well-established. This is achieved by considering a Gaussian process model for the integrand whose mean is a parametric regression model, with an improper flat prior on each regression coefficient. The features in the regression model consist of test functions which are guaranteed to be exactly integrated, with remaining degrees of freedom afforded to the non-parametric part. The asymptotic convergence of the Bayes‚ÄìSard cubature method is established and the theoretical results are numerically verified. In particular, we report two orders of magnitude reduction in error compared to Bayesian cubature in the context of a high-dimensional financial integral.","Toni Karvonen*,Aalto University;Chris Oates,Newcastle;Simo Saerkkae,Aalto University",toni.karvonen@aalto.fi;chris.oates@newcastle.ac.uk;simo.sarkka@aalto.fi,Probabilistic Methods*; Probabilistic Methods/Bayesian Nonparametrics; Probabilistic Methods/Gaussian Processes,Poster
2847,Dual Swap Disentangling,"Learning interpretable disentangled representations is a crucial yet challenging task. In this paper, we propose a weakly semi-supervised method, termed as \emph{Dual Swap Disentangling~(DSD)}, for disentangling using both labeled and unlabeled data. Unlike conventional weakly supervised methods that rely on full annotations on the group of samples, we require only limited annotations on paired samples that indicate their shared attribute like the color. Our model takes the form of a dual autoencoder structure. To achieve disentangling using the labeled pairs, we follow a ``encoding-swap-decoding'' process, where we first swap the parts of their encodings corresponding to the shared attribute, and then decode the obtained hybrid codes to reconstruct the original input pairs. For unlabeled pairs, we follow the ``encoding-swap-decoding'' process twice on designated encoding parts and enforce the final outputs to approximate the input pairs. By isolating parts of the encoding and swapping them back and forth, we impose the dimension-wise modularity and portability of the encodings of the unlabeled samples, which implicitly encourages disentangling under the guidance of labeled pairs. This dual swap mechanism, tailored for semi-supervised setting, turns out to be very effective. Experiments on image datasets from a wide domain show that our model yields state-of-the-art disentangling performances.","Zunlei Feng*,Zhejiang University;Xinchao Wang,Stevens Institute of Technology;Chenglong Ke,Zhejiang University;An-Xiang Zeng,Alibaba;Dacheng Tao,University of Sydney;Mingli Song,Zhejiang University",zunleifeng@zju.edu.cn;xinchao.wang@stevens.edu;chenglongke@zju.edu.cn;renzhong@taobao.com;dacheng.tao@sydney.edu.au;brooksong@zju.edu.cn,Algorithms/Representation Learning*; Algorithms/Semi-Supervised Learning; Deep Learning/Deep Autoencoders; Deep Learning/Embeddi,Poster
2854,Diverse Ensemble Evolution: Curriculum based Data-Model Marriage,"We study how to train an ensemble of models based on their changing diversity requirements and expertise. Previous ensemble methods usually determine diversity before training starts, either by resampling training data or via random initialization. Instead, we propose ``Diverse Ensemble Evolution (DivE$^2$),'' a method that assigns data to models at each training epoch, based on their capabilities and a scheduled diversity requirement. DivE$^2$ starts with selecting easy-to-learn samples for every model, and slowly moves to selecting models with accurate predictions for a data sample. To expand the realm of expertise for each model while enforcing diversity over all models, we propose an intra-model diversity term on data assigned to each model, and an inter-model diversity term to penalize redundancy over data assigned to different models. We formulate such data assignment problem as a generalized bipartite matching problem with two partition matroid constraints. DivE$^2$ solves a sequence of continuous-combinatorial optimizations with slowly varying objectives and constraints. The combinatorial part handles the data assignment with submodular maximization, and the continuous part updates model based on the assigned data. In experiments, DivE$^2$ outperforms other ensemble training methods under several inference techniques while maintaining competitive efficiency.","Tianyi Zhou*,University of Washington;Shengjie Wang,""University of Washington, Seattle"";Jeff Bilmes,UW",tianyi.david.zhou@gmail.com;wsjloomise@gmail.com;bilmes@uw.edu,Algorithms/Boosting and Ensemble Methods*; Deep Learning; Deep Learning/Meta-Learning; Optimization/Submodular Optimization,Poster
2861,Binary Classification from Positive-Confidence Data,"Reducing labeling costs in supervised learning is a critical issue in many practical machine learning applications.  In this paper, we consider positive-confidence (Pconf) classification, the problem of training a binary classifier only from positive data equipped with confidence.  Pconf classification can be regarded as a discriminative extension of one-class classification (which is aimed at ``describing'' the positive class by clustering-related methods), with ability to tune hyper-parameters for ``classifying'' positive and negative samples.  Pconf classification is also related to positive-unlabeled (PU) classification (which uses hard-labeled positive data and unlabeled data), but the difference is that it enables us to avoid estimating the class priors, which is a critical bottleneck in typical PU classification methods.  For the Pconf classification problem, we provide a simple empirical risk minimization framework and give a formulation for linear-in-parameter models that can be implemented easily and computationally efficiently.  We also theoretically establish the consistency and an estimation error bound for Pconf classification, and demonstrate the practical usefulness of the proposed method for deep neural networks through experiments.","Takashi Ishida*,University of Tokyo / RIKEN;Gang Niu,RIKEN;Masashi Sugiyama,RIKEN/The University of Tokyo",ishida@ms.k.u-tokyo.ac.jp;gang.niu@riken.jp;sugi@k.u-tokyo.ac.jp,Algorithms/Semi-Supervised Learning,Spotlight
2866,Deep Generative Models for Distribution-Preserving Lossy Compression,"We propose and study the problem of distribution-preserving lossy compression. Motivated by the recent advances in extreme image compression which allow to maintain artifact-free reconstructions even at very low bitrates, we propose to optimize the rate-distortion tradeoff under the constraint that the reconstructed samples follow the distribution of the training data. Such a compression system recovers both ends of the spectrum: On one hand, at zero bitrate it learns a generative model of the data, and at high enough bitrates it achieves perfect reconstruction. Furthermore, for intermediate bitrates it smoothly interpolates between matching the distribution of the training data and perfectly reconstructing the training samples. We study several methods to approximately solve the proposed optimization problem, including a novel combination of Wasserstein GAN and Wasserstein Autoencoder, and present strong theoretical and empirical results for the proposed compression system.","Michael Tschannen*,ETH Zurich;Eirikur Agustsson,ETH Zurich;Mario Lucic,Google Brain",michaelt@nari.ee.ethz.ch;aeirikur@vision.ee.ethz.ch;lucic@google.com,Deep Learning/Generative Models*; Algorithms/Representation Learning; Deep Learning/Adversarial Networks,Poster
2889,Exact natural gradient in deep linear networks and its application to the nonlinear case,"Stochastic gradient descent (SGD) remains the method of choice for deep learning, despite the limitations arising for ill-behaved objective functions.
In cases where it could be estimated, the natural gradient has proven very effective at mitigating the catastrophic effects of pathological curvature in the objective function, but little is known theoretically about its convergence properties, and it has yet to find a practical implementation that would scale to very deep and large networks.
Here, we derive an exact expression for the natural gradient in deep linear networks, which exhibit pathological curvature similar to the nonlinear case, and provide for the first time an analytical solution for its convergence rate, showing that natural gradient descent converges exponentially fast to the global minimum in parameter space.
Our expression for the natural gradient is surprisingly simple, computationally tractable, and explains why some approximations proposed previously work well in practice.
This opens new avenues for approximating the natural gradient in the nonlinear case, and we show that, promisingly, our online natural gradient descent outperforms SGD for MNIST autoencoders while sharing its computational simplicity.","Alberto Bernacchia*,University of Cambridge;M√°t√© Lengyel,University of Cambridge;Guillaume Hennequin,Cambridge",ab2349@cam.ac.uk;m.lengyel@eng.cam.ac.uk;g.hennequin@eng.cam.ac.uk,Deep Learning/Optimization for Deep Networks*; Deep Learning/Biologically Plausible Deep Networks; Deep Learning/Efficient Training Methods;,Poster
2893,Constructing Fast Network through Deconstruction of Convolution,"Convolutional neural networks have achieved great success in various vision tasks; however, they incur heavy resource costs. By using deeper and wider networks, network accuracy can be improved rapidly. However, in an environment with limited resources (e.g., mobile applications), heavy networks may not be usable. This study shows that naive convolution can be deconstructed into a shift operation and pointwise convolution. To cope with various convolutions, we propose a new shift operation called active shift layer (ASL) that formulates the amount of shift as a learnable function with shift parameters. This new layer can be optimized end-to-end through backpropagation and it can provide optimal shift values. Finally, we apply this layer to a light and fast network that surpasses existing state-of-the-art networks.","Yunho Jeon*,KAIST;Junmo Kim,KAIST",jyh2986@kaist.ac.kr;junmo.kim@kaist.ac.kr,Deep Learning/CNN Architectures*; Algorithms/Classification; Deep Learning/Optimization for Deep Networks; Deep Learning/Supe,Spotlight
2901,Memory Replay GANs: Learning to Generate New Categories without Forgetting,"Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories.","Chenshen Wu*,Computer Vision Center;Luis Herranz,Computer Vision Center (Ph.D.);Xialei Liu,Computer Vision Center Barcelona;yaxing wang,Computer Vision Center;Joost van de Weijer,Computer Vision Center;BOGDAN RADUCANU,Computer Version Center, Edifici",chenshen@cvc.uab.es;lherranz@cvc.uab.es;xialei@cvc.uab.es;yaxing@cvc.uab.es;joost@cvc.uab.es;bogdan@cvc.uab.es,Deep Learning/Adversarial Networks*; Applications/Computer Vision,Poster
2905,The Convergence of Sparsified Gradient Methods,"Distributed training of massive machine learning models, in particular deep neural networks, via Stochastic Gradient Descent (SGD) is becoming commonplace. Several families of communication-reduction methods, such as quantization, large-batch methods, and gradient sparsification, have been proposed. To date, gradient sparsification methods--where each node sorts gradients by magnitude, and only communicates a subset of the components, accumulating the rest locally--are known to yield some of the largest practical gains. Such methods can reduce the amount of communication per step by up to \emph{three orders of magnitude}, while preserving model accuracy. Yet, this family of methods currently has no theoretical justification. 

This is the question we address in this paper. 
We prove that, under analytic assumptions, sparsifying gradients by magnitude with local error correction provides convergence guarantees, for both convex and non-convex smooth objectives, for data-parallel SGD. 
The main insight is that sparsification methods implicitly maintain bounds on the maximum impact of stale updates, thanks to selection by magnitude. 
Our analysis and empirical validation also reveal that these methods do require analytical conditions to converge well, justifying existing heuristics. ","Dan Alistarh*,IST Austria & NeuralMagic;Torsten Hoefler,ETH Z√ºrich;Mikael Johansson,KTH Royal Institute of Technology;Nikola Konstantinov,IST Austria;Sarit Khirirat,KTH;Cedric Renggli,ETH Zurich",dan.alistarh@ist.ac.at;torsten.hoefler@inf.ethz.ch;mikaelj@kth.se;nikola.konstantinov@ist.ac.at;sarit@kth.se;cedric.renggli@inf.ethz.ch,Algorithms*; Algorithms/Stochastic Methods; Applications/Communication- or Memory-Bounded Learning; Optimiz,Poster
2927,Automating Bayesian optimization with Bayesian optimization,"Bayesian optimization is a powerful tool for global optimization of expensive functions. One of its key components is the underlying probabilistic model used for the objective function f. In practice, however, it is often unclear how one should appropriately choose a model, especially when gathering data is expensive. 
In this work, we introduce a novel automated Bayesian optimization approach that dynamically selects promising models for explaining the observed data using Bayesian Optimization in the model space. Crucially, we account for the uncertainty in the model choice; our method is capable of using multiple models to represent its current belief about f and subsequently using this information for decision making. We argue, and demonstrate empirically, that our approach automatically finds suitable models for the objective function, which ultimately results in more-efficient optimization.","Gustavo Malkomes*,Washington University in St. Louis;Roman Garnett,-",luizgustavo@wustl.edu;garnett@wustl.edu,Probabilistic Methods/Gaussian Processes*; Algorithms/AutoML,Poster
2928,Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning,"Zero-Shot Learning (ZSL) is generally achieved via aligning the semantic relationships between the visual features and the corresponding class semantic descriptions. However, using the global features to represent fine-grained images may lead to sub-optimal results since they neglect the discriminative differences of local regions. Besides, different regions contain distinct discriminative information. The important regions should contribute more to the prediction. To this end, we propose a novel stacked semantics-guided attention (S2GA) model to obtain semantic relevant features by using individual class semantic features to progressively guide the visual features to generate an attention map for weighting the importance of different local regions. Feeding both the integrated visual features and the class semantic features into a multi-class classification architecture, the proposed framework can be trained end-to-end. Extensive experimental results on CUB and NABird datasets show that the proposed approach has a consistent improvement on both fine-grained zero-shot classification and retrieval tasks.","Yunlong Yu,Tianjing University;Zhong Ji*,Tianjin University;Yanwei Fu,Fudan Univertiy; AItrics Inc.;Jichang Guo,Tianjin University;Yanwei Pang,Tianjing University;Zhongfei Zhang,Binghamton University",yuyunlong@tju.edu.cn;jizhong@tju.edu.cn;yanweifu@fudan.edu.cn;jcguo@tju.edu.cn;pyw@tju.edu.cn;zhongfei@cs.binghamton.edu,Applications/Visual Features*; Applications/Object Recognition; Deep Learning/Attention Models,Poster
2935,Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification,"In this paper, we study the problem of deriving fast and accurate classification algorithms with uncertainty quantification. Gaussian process classification provides a principled approach, but the corresponding computational burden is hardly sustainable in large-scale problems and devising efficient alternatives is a challenge. In this work, we investigate if and how Gaussian process regression directly applied to the classification labels  can be used to tackle this question. While in this case training time is remarkably  faster, predictions need be calibrated for classification and uncertainty estimation. To this aim, we propose a novel approach based on  interpreting  the labels as the output of a Dirichlet distribution. Extensive experimental results show  that the proposed approach provides  essentially the same accuracy and uncertainty quantification of  Gaussian process classification  while requiring only a fraction of computational resources.","Dimitrios Milios*,EURECOM;Raffaello Camoriano,Istituto Italiano di Tecnologia;Pietro Michiardi,EURECOM;Lorenzo Rosasco,;Maurizio Filippone,EURECOM",milios@eurecom.fr;raffaello.camoriano@iit.it;Pietro.Michiardi@eurecom.fr;lrosasco@mit.edu;maurizio.filippone@eurecom.fr,Probabilistic Methods/Gaussian Processes*; Algorithms/Classification; Algorithms/Kernel Methods,Poster
2947,Multi-Task Zipping via Layer-wise Neuron Sharing,"Future mobile devices are anticipated to perceive, understand and react to the world on their own by running multiple correlated deep neural networks on-device. Yet the complexity of these neural networks needs to be trimmed down both within-model and cross-model to fit in mobile storage and memory. Previous studies focus on squeezing the redundancy within a single neural network. In this work, we aim to reduce the redundancy across multiple models. We propose Multi-Task Zipping (MTZ), a framework to automatically merge correlated, pre-trained deep neural networks for cross-model compression. Central in MTZ is a layer-wise neuron sharing and incoming weight updating scheme that induces a minimal change in the error function. MTZ inherits information from each model and demands light retraining to re-boost the accuracy of individual tasks. Evaluations show that MTZ is able to fully merge the hidden layers of two VGG-16 networks with a 3.18% increase in the test error averaged on ImageNet and CelebA, or share 39.61% parameters between the two networks with <0.5% increase in the test errors for both tasks. The number of iterations to retrain the combined network is at least 17.8 times lower than that of training a single VGG-16 network.","Xiaoxi He*,ETH Z√ºrich;Zimu Zhou,ETH Zurich;Lothar Thiele,ETH Z√ºrich",hex@ethz.ch;zzhou@tik.ee.ethz.ch;thiele@ethz.ch,Deep Learning*; Deep Learning/Optimization for Deep Networks,Poster
2949,Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo,"Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-dimensional  distributions in  Statistics and Machine learning. HMC is known to run very efficiently in practice and its popular second-order ``leapfrog"" implementation has long been conjectured to run in $d^{1/4}$ steps.
Here we  show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data. Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster running time bounds for a  much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone.  Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an ``incoherence"" property. Our result  compares favorably with the best available running time bounds for the class of strongly log-concave distributions, which grow like $d^{1/2}$ with the dimension.  Moreover, our simulations on synthetic data suggest that, when our regularity condition is satisfied, the leapfrog HMC performs better than its competitors -- both in terms of accuracy and running time. ","Oren Mangoubi*,EPFL;Nisheeth Vishnoi,EPFL",omangoubi@gmail.com;nisheeth.vishnoi@epfl.ch,Probabilistic Methods/MCMC*; Algorithms/Stochastic Methods,Poster
2961,Approximation algorithms for stochastic clustering,"We consider stochastic settings for clustering, and develop provably-good (approximation) algorithms for a number of these notions. These algorithms allow one to obtain better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including providing fairer clustering and clustering which has better long-term behavior for each user. In particular, they ensure that *every user* is guaranteed to get good service (on average). We also complement some of these with impossibility results. ","David Harris*,University of Maryland;Shi Li,University at Buffalo;Aravind Srinivasan,University of Maryland College Park;Khoa Trinh,;Thomas Pensyl,",davidgharris29@gmail.com;shil@buffalo.edu;srin@cs.umd.edu;khoa@cs.umd.edu;tpensyl@cs.umd.edu,"Applications/Fairness, Accountability, and Transparency*; Optimization/Combinatorial Optimization; Theory/Computational Complexity",Poster
2968,Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks,"We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD) framework for optimizing deep neural networks. ESGD combines SGD and gradient-free evolutionary algorithms as complementary algorithms in one framework in which the optimization alternates between the SGD step and evolution step to improve the average fitness of the population. With a back-off strategy in the SGD step and an elitist strategy in the evolution step, it guarantees that the best fitness in the population will never degrade. In addition, various SGD-based optimizers with distinct hyper-parameters are considered in the SGD step as competing species in a coevolution setting such that their complementarity is also taken into account. The effectiveness of ESGD is demonstrated across multiple applications including speech recognition, image recognition and language modeling using networks with a variety of deep architectures.","Xiaodong Cui*,IBM T. J. Watson Research Center;Wei Zhang,IBM T.J. Watson;Zoltan Tuske,IBM T. J. Watson Research Center;Michael Picheny,IBM T. J. Watson Research Center",cuix@us.ibm.com;weiz@us.ibm.com;Zoltan.Tuske@ibm.com;picheny@us.ibm.com,Deep Learning*; Deep Learning/Biologically Plausible Deep Networks; Deep Learning/Meta-Learning; Deep Learning,Poster
2974,Learning to Infer Graphics Programs from Hand-Drawn Images,"  We introduce a model that learns to convert simple hand drawings
  into graphics programs written in a subset of \LaTeX.~The model
  combines techniques from deep learning and program synthesis.  We
  learn a convolutional neural network that proposes plausible drawing
  primitives that explain an image. These drawing primitives are a
  specification (spec) of what the graphics program needs to draw.  We
  learn a model that uses program synthesis techniques to recover a
  graphics program from that spec. These programs have constructs like
  variable bindings, iterative loops, or simple kinds of
  conditionals. With a graphics program in hand, we can correct errors
  made by the deep network and extrapolate drawings.","Kevin Ellis*,MIT;Daniel Ritchie,Brown University;Armando Solar-Lezama,MIT;Joshua Tenenbaum,MIT",ellisk@mit.edu;daniel_ritchie@brown.edu;asolar@csail.mit.edu;jbt@mit.edu,Deep Learning/Program Induction,Spotlight
2991,Graphical Generative Adversarial Networks,"We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.","Chongxuan Li*,Tsinghua University;Max Welling,University of Amsterdam;Jun Zhu,Tsinghua University;Bo Zhang,Tsinghua University",chongxuanli1991@gmail.com;welling.max@gmail.com;dcszj@mail.tsinghua.edu.cn;dcszb@mail.tsinghua.edu.cn,Deep Learning/Adversarial Networks*; Algorithms/Unsupervised Learning; Deep Learning/Generative Models,Poster
2992,Variational Learning on Aggregate Outputs with Gaussian Processes,"While a typical supervised learning framework assumes that the inputs and the outputs are measured at the same levels of granularity, many applications, including global mapping of disease, only have access to outputs at a much coarser level than that of the inputs. Aggregation of outputs makes generalization to new inputs much more difficult. We consider an approach to this problem based on variational learning with a model of output aggregation and Gaussian processes, where aggregation leads to intractability of the standard evidence lower bounds. We propose new bounds and tractable approximations, leading to improved prediction accuracy and scalability to large datasets, while explicitly taking uncertainty into account. We develop a framework which extends to several types of likelihoods, including the Poisson model for aggregated count data. We apply our framework to a challenging and important problem, the fine-scale spatial modelling of malaria incidence, with over 1 million observations.","Ho Chung Leon Law*,University of Oxford;Dino Sejdinovic,University of Oxford;Ewan Cameron,;Tim  Lucas,University of Oxford;Seth Flaxman,Imperial College London;Katherine  Battle,University of Oxford;Kenji Fukumizu,The Institute of Statistical Mathematics",ho.law@spc.ox.ac.uk;dino.sejdinovic@stats.ox.ac.uk;dr.ewan.cameron@gmail.com;timcdlucas@gmail.com;s.flaxman@imperial.ac.uk;katherine.battle@bdi.ox.ac.uk;fukumizu@ism.ac.jp,Probabilistic Methods/Gaussian Processes*; Algorithms/Semi-Supervised Learning,Poster
3001,MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models,"Machine Comprehension (MC) is one of the core problems in natural language processing, requiring both understanding of natural language and knowledge about the world. Rapid progress has been made since the release of several benchmark datasets, and recently the state-of-the-art models even surpass human performance on the well-known SQuAD evaluation. In this paper, we transfer knowledge learned from machine comprehension to the sequence-to-sequence tasks to deepen the understanding of the text. We propose MacNet: a novel encoder-decoder supplementary architecture to the widely used attention-based sequence-to-sequence models. Experiments on neural machine translation (NMT) and abstractive text summarization show that our proposed framework can significantly improve the performance of the baseline models, and our method for the abstractive text summarization achieves the state-of-the-art results on the Gigaword dataset.","Boyuan Pan*,Zhejiang University;Yazheng Yang,Zhejiang University;Hao Li,Zhejiang University;Zhou Zhao,Zhejiang University;Yueting Zhuang,Zhejiang University;Deng Cai,Zhejiang University;Xiaofei He,Zhejiang University",panby@zju.edu.cn;yazheng_yang@zju.edu.cn;haolics@zju.edu.cn;zhaozhou@zju.edu.cn;yzhuang@zju.edu.cn;dengcai@cad.zju.edu.cn;xiaofeihe@cad.zju.edu.cn,Applications/Natural Language Processing*; Applications/Text Analysis,Poster
3010,Poison Frogs! Targeted Clean-Label PoisoningAttacks on Neural Networks,"Data poisoning an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use ``clean-labels''; they don't require the attacker to have any control over the labeling of training data.  They are also targeted; they control the behavior of the classifier on a specific test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by putting them online and waiting for them to be scraped by a data collection bot.

We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a ``watermarking'' strategy that makes poisoning reliable using multiple (approx. 50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.","Ali Shafahi*,University of Maryland;W. Ronny Huang,University of Maryland;Mahyar Najibi,University of Maryland;Octavian Suciu,University of Maryland;Christoph Studer,Nil;Tudor Dumitras,University of Maryland;Tom Goldstein,University of Maryland, College Park",ashafahi@cs.umd.edu;wronnyhuang@gmail.com;najibi@cs.umd.edu;osuciu@umiacs.umd.edu;studer@cornell.edu;tdumitra@umiacs.umd.edu;tomg@cs.umd.edu,Deep Learning*; Applications/Computer Vision; Deep Learning/Adversarial Networks; Deep Learning/Visualizati,Poster
3018,Information Constraints on Auto-Encoding Variational Bayes,"Parameterizing the approximate posterior of a generative model with neural networks has become a common theme in recent machine learning research. While providing appealing flexibility, this approach makes it difficult to impose or assess structural constraints such as conditional independence. We propose a framework for learning representations that relies on Auto-Encoding Variational Bayes and whose search space is constrained via kernel-based measures of independence.  In particular, our method employs the $d$-variable Hilbert-Schmidt Independence Criterion (dHSIC) to enforce independence between the latent representations and arbitrary nuisance factors.
We show how to apply this method to a range of problems, including the problems of learning invariant representations and the learning of interpretable representations. We also present a full-fledged application to single-cell RNA sequencing (scRNA-seq). In this setting the biological signal in mixed in complex ways with sequencing errors and sampling effects.  We show that our method out-performs the state-of-the-art in this domain.","Romain Lopez*,UC Berkeley;Jeffrey Regier,UC Berkeley;Michael Jordan,UC Berkeley;Nir Yosef,UC Berkeley",romain_lopez@berkeley.edu;regier@berkeley.edu;jordan@cs.berkeley.edu;niryosef@berkeley.edu,Deep Learning/Generative Models*; Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning; Algorithms/Unsupervised L,Poster
3020,Recurrent Transformer Networks for Semantic Correspondence,"We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.","Seungryong Kim*,Yonsei University;Stephen Lin,Microsoft Research;Sangryul Jeon,Yonsei university;Dongbo Min,Ewha Womans University;Kwanghoon Sohn ,Yonsei Univ.",srkim89@yonsei.ac.kr;stevelin@microsoft.com;cheonjsr@yonsei.ac.kr;dbmin@ewha.ac.kr;khsohn@yonsei.ac.kr,Applications/Computer Vision*; Applications/Visual Features,Spotlight
3021,Online convex optimization for cumulative constraints,"  We propose the algorithms for online convex
  optimization which lead to cumulative squared constraint violations
  of the form
  $\sum\limits_{t=1}^T\big([g(x_t)]_+\big)^2=O(T^{1-\beta})$, where
  $\beta\in(0,1)$
  .  Previous literature has
  focused on long-term constraints of the form
  $\sum\limits_{t=1}^Tg(x_t)$. There, strictly feasible solutions
  can cancel out the effects of violated constraints.
  In contrast, the new form heavily penalizes large constraint
  violations and cancellation effects cannot occur. 
  Furthermore, useful bounds on the single step constraint violation
  $[g(x_t)]_+$ are derived.
  For convex objectives, our regret bounds generalize
  existing bounds, and for strongly convex objectives we give improved
  regret bounds.
  In numerical experiments, we show that our algorithm closely follows
  the constraint boundary leading to low cumulative violation. ","Jianjun Yuan,University of Minnesota;Andrew Lamperski*,University of Minnesota",yuanx270@umn.edu;alampers@umn.edu,Algorithms/Online Learning*; Optimization/Convex Optimization,Poster
3023,Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer,"In many machine learning applications, there are multiple decision-makers involved, both automated and human.
The interaction between these agents often goes unaddressed in algorithmic development.
In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker.
The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning.
We extend this concept by proposing learning to defer, which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers
in a system. Experiments demonstrate that learning
to defer cam make systems not only more accurate but also less biased. Even when
working with inconsistent or biased users, we show that
deferring models still greatly improve the accuracy and/or fairness of the entire system.","David Madras*,University of Toronto;Toniann Pitassi,University of Toronto;Richard Zemel,University of Toronto",david.madras@mail.utoronto.ca;toni@cs.toronto.edu;zemel@cs.toronto.edu,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Classification",Poster
3031,Deep State Space Models for Unconditional Word Generation,"Autoregressive feedback is considered a necessity for successful unconditional text generation using stochastic sequence models. However, such feedback is known to introduce systematic biases into the training and it obscures a principle of generation: committing to global information and forgetting local nuances. We show that a non-autoregressive deep state space model with a clear separation of global and local uncertainty can be build from only two ingredients: An independent noise source and a deterministic transition function. Recent advances on flow-based variational inference allow training an evidence lower-bound without resorting to annealing, auxiliary losses or similar measures. The result is a highly interpretable generative model on par with a comparable auto-regressive model on the task of word generation.","Florian Schmidt*,ETHZ;Thomas Hofmann,ETH Zurich",florian.schmidt@inf.ethz.ch;thomas.hofmann@inf.ethz.ch,Probabilistic Methods/Latent Variable Models*; Applications/Natural Language Processing,Poster
3035,ResNet with one-neuron hidden layers is a Universal Approximator,"We demonstrate that a very deep ResNet with stacked modules with one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. \ell_1(\R^d). Because of the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.","Lin Hongzhou*,Inria;Stefanie  Jegelka,MIT",hongzhou.lin@inria.fr;stefje@csail.mit.edu,Deep Learning*; Theory/Learning Theory,Spotlight
3040,Transfer of Value Functions via Variational Methods,"We consider the problem of transferring value functions in reinforcement learning. We propose an approach that uses the given source tasks to learn a prior distribution over optimal value functions and provide an efficient variational approximation of the corresponding posterior in a new target task. We show our approach to be general, in the sense that it can be combined with complex parametric function approximators and distribution models, while providing two practical algorithms based on Gaussians and Gaussian mixtures. We theoretically analyze them by deriving a finite-sample analysis and provide a comprehensive empirical evaluation in four different domains.","Andrea Tirinzoni*,Politecnico di Milano;Rafael Rodriguez,Politecnico di Milano;Marcello Restelli,Politecnico di Milano",andrea.tirinzoni@polimi.it;rafaelalberto.rodriguez@mail.polimi.it;marcello.restelli@polimi.it,Reinforcement Learning and Planning/Reinforcement Learning*; Algorithms/Multitask and Transfer Learning,Poster
3042,"The Cluster Description Problem - Complexity Results, Formulations and Approximations","Consider the situation where you are given an existing $k$-way clustering $\pi$. A challenge for explainable AI is to find a compact and distinct explanations of each cluster which in this paper is using instance-level descriptors/tags from a common dictionary. Since the descriptors/tags were not given to the clustering method, this is not a semi-supervised learning situation.  We show that the \emph{feasibility} problem of just testing whether any distinct description (not the most compact) exists is generally intractable for just two clusters. This means that unless \textbf{P} = \cnp,  there cannot exist an efficient algorithm for the cluster description problem. Hence, we explore ILP formulations for smaller problems and a relaxed but restricted setting that leads to a polynomial time algorithm for larger problems.  We explore several extension to the basic setting such as the ability to ignore some instances and composition constraints on the descriptions of the clusters.  We show our formulation's usefulness on Twitter data where the communities were found using social connectivity (i.e. \texttt{follower} relation) but the explanation of the communities is based on behavioral properties of the nodes (i.e. hashtag usage) not available to the clustering method.","Ian Davidson*,UC Davis;Antoine Gourru,University of Lyon - 2;S Ravi,Biocomplexity Institute",davidson@cs.ucdavis.edu;antoine.gourru@univ-lyon2.fr;ssravi0@gmail.com,Algorithms/Clustering,Poster
3043,Sharp Bounds for Generalized Uniformity Testing,"We study the problem of generalized uniformity testing of a discrete probability distribution: Given samples from a probability distribution p over an unknown size discrete domain ‚Ñ¶, we want to distinguish, with probability at least 2/3, between the case that p is uniform on some subset of ‚Ñ¶ versus Œµ-far, in total variation distance, from any such uniform distribution. We establish tight bounds on the sample complexity of generalized uniformity testing. In more detail, we present a computationally efficient tester whose sample complexity is optimal, within constant factors, and a matching worst-case information-theoretic lower bound. Specifically, we show that the sample complexity of generalized uniformity testing is Œò(1/(Œµ^(4/3) ||p||_3) + 1/(Œµ^2 ||p||_2 )).","Ilias Diakonikolas,USC;Daniel M. Kane,UCSD;Alistair Stewart*,University of Southern California",iliasdiakonikolas@gmail.com;dakane@cs.ucsd.edu;stewart.al@gmail.com,Theory/Learning Theory*; Algorithms,Spotlight
3047,Deep Neural Networks with Box Convolutions,"Box filters computed using integral images have been part of the computer vision toolset for a long time. Here, we show that a convolutional layer that computes box filter responses in a sliding manner can be used within deep architectures, whereas the dimensions and the offsets of the sliding boxes in such a layer can be learned as part of an end-to-end loss minimization. Crucially, the training process can make the size of the boxes in such a layer arbitrarily large without incurring extra computational cost and without the need to increase the number of learnable parameters. Due to its ability to integrate information over large boxes, the new layer facilitates long-range propagation of information and leads to the efficient increase of the receptive fields of downstream units in the network. By incorporating the new layer into existing architectures for semantic segmentation, we are able to achieve both the increase in segmentation accuracy as well as the decrease in the computational cost and the number of learnable parameters.","Egor Burkov,Skoltech;Victor Lempitsky*,Skoltech",egor.burkov@skoltech.ru;lempitsky@skoltech.ru,Deep Learning/CNN Architectures*; Applications/Computer Vision; Deep Learning/Efficient Inference Methods,Poster
3053,Learning towards Minimum Hyperspherical Energy,"Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as even as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our method, by showing the superior performance with MHE regularization.","Weiyang Liu*,Georgia Tech;Rongmei Lin,MERL;Zhen Liu,Georgia Tech;Lixin Liu,SCUT;Zhiding Yu,NVIDIA;Bo Dai,Google Brain;Le Song,Ant Financial & Georgia Institute of Technology ",wyliu@gatech.edu;linrm74@gmail.com;liuzhen1994@gatech.edu;kerryliu1997@gmail.com;zhidingy@nvidia.com;bohr.dai@gmail.com;lsong@cc.gatech.edu,Applications/Computer Vision*; Deep Learning/CNN Architectures,Poster
3068,LF-Net: Learning Local Features from Images,"We present a novel strategy to learn a local feature pipeline from collections of images with deep networks, without the need for human supervision. To do so we frame the learning problem with a two-branch network. We posit that training both branches with a standard Siamese architecture is not feasible, as solving the correspondence problem jointly with feature learning is too challenging, and does not converge well enough to train from scratch. Instead, we propose to break differentiability on one branch and use ground-truth geometry to remove the burden of solving the correspondence problem while training, while keeping the other fully-differentiable. In order to train this setup with gradient-based methods, we optimize for the differentiable branch while using the parameters from the previous training step for the other, and demonstrate that both converge to a single, optimal solution. Our method can be trained with only the relative camera pose and depth information---furthermore, we show that this ground truth does not need to be perfect and can be easily obtained with off-the-shelf Structure-from-Motion solutions. Our models outperform the state of the art on sparse feature matching on  both indoor and outdoor datasets, while running at 60+ fps for QVGA images.","Yuki Ono,SONY;Eduard Trulls*,EPFL;Pascal Fua,EPFL, Switzerland;Kwang Moo Yi,University of Victoria",yuki.ono@sony.com;eduard.trulls@epfl.ch;pascal.fua@epfl.ch;kyi@uvic.ca,Applications/Computer Vision*; Applications/Visual Features; Deep Learning/CNN Architectures,Poster
3083,SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient,"We focus on the task of uncertainty estimation in deep learning models, where it is computationally challenging to even form a Gaussian approximation to the posterior. Because of this, many existing Gaussian approximations only use a diagonal covariance matrix even though such matrices are known to give poor uncertainty estimates. We propose a new stochastic, low-rank, approximate natural-gradient (SLANG) method for variational inference, which allows us to efficiently fit a non-diagonal approximation. The method estimates a ``diagonal plus low-rank'' structure based solely on back-propagated gradients of the network log-likelihood. This requires strictly less gradient computation than methods that require the gradient of the whole variational objective.
Empirical evaluations on standard benchmarks confirm that SLANG obtains reasonable posterior approximations which give comparable accuracy to the state-of-the-art.","Aaron Mishkin,University of British Columbia;Frederik Kunstner,EPFL;Didrik Nielsen,RIKEN;Mark Schmidt,University of British Columbia;Mohammad Emtiyaz Khan*,RIKEN",aaronpmishkin@gmail.com;frederik.kunstner@gmail.com;didrik.nielsen@gmail.com;schmidtm@cs.ubc.ca;emtiyaz.khan@riken.jp,Probabilistic Methods/Variational Inference*; Deep Learning/Efficient Inference Methods,Poster
3090,Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming,"The use of derivatives, especially gradients, is pervasive in machine learning, and researchers have access to a wide variety of tools to automatically compute derivatives. However, the need to efficiently calculate first- and higher-order derivatives of increasingly complex models expressed in Python has stressed or exceeded the capabilities of available tools. In this work, we show that techniques from the field of automatic differentiation can give researchers expressive power, performance and strong usability. We implement these ideas in the Tangent software library, an automatic differentiation (AD) framework for Python which uses source-code transformation (SCT) to produce derivatives of user-specified numeric code.","Bart van Merrienboer,MILA, Google;Dan Moldovan,Google Inc.;Alexander Wiltschko*,Google Inc.",bart.vanmerrienboer@gmail.com;mdan@google.com;alexbw@google.com,"Data, Competitions, Implementations, and Software/Software Toolkits*; Applications/Hardware and Systems",Poster
3092,Multi-domain Causal Structure Learning in Linear Systems,"We study causal structure learning in linear systems with observational data given in multiple domains, across which the causal coefficients, and/or the distribution of the exogenous noises may vary. Unlike previous work, we do not assume any invariance of the causal process among the domains. The main tool used in our approach is the principle that in a causally sufficient system, P(cause) and P(effect | cause), as well as their included parameters, change independently across domains. We first present our main approach to finding causal relations from multi-domain observational information, and then propose efficient tests for causal structure learning based on the introduced idea. For the case of two variables, the proposed tests are generally capable of identifying causal direction from fewer than ten domains.","AmirEmad Ghassami*,UIUC;Negar Kiyavash,University of Illinois at Urbana-Champaign;Biwei Huang,CMU;Kun Zhang,Carnegie Mellon University",ghassam2@illinois.edu;kiyavash@illinois.edu;biweih@andrew.cmu.edu;kunz1@cmu.edu,Probabilistic Methods/Causal Inference*; Algorithms/Model Selection and Structure Learning; Probabilistic Methods/Graphical Models,Poster
3097,Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences,"Differential privacy comes equipped with multiple analytical tools for the design of private data analyses. One important tool is the so called ‚Äúprivacy amplification by subsampling‚Äù principle, which ensures that a differentially private mechanism run on a random subsample of a population provides higher privacy guarantees than when run on the entire population. Several instances of this principle have been studied for different random subsampling methods, each with an ad-hoc analysis.

In this paper, we present a general method that recovers and improves prior analyses, yields lower bounds and derives new instances of privacy amplification by subsampling. Our method leverages a characterization of differential privacy as a divergence which emerged in the program verification community. Furthermore, it introduces new tools, including advanced joint convexity and privacy profiles, of independent interest.","Borja Balle*,Amazon;Gilles Barthe,IMDEA Software Institute;Marco Gaboardi,Univeristy at Buffalo",borja.balle@gmail.com;gjbarthe@gmail.com;gaboardi@buffalo.edu,"Applications/Privacy, Anonymity, and Security",Poster
3106,Exponentially Weighted Imitation Learning for Batched Historical Data,"We consider deep policy learning with only batched historical trajectories. The main challenge of this problem is that the learner no longer has a simulator or ``environment oracle'' as in most reinforcement learning settings. To solve this problem, we propose a novel exponentially weighted imitation learning strategy that is applicable to problems with complex nonlinear function approximation and works well with hybrid (discrete and continuous) action space. The method does not rely on the knowledge of the behavior policy, thus can be used to learn from data generated by an unknown policy. Under mild conditions, our algorithm, though surprisingly simple, has a policy improvement guarantee and outperforms most competing methods empirically. Thorough numerical results are also provided to demonstrate the efficacy of the proposed methodology.","Qing Wang*,Tencent AI Lab;Jiechao Xiong,Tencent AI Lab;Lei Han,;Peng Sun,Tencent AI Lab;Han Liu,Princeton University;Tong Zhang,Tecent AI Lab",wqing@pku.edu.cn;jcxiong@tencent.com;lxhan@tencent.com;pengsun000@gmail.com;hanliu@princeton.edu;tongzhang@tongzhang-ml.org,Applications/Game Playing*; Deep Learning/Supervised Deep Networks; Reinforcement Learning and Planning/Reinforcement Lear,Poster
3114,Algebraic tests of general Gaussian latent tree models,"We consider general Gaussian latent tree models in which the observed variables are not restricted to be leaves of the tree. Extending related recent work, we give a full semi-algebraic description of the set of covariance matrices of any such model.  In other words, we find polynomial constraints that characterize when a matrix is the covariance matrix of a distribution in a given latent tree model. However, leveraging these constraints to test a given such model is often complicated by the number of constraints being large and by singularities of individual polynomials, which may invalidate standard approximations to relevant probability distributions. Illustrating with the star tree, we propose a new testing methodology that circumvents singularity issues by trading off some statistical estimation efficiency and handles cases with many constraints through recent advances on Gaussian approximation for maxima of sums of high-dimensional random vectors. Our test avoids the need to maximize the possibly multimodal likelihood function of such models and is applicable to models with larger number of variables.  These points are illustrated in numerical experiments.","Dennis Leung*,University of Southern California;Mathias Drton,University of Washington",dmhleung@uw.edu;md5@uw.edu,Probabilistic Methods/Latent Variable Models*; Probabilistic Methods/Graphical Models,Poster
3115,Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models,"Neural language models (NLMs) have recently gained a renewed interest by achieving state-of-the-art performance across many natural language processing (NLP) tasks. However, NLMs are very computationally demanding largely due to the computational cost of the softmax function over a large vocabulary. We observe that, in decoding of many NLP tasks, only the probabilities of the top-K hypotheses need to be calculated preciously and K is often much smaller than the vocabulary size. This paper proposes a novel softmax approximation algorithm, called Fast Graph Decoder (FGD), which quickly identifies, for a given context, a set of K words that are most likely to occur according to a NLM.  We demonstrate that FGD reduces the decoding time by an order of magnitude while attaining close to the full softmax baseline accuracy on neural machine translation and language modeling tasks. We also prove the theoretical guarantee on the softmax approximation quality.","Minjia Zhang*,Microsoft;Wenhan Wang,Microsoft;Xiaodong Liu,Microsoft;Jianfeng  Gao,Microsoft Research;Yuxiong He,Microsoft",minjiaz@microsoft.com;wenhanw@microsoft.com;xiaodl@microsoft.com;jfgao@microsoft.com;yuxionghe@gmail.com,Deep Learning/Efficient Inference Methods*; Applications/Natural Language Processing; Deep Learning,Poster
3122,Deep Structured Prediction via Nonlinear Output Transformations,"Deep structured models are widely used for tasks like semantic segmentation, where explicit correlations between variables provide important prior information which generally helps to reduce the data needs of deep nets. However, current deep structured models are restricted by oftentimes very local neighborhood structure, which cannot be increased for computational complexity reasons, and by the fact that the output configuration, or a representation thereof, cannot be transformed further. Very recent approaches which address those issues include graphical model inference inside deep nets so as to permit subsequent non-linear output space transformations. However, optimization of those formulations is challenging and not well understood. Here, we develop a novel model which generalizes existing approaches, such as structured prediction energy networks, and discuss a formulation which maintains applicability of existing inference techniques.","Colin Graber*,UIUC;Ofer Meshi,Google Research;Alexander Schwing,UIUC",cgraber2@illinois.edu;meshi@google.com;aschwing@illinois.edu,Algorithms/Structured Prediction*; Deep Learning/Supervised Deep Networks,Poster
3124,Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling,"Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-problem in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice.","Emilie Kaufmann,Universite de Lille;Wouter Koolen*,CWI Amsterdam;Aur√©lien Garivier,ENS Lyon",emilie.kaufmann@univ-lille1.fr;wmkoolen@cwi.nl;aurelien.garivier@ens-lyon.fr,Algorithms/Bandit Algorithms,Poster
3133,Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization,"We consider the scenario where several independent data owners wish to collaboratively learn a model over their sensitive data sets, without exposing their private data. Our approach combines differential privacy with secure multi-party computation to achieve private distributed machine learning. In particular, we explore two popular methods of differential privacy, output perturbation and gradient perturbation, and advance the state-of-the-art for both methods in the distributed learning setting. In our output perturbation method, the parties combine their local models within a secure computation and then add the required differential privacy noise within the secure computation before revealing the model. In our gradient perturbation method, the data owners collaboratively train a global model via an iterative learning algorithm.  At each iteration, the parties aggregate their local gradients within a secure computation, adding sufficient noise to ensure privacy before the gradient updates are revealed. In both cases, we show that the noise can be reduced in the multi-party setting by adding the noise inside the secure computation after aggregation, asymptotically improving upon the best previous results. Thorough experiments on real world data sets corroborate our theory.","Bargav Jayaraman,University of Virginia;Lingxiao Wang,University of California, Los Angeles;David Evans,University of Virginia;Quanquan Gu*,UCLA",bj4nq@virginia.edu;lingxw@ucla.edu;evans@virginia.edu;qg5w@virginia.edu,"Applications/Privacy, Anonymity, and Security*; Optimization/Convex Optimization",Poster
3135,A no-regret generalization of hierarchical softmax to extreme multi-label classification,"Extreme multi-label classification (XMLC) is the problem of tagging an instance with a small subset of relevant labels chosen from an extremely large pool of possible labels. Large label spaces can be efficiently handled by organizing labels as a tree, like in the hierarchical softmax (HSM) approach. In this paper, we investigate Probabilistic Label Trees (PLTs) that have been recently devised for tackling XMLC problems. We show that PLT is a no-regret multi-label generalization of HSM when precision@k is used as a model evaluation metric. Critically, we prove that pick-one-label heuristic---a reduction technique from multi-label to multi-class that is routinely used along with HSM---is not consistent in general. We also show that our implementation of PLTs, referred to as XT, obtains significantly better results than HSM with the pick-one-label heuristic and XML-CNN, a deep network specifically designed for XMLC problems. Moreover, XT is competitive to many state-of-the-art approaches in terms of statistical performance, model size and prediction time which makes it amenable to deploy in an online system. ","Marek Wydmuch,Poznan University of Technology;Kalina Jasinska,Poznan University of Technology;Mikhail Kuznetsov,Yahoo! Research;Robert Busa-Fekete,Yahoo! Research;Krzysztof Dembczy≈Ñski*,Poznan University of Technology",marek@wydmuch.poznan.pl;kalina.jasinska@gmail.com;kuznetsov@oath.com;busarobi@gmail.com;Krzysztof.Dembczynski@cs.put.poznan.pl,Algorithms/Structured Prediction*; Algorithms/Classification,Poster
3136,Efficient Formal Safety Analysis of Neural Networks,"Neural networks are increasingly deployed in real-world safety-critical domains such as autonomous driving, aircraft collision avoidance, and malware detection. However, these networks have been shown to often mispredict on inputs with minor adversarial or even accidental perturbations. Consequences of such errors can be disastrous and even potentially fatal as shown by the recent Tesla autopilot crash. Thus, there is an urgent need for formal analysis systems that can rigorously check neural networks for violations of different safety properties such as robustness against adversarial perturbations within a certain L-norm of a given image. An effective safety analysis system for a neural network must be able to either ensure that a safety property is satisfied by the network or find a counterexample, i.e., an input for which the network will violate the property. Unfortunately, most existing techniques for performing such analysis struggle to scale beyond very small networks and the ones that can scale to larger networks suffer from high false positives and cannot produce concrete counterexamples in case of a property violation. In this paper, we present a new efficient approach for rigorously checking different safety properties of neural networks that significantly outperforms existing approaches by multiple orders of magnitude. Our approach can check different safety properties and find concrete counterexamples for networks that are 10x larger than the ones supported by existing analysis techniques. We believe that our approach to estimating tight output bounds of a network for a given input range can also help improve the explainability of neural networks and guide the training process of more robust neural networks.","Shiqi Wang*,Columbia University;Kexin Pei,Columbia University;Justin Whitehouse,Columbia University;Junfeng Yang,Columbia University;Suman Jana,Columbia University",tcwangshiqi@cs.columbia.edu;kpei@cs.columbia.edu;jaw2228@columbia.edu;junfeng@cs.columbia.edu;suman@cs.columbia.edu,"Applications/Privacy, Anonymity, and Security*; Applications/Fairness, Accountability, and Transparency",Poster
3138,Bayesian Distributed Stochastic Gradient Descent,"We introduce Bayesian distributed stochastic gradient descent (BDSGD), a high-throughput algorithm for training deep neural networks on parallel clusters. This algorithm uses amortized inference in a deep generative model to perform joint posterior predictive inference of mini-batch gradient computation times in a compute cluster specific manner. Specifically, our algorithm mitigates the straggler effect in synchronous, gradient-based optimization by choosing an optimal cutoff beyond which mini-batch gradient messages from slow workers are ignored. In our experiments, we show that eagerly discarding the mini-batch gradient computations of stragglers not only increases throughput but actually increases the overall rate of convergence as a function of wall-clock time by virtue of eliminating idleness.  The principal novel contribution and finding of this work goes beyond this by demonstrating that using the predicted run-times from a generative model of cluster worker performance improves substantially over the static-cutoff prior art, leading to reduced deep neural net training times on large computer clusters.","Michael Teng*,University of Oxford;Frank Wood,University of British Columbia",mteng@robots.ox.ac.uk;fwood@cs.ubc.ca,Deep Learning/Predictive Models*; Deep Learning/Optimization for Deep Networks,Poster
3144,Visualizing the Loss Landscape of Neural Nets,"Neural network training relies on our ability to find ""good"" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple ""filter normalization"" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.","Hao Li*,University of Maryland, College Park;Zheng Xu,Univ. of Maryland;Gavin Taylor,;Christoph Studer,Nil;Tom Goldstein,University of Maryland, College Park",haoli@cs.umd.edu;xuzh@cs.umd.edu;taylor@usna.edu;studer@cornell.edu;tomg@cs.umd.edu,Deep Learning*; Deep Learning/Visualization or Exposition Techniques for Deep Networks,Poster
3150,The Limits of Post-Selection Generalization,"While statistics and machine learning offers numerous methods for ensuring generalization, these methods often fail in the presence of *post selection*---the common practice in which the choice of analysis depends on previous interactions with the same dataset.  A recent line of work has introduced powerful, general purpose algorithms that ensure a property called *post hoc generalization* (Cummings et al., COLT'16), which says that no person when given the output of the algorithm should be able to find any statistic for which the data differs significantly from the population it came from.

In this work we show several limitations on the power of algorithms satisfying post hoc generalization.  First, we show a tight lower bound on the error of any algorithm that satisfies post hoc generalization and answers adaptively chosen statistical queries, showing a strong barrier to progress in post selection data analysis.  Second, we show that post hoc generalization is not closed under composition, despite many examples of such algorithms exhibiting strong composition properties. ","Jonathan Ullman*,Northeastern University;Adam Smith,Boston University;Kobbi Nissim,Georgetown University;Uri Stemmer,Weizmann Institute;Thomas Steinke,IBM Research - Almaden",jullman@ccs.neu.edu;adamdavisonsmith@gmail.com;kobbini@hotmail.com;uri.stemmer@gmail.com;cmt@thomas-steinke.net,Algorithms/Adaptive Data Analysis*; Theory/Learning Theory,Poster
3162,Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation,"Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models that finds molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.","Jiaxuan You*,Stanford University;Bowen Liu,Stanford University;Rex Ying,Stanford University;Vijay Pande,Stanford;Jure Leskovec,Stanford",jiaxuan@stanford.edu;liubowen@stanford.edu;rexying@stanford.edu;pande@stanford.edu;jure@cs.stanford.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Applications/Network Analysis,Spotlight
3163,On Controllable Sparse Alternatives to Softmax,"Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classification, multilabel classification, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a unified framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparseflex and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classification setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization.","Anirban Laha*,IBM Research;Saneem Chemmengath,IBM Research AI;Priyanka Agrawal,IBM Research AI;Mitesh Khapra,IIT Madras;Karthik Sankaranarayanan,IBM Research;Harish Ramaswamy,IIT Madras",anirlaha@in.ibm.com;saneem.cg@in.ibm.com;priyanka.agrawal@in.ibm.com;khapra.mitesh@gmail.com;kartsank@in.ibm.com;harishguruprasad@gmail.com,Algorithms/Sparsity and Compressed Sensing*; Algorithms/Classification; Deep Learning/Attention Models,Poster
3164,L4: Practical loss-based stepsize adaptation for deep learning,"We propose a stepsize adaptation scheme for stochastic gradient descent.
It operates directly with the loss function and rescales the gradient in order to make fixed predicted progress on the loss.
We demonstrate its capabilities by conclusively improving the performance of Adam and Momentum optimizers.
The enhanced optimizers with default hyperparameters
 consistently outperform their constant stepsize counterparts, even the best ones,
 without a measurable increase in computational cost.
The performance is validated on multiple architectures including dense nets, CNNs, ResNets, and the recurrent Differential Neural Computer on classical datasets MNIST, fashion MNIST, CIFAR10 and others.","Michal Rolinek*,Max Planck Institute for Intelligent Systems;Georg Martius,MPI for Intelligent Systems",michal.rolinek@tuebingen.mpg.de;georg.martius@tuebingen.mpg.de,Deep Learning/Optimization for Deep Networks,Poster
3170,Learning Latent Subspaces in Variational Autoencoders,"We present a method for training variational autoencoders on labelled datasets which encode information corresponding to the labels in explicitly predetermined subspaces of the latent space. We motivate our model from both an information theoretic perspective as well as a adversarial game perspective. By separating labelled information into a less complicated space we allow the model to more easily disentangle representations. This provides a form of semi-supervised learning of attributes. Since these subspaces can be chosen a priori, setting them to be low-dimensional provides a form of dimensionality reduction. We demonstrate the utility of our model on attribute manipulation tasks with several image datasets.","Jack Klys*,University of Toronto;Jake Snell,University of Toronto;Richard Zemel,University of Toronto",jackklys@gmail.com;jsnell@cs.toronto.edu;zemel@cs.toronto.edu,Algorithms/Representation Learning*; Deep Learning/Deep Autoencoders; Deep Learning/Generative Models,Poster
3171,Turbo Learning for Captionbot and Drawingbot,"We study in this paper the problems of both image captioning and text-to-image generation, and present a novel turbo learning approach to jointly training an image-to-text generator (a.k.a. captionbot) and a text-to-image generator (a.k.a. drawingbot). The key idea behind the joint training is that image-to-text generation and text-to-image generation as dual problems can form a closed loop to provide informative feedback to each other. Based on such feedback, we introduce a new loss metric by comparing the original input with the output produced by the closed loop. In addition to the old loss metrics used in captionbot and drawingbot, this extra loss metric makes the jointly trained captionbot and drawingbot better than the separately trained captionbot and drawingbot. Furthermore, the turbo-learning approach enables semi-supervised learning since the closed loop can provide peudo-labels for unlabeled samples. Experimental results on the COCO dataset demonstrate that the proposed turbo learning can significantly improve the performance of both captionbot and drawingbot by a large margin.","Qiuyuan Huang*,Microsoft Research AI;Pengchuan Zhang,Microsoft Research AI;Dapeng Wu,University of Florida;Lei Zhang,Microsoft Research",qihua@microsoft.com;penzhan@microsoft.com;dpwu@ieee.org;leizhang@microsoft.com,Applications/Computer Vision*; Applications/Natural Language Processing,Poster
3182,Learning to Teach with Dynamic Loss Functions,"Teaching is critical to human society: it is with teaching that prospective students are educated and human civilization can be inherited and advanced. A good teacher not only provides his/her students with qualified teaching materials (e.g., textbooks), but also sets up appropriate learning objectives (e.g., course projects and exams) considering different situations of a student. When it comes to artificial intelligence, treating machine learning models as students, the loss functions that are optimized act as perfect counterparts of the learning objective set by the teacher. In this work, we explore the possibility of imitating human teaching behaviors by dynamically and automatically outputting appropriate loss functions to train machine learning models. Different from typical learning settings in which the loss function of a machine learning model is predefined and fixed, in our framework, the loss function of a machine learning model (we call it student) is defined by another machine learning model (we call it teacher). The ultimate goal of teacher model is cultivating the student to have better performance measured on development dataset. Towards that end, similar to human teaching, the teacher, a parametric model, dynamically outputs different loss functions that will be used and optimized by its student model at different training stages. We develop an efficient learning method for the teacher model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as ``learning to teach with dynamic loss functions'' (L2T-DLF for short). Extensive experiments on real world tasks including image classification and neural machine translation demonstrate that our method significantly improves the quality of various student models. ","Lijun Wu,Sun Yat-sen University;Fei Tian*,Microsoft Research;Yingce Xia,Microsoft Research Asia;Yang Fan,University of Science and Technology of China;Tao Qin,Microsoft Research Asia;Jian-Huang Lai,Sun Yat-sen University;Tieyan Liu,",wulijun3@mail2.sysu.edu.cn;fetia@microsoft.com;yingce.xia@microsoft.com;fyabc@mail.ustc.edu.cn;taoqin@microsoft.com;stsljh@mail.sysu.edu.cn;tyliu@microsoft.com,Algorithms/AutoML*; Deep Learning; Deep Learning/Meta-Learning; Neuroscience and Cognitive Science,Poster
3185,Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation,"We consider the problem of scaling deep generative shape models to high-resolution. Drawing motivation from the canonical view representation of objects, we introduce a novel method for the fast up-sampling of 3D objects in voxel space through networks that perform super-resolution on the six orthographic depth projections. This allows us to efficiently generate high-resolution objects, without the cubic computational costs associated with voxel data. We further decompose the learning problem into silhouette and depth prediction to capture both structure and fine detail, easing the burden of an individual network generating sharp edges. We evaluate our work on multiple experiments concerning high-resolution 3D objects, and show our system is capable of accurately producing objects at resolutions as large as 512x512x512 -- the highest resolution reported for this task, to our knowledge. We achieve state-of-the-art performance on 3D object reconstruction from RGB images on the ShapeNet dataset, and further demonstrate the first effective 3D super-resolution method. ","Edward Smith*,McGill;Scott Fujimoto,McGill;David Meger,University of British Columbia",edward.smith@mail.mcgill.ca;scott.fujimoto@mail.mcgill.ca;dmeger@cim.mcgill.ca,Applications/Computer Vision*; Deep Learning/Generative Models,Poster
3213,Size-Noise Tradeoffs in Generative Networks,"Generative networks can modify their noise distributions to look like other noise distributions. Firstly, we demonstrate a construction that allows ReLU networks to increase the dimensionality of their noise distribution by implementing a ``space-filling'' function based on iterated tent maps. We show this construction is optimal by analyzing the number of affine pieces in functions computed by multivariate ReLU networks. We also develop a toolkit of techniques for function approximation with neural networks, including a Taylor series approximation and a binary search gadget for computing function inverses. This toolkit provides efficient ways (using $\polylog(1/\epsilon)$ nodes) for networks to pass between univariate normal and uniform distributions. ","Bolton Bailey*,University of Illinois;Matus Telgarsky,UIUC",boltonb2@illinois.edu;mjt@illinois.edu,Theory*; Deep Learning/Adversarial Networks; Deep Learning/Generative Models,Spotlight
3218,"Online Adaptive Methods, Universality and Acceleration","We present a novel method for convex unconstrained optimization that, without  any modifications ensures: (1) accelerated convergence rate for smooth objectives, (2) standard convergence rate in the general (non-smooth) setting, and (3)  standard convergence rate in the stochastic optimization setting.  
To the best of our knowledge, this is the first method that simultaneously applies to all of the above settings.  
At the heart of our method is an adaptive learning rate rule that employs importance weights, in the spirit of adaptive online learning algorithms  [duchi2011adaptive,levy2017online],  combined with an update  that linearly couples two sequences, in the spirit of [AllenOrecchia2017]. An empirical examination of our method demonstrates its applicability to the above mentioned scenarios and corroborates our theoretical findings.","Kfir Levy*,ETH;Alp Yurtsever,EPFL;Volkan Cevher,EPFL",kfiryehud@gmail.com;alp.yurtsever@epfl.ch;volkan.cevher@epfl.ch,Algorithms/Online Learning,Poster
3226,Compact Generalized Non-local Network,"The non-local module is designed for capturing long-range spatio-temporal dependencies in images and videos. Although having shown excellent performance, it lacks the mechanism to model the interactions between positions across channels, which are of vital importance in recognizing fine-grained objects and actions. To address this limitation, we generalize the non-local module and take the correlations between the positions of any two channels into account. This extension unifies second-order feature pooling and achieves state-of-the-arts performance on a variety of fine-grained classification tasks. However, it also leads to an explosion in the computational complexity. To alleviate such an issue, we further propose its compact representation to reduce the high-dimensional feature space and large computation burden involved. Moreover, we try to group the channels and do our generalized non-local method within each group. Experimental results illustrate the significant improvements and practical applicability of the generalized non-local module on both fine-grained object and video classification. Code will be made publicly available to ease the future research.","Kaiyu Yue,Baidu Inc.;Ming Sun,baidu;Yuchen Yuan,Baidu Inc.;Feng Zhou*,Baidu Inc.;Errui Ding,Baidu Inc.;Fuxin Xu,Central South University",kaiyuyue@gmail.com;sunming05@baidu.com;yuanyuchen02@baidu.com;zhfe99@gmail.com;dingerrui@baidu.com;xfx.300@163.com,Applications/Computer Vision*; Algorithms/Classification,Poster
3233,On the Local Hessian in Back-propagation,"Back-propagation (BP) is the foundation for successfully training deep neural networks. However, BP sometimes has difficulties in propagating a learning signal deep enough effectively, e.g., the vanishing gradient phenomenon. Meanwhile, BP often works well when combining with ``designing tricks'' like orthogonal initialization, batch normalization and skip connection. There is no clear understanding on what is essential to the efficiency of BP. In this paper, we take one step towards clarifying this problem. We view BP as a solution of back-matching propagation which minimizes a sequence of back-matching losses each corresponding to one block of the network. We study the Hessian of the local back-matching loss (local Hessian)  and connect it to the efficiency of BP. It turns out that those designing tricks facilitate BP by improving the spectrum of local Hessian. In addition, we can utilize the local Hessian to balance the training pace of each block and design new training algorithms. Based on a scalar approximation of local Hessian, we propose a scale-amended SGD algorithm. We apply it to train neural networks with batch normalization, and achieve favorable results over vanilla SGD. This corroborates the importance of local Hessian from another side.","Huishuai Zhang*,Microsoft Research Asia;Wei Chen,Microsoft;Tieyan Liu,",huzhang@microsoft.com;wche@microsoft.com;tyliu@microsoft.com,Deep Learning*; Deep Learning/Efficient Training Methods; Deep Learning/Optimization for Deep Networks,Poster
3253,The Everlasting Database: Statistical Validity at a Fair Price,"  The problem of handling adaptivity in data analysis, intentional or not,  permeates
  a variety of fields, including  test-set overfitting in ML challenges and the
  accumulation of invalid scientific discoveries.
  We propose a mechanism for answering an arbitrarily long sequence of
  potentially adaptive statistical queries, by charging a price for
  each query and using the proceeds to collect additional samples.
  Crucially, we guarantee statistical validity without any assumptions on
  how the queries are generated. We also ensure with high probability that
  the cost for $M$ non-adaptive queries is $O(\log M)$,
  while the cost to a potentially adaptive user who makes $M$
  queries that do not depend on any others is $O(\sqrt{M})$.","Blake Woodworth*,TTI-Chicago;Vitaly Feldman,Google;Saharon Rosset,Technion;Nathan Srebro,Toyota Technical Institute of Chicago",blake@ttic.edu;vitaly.edu@gmail.com;saharon@post.tau.ac.il;nati@ttic.edu,Algorithms/Adaptive Data Analysis*; Applications/Computational Social Science; Theory/Learning Theory,Poster
3258,Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks,"High sensitivity of neural networks against malicious perturbations on inputs causes security concerns. To take a steady step towards robust classifiers, we aim to create neural network models provably defended from perturbations. Prior certification work requires strong assumptions on network structures and massive computational costs, and thus, their applications are limited. Based on the relationship between the Lipschitz constants and prediction margins, we present a computationally efficient calculation technique that lower-bounds the size of adversarial perturbations that can deceive networks, and that is widely applicable to various complicated networks. Moreover, we propose an efficient training procedure, which robustifies networks and significantly improves the provably guarded areas around data points. In experimental evaluations, our method showed its ability to provide a non-trivial guarantee and improve robustness for even large networks.","Yusuke Tsuzuku*,The University of Tokyo/RIKEN;Issei Sato,The university of Tokyo/RIKEN;Masashi Sugiyama,RIKEN/The University of Tokyo",tsuzuku@ms.k.u-tokyo.ac.jp;sato@k.u-tokyo.ac.jp;sugi@k.u-tokyo.ac.jp,Deep Learning*; Applications/Computer Vision,Poster
3265,Proximal SCOPE for Distributed Sparse Learning,"Distributed sparse learning with a cluster of multiple machines has attracted much attention in machine learning, especially for large-scale applications with high-dimensional data. One popular way to implement sparse learning is to use L_1 regularization. In this paper, we propose a novel method, called proximal SCOPE(pSCOPE), for distributed sparse learning with L_1 regularization. pSCOPE is based on a cooperative autonomous local learning (CALL) framework. In the CALL framework of pSCOPE, we find that the data partition affects the convergence of the learning procedure, and subsequently we define a metric to measure the goodness of a data partition. Based on the defined metric, we theoretically prove that pSCOPE is convergent with a linear convergence rate if the data partition is good enough. We also prove that better data partition implies faster convergence rate. Furthermore, pSCOPE is also communication efficient. Experimental results on real data sets show that pSCOPE can outperform other state-of-the-art distributed methods for sparse learning.","Shen-Yi Zhao*,Nanjing University;Gong-Duo Zhang,Nanjing University;Ming-Wei Li,;Wu-Jun Li,Nanjing University",zhaosy@lamda.nju.edu.cn;zhanggd@lamda.nju.edu.cn;limw@lamda.nju.edu.cn;liwujun@nju.edu.cn,Optimization*; Optimization/Convex Optimization,Poster
3300,On Coresets for Logistic Regression,"Coresets are one of the central methods to facilitate the analysis of large data. We continue a recent line of research applying the theory of coresets to logistic regression. First, we show the negative result that no strongly sublinear sized coresets exist for logistic regression. To deal with intractable worst-case instances   we introduce a complexity measure $\mu(X)$, which quantifies the hardness of compressing a data set for logistic regression. $\mu(X)$ has an intuitive statistical interpretation that may be of independent interest. For data sets with bounded $\mu(X)$-complexity, we show that a novel sensitivity sampling scheme produces the first provably sublinear $(1\pm\eps)$-coreset. We illustrate the performance of our method by comparing to uniform sampling as well as to state of the art methods in the area. The experiments are conducted on real world benchmark data for logistic regression.","Alexander Munteanu*,TU Dortmund;Chris Schwiegelshohn,Sapienza University of Rome;Christian Sohler,TU Dortmund;David Woodruff,Carnegie Mellon University",alexander.munteanu@tu-dortmund.de;schwiegelshohn@diag.uniroma1.it;christian.sohler@tu-dortmund.de;dwoodruf@cs.cmu.edu,Theory*; Algorithms; Theory/Computational Complexity,Spotlight
3310,Neural Ordinary Differential Equations,"We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.","Ricky T. Q. Chen*,University of Toronto;Yulia Rubanova,University of Toronto;Jesse Bettencourt,University of Toronto;David Duvenaud,University of Toronto",rtqichen@cs.toronto.edu;rubanova@cs.toronto.edu;jessebett@cs.toronto.edu;duvenaud@cs.toronto.edu,Deep Learning/Generative Models*; Algorithms/Density Estimation; Algorithms/Dynamical Systems; Deep Learning/Deep Autoencoder,Oral
3313,Unsupervised Learning of Artistic Styles with Archetypal Style Analysis,"In this paper, we introduce an unsupervised learning approach to automatically dis-
cover, summarize, and manipulate artistic styles from large collections of paintings.
Our method is based on archetypal analysis, which is an unsupervised learning
technique akin to sparse coding with a geometric interpretation. When applied
to deep image representations from a data collection, it learns a dictionary of
archetypal styles, which can be easily visualized. After training the model, the style
of a new image, which is characterized by local statistics of deep visual features,
is approximated by a sparse convex combination of archetypes. This allows us
to interpret which archetypal styles are present in the input image, and in which
proportion. Finally, our approach allows us to manipulate the coefficients of the
latent archetypal decomposition, and achieve various special effects such as style
enhancement, transfer, and interpolation between multiple archetypes.","Daan Wynen,Inria;Cordelia Schmid,INRIA;Julien Mairal*,INRIA",daan.wynen@inria.fr;cordelia.schmid@inria.fr;julien.mairal@inria.fr,Applications*; Applications/Computational Photography; Applications/Computer Vision,Poster
3326,Approximating Real-Time Recurrent Learning with Random Kronecker Factors,"Despite all the impressive advances of recurrent neural networks, sequential data is still in need of better modelling. Truncated backpropagation through time (TBPTT), the learning algorithm most widely used in practice, suffers from the truncation bias, which drastically limits its ability to learn long-term dependencies.The Real Time Recurrent Learning algorithm (RTRL) addresses this issue,  but its high computational requirements  make it infeasible in practice. The Unbiased Online Recurrent Optimization algorithm (UORO) approximates RTRL with a smaller runtime and memory cost, but with the disadvantage  of obtaining noisy gradients that also limit its practical applicability. In this paper we propose the Kronecker Factored RTRL (KF-RTRL) algorithm that uses a Kronecker product decomposition to approximate the gradients for a large class of RNNs. We show that KF-RTRL is an unbiased and memory efficient online learning algorithm. Our theoretical analysis shows that, under reasonable assumptions, the noise introduced by our algorithm is not only stable over time but also asymptotically much smaller than the one of the UORO algorithm. We also confirm these theoretical results experimentally. Further, we show empirically that the KF-RTRL algorithm captures long-term dependencies and almost matches the performance of TBPTT on real world tasks by training Recurrent Highway Networks on a synthetic string memorization task and on the Penn TreeBank task, respectively. These results indicate that RTRL based approaches might be a promising future alternative to TBPTT.","Asier Mujika*,ETH Zurich;Florian Meier,ETH Zurich;Angelika Steger,ETH Zurich",asierm@inf.ethz.ch;meierflo@inf.ethz.ch;steger@inf.ethz.ch,Algorithms/Online Learning*; Deep Learning/Optimization for Deep Networks; Deep Learning/Recurrent Networks; Theory/Lear,Poster
3334,Contamination Attacks in Multi-Party Machine Learning,"Machine learning is data hungry; the more data a model has access to in training, the more likely it is to perform well at inference time. Distinct parties may want to combine their local data to gain the benefits of a model trained on a large corpus of data. We consider such a case, where the local data remains private and only the model trained on the joint data is revealed. We show that there exists attacks that are stealthy and can compromise the integrity of the model. We then show how adversarial training can defend against such attacks.","Jamie Hayes*,University College London;Olga Ohrimenko,Microsoft",j.hayes@cs.ucl.ac.uk;oohrim@microsoft.com,"Applications/Privacy, Anonymity, and Security*; Applications/Fairness, Accountability, and Transparency; Deep Learning/Adversarial Networks",Poster
3335,An Improved Analysis of Alternating Minimization for Structured Multi-Response Regression,"Multi-response linear models aggregate a set of vanilla linear models by assuming correlated noise across them, which has an unknown covariance structure. To find the coefficient vector, estimators with a joint approximation of the noise covariance are often preferred than the simple linear regression in view of their superior empirical performance, which can be generally solved by alternating-minimization type procedures. Due to the non-convex nature of such joint estimators, the theoretical justification of their efficiency is typically challenging. The existing analyses fail to fully explain the empirical observations due to the assumption of resampling on the alternating procedures, which requires access to fresh samples in each iteration. In this work, we present a resampling-free analysis for the alternating minimization algorithm applied to the multi-response regression. In particular, we focus on the high-dimensional setting of multi-response linear models with structured coefficient parameter, and the statistical error of the parameter can be expressed by the complexity measure, Gaussian width, which is related to the assumed structure. More importantly, to the best of our knowledge, our result reveals for the first time that the alternating minimization with random initialization can achieve the same performance as the well-initialized one when solving this multi-response regression problem. Experimental results support our theoretical developments.","Sheng Chen*,University of Minnesota;Arindam Banerjee,University of Minnesota",shengc@cs.umn.edu;banerjee@cs.umn.edu,Theory/Frequentist Statistics*; Optimization/Non-Convex Optimization,Poster
3340,Incorporating Context into Language Encoding Models for fMRI,"Language encoding models help explain language processing in the human brain by learning functions that predict brain responses from the language stimuli that elicited them. Current word embedding-based approaches treat each stimulus word independently and thus ignore the influence of context on language understanding. In this work we instead build encoding models using rich contextual representations derived from an LSTM language model. Our models show a significant improvement in encoding performance relative to state-of-the-art embeddings in nearly every brain area. By varying the amount of context used in the models and providing the models with distorted context, we show that this improvement is due to a combination of better word embeddings learned by the LSTM language model and contextual information. We are also able to use our models to map context sensitivity across the cortex. These results suggest that LSTM language models learn high-level representations that are related to representations in the human brain.","Shailee Jain,The University of Texas at Austin;Alexander Huth*,The University of Texas at Austin",shailee@cs.utexas.edu;huth@cs.utexas.edu,Neuroscience and Cognitive Science/Brain Mapping*; Applications/Natural Language Processing; Neuroscience and Cognitive Science/Brain Imaging; Ne,Poster
3342,CatBoost: unbiased boosting with categorical features,"This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.","Liudmila Prokhorenkova*,Yandex;Gleb Gusev,Yandex;Aleksandr Vorobev,Yandex;Anna Veronika Dorogush,Yandex;Andrey Gulin,Yandex",ostroumova-la@yandex-team.ru;gleb57@yandex-team.ru;alvor88@yandex-team.ru;annaveronika@yandex-team.ru;gulin@yandex-team.ru,Algorithms/Boosting and Ensemble Methods,Poster
3346,Query K-means Clustering and the Double Dixie Cup Problem,"We consider the problem of approximate $K$-means clustering with outliers and side information provided by same-cluster queries and possibly noisy answers. Our solution shows that, under some mild assumptions on the smallest cluster size, one can obtain an $(1+\epsilon)$-approximation for the optimal potential with probability at least $1-\delta$, where $\epsilon>0$ and $\delta\in(0,1)$, using an expected number of $O(\frac{K^3}{\epsilon \delta})$ noiseless same-cluster queries and comparison-based clustering of complexity $O(ndK + \frac{K^3}{\epsilon \delta})$; here, $n$ denotes the number of points and $d$ the dimension of space. Compared to a handful of other known approaches that perform importance sampling to account for small cluster sizes, the proposed query technique reduces the number of queries by a factor of roughly $O(\frac{K^6}{\epsilon^3})$, at the cost of possibly missing very small clusters. We extend this settings to the case where some queries to the oracle produce erroneous information, and where certain points, termed outliers, do not belong to any clusters. Our proof techniques differ from previous methods used for $K$-means clustering analysis, as they rely on estimating the sizes of the clusters and the number of points needed for accurate centroid estimation and subsequent nontrivial generalizations of the double Dixie cup problem. We illustrate the performance of the proposed algorithm both on synthetic and real datasets, including MNIST and CIFAR $10$.","I Chien*,UIUC;Chao Pan,University of Illinois Urbana-Champaign;Olgica Milenkovic,University of Illinois UC",ichien3@illinois.edu;chaopan2@illinois.edu;milenkov@illinois.edu,Algorithms/Clustering*; Algorithms/Semi-Supervised Learning,Poster
3356,Training Neural Networks Using Features Replay,"Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network. The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources.  Recently, there are several works trying to decouple and parallelize the backpropagation algorithm. However, all of them suffer from severe accuracy loss or memory explosion when the neural network is deep.  To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network. After that,  we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally, we apply our method to training deep convolutional neural networks, and the experimental results show that the proposed method achieves {faster} convergence, {lower} memory consumption, and {better} generalization error than compared methods.","Zhouyuan Huo*,University of Pittsburgh;Bin Gu,Pittsburgh University;Heng Huang,University of Pittsburgh",zhouyuan.huo@pitt.edu;jsgubin@gmail.com;henghuanghh@gmail.com,Deep Learning/Optimization for Deep Networks,Spotlight
3361,Modeling Dynamic Missingness of Implicit Feedback for Recommendation,"Implicit feedback is widely used in collaborative filtering methods for recommendation. It is well known that implicit feedback contains a large number of values that are \emph{missing not at random} (MNAR); and the missing data is a mixture of negative and unknown feedback, making it difficult to learn user's negative preferences. 
Recent studies modeled \emph{exposure}, a latent missingness variable which indicates whether an item is missing to a user, to give each missing entry a confidence of being negative feedback.
However, these studies use static models and ignore the information in temporal dependencies among items, which seems to be a essential underlying factor to subsequent missingness. To model and exploit the dynamics of missingness, we propose a latent variable named ``\emph{user intent}'' to govern the temporal changes of item missingness, and a hidden Markov model to represent such a process. The resulting framework captures the dynamic item missingness and incorporate it into matrix factorization (MF) for recommendation. We also explore two types of constraints to achieve a more compact and interpretable representation of \emph{user intents}. Experiments on real-world datasets demonstrate the superiority of our method against state-of-the-art recommender systems.","Menghan Wang*,Zhejiang University;Mingming Gong,University of Pittsburgh;Xiaolin Zheng,Zhejiang University;Kun Zhang,Carnegie Mellon University",wangmengh@zju.edu.cn;MIG73@pitt.edu;xlzheng@zju.edu.cn;kunz1@cmu.edu,Applications/Recommender Systems*; Algorithms/Missing Data; Algorithms/Ranking and Preference Learning,Poster
3363,Representation Learning of Compositional Data,"  We consider the problem of learning a low dimensional representation for
  compositional data. Compositional data consists of a collection of nonnegative data
  that sum to a constant value. Since the parts of the collection are statistically
  dependent, many standard tools cannot be directly applied. Instead compositional data must
  be first transformed before analysis. Focusing on
  principal component analysis (PCA), we propose an approach that allows low dimensional
  representation learning directly from the original data.
  We show that our proposed loss function upper bounds the exponential family PCA
  loss applied to transformed compositional data.
  A key tool in showing this relationship is a generalization of the scaled Bregman
  Theorem that equates the perspective transform of the generator of a Bregman divergence
  to the Bregman divergence of the perspective transform and a conformal divergence.
  Our proposed surrogate loss has an easy to optimize form, and we also derive the corresponding
  form for nonlinear autoencoders. Experiments on simulated data and microbiome data show
  the promise of our method.","Marta Avalos-Fernandez,University of Bordeaux;Richard Nock*,Data61, CSIRO;Cheng Soon Ong,Data61 and ANU;Julien Rouar,University of Bordeaux;Ke Sun,Data61, CSIRO",marta.avalos-fernandez@u-bordeaux.fr;Richard.Nock@data61.csiro.au;chengsoon.ong@anu.edu.au;julien.rouar@etu.u-bordeaux.fr;ke.sun@data61.csiro.au,Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning*; Algorithms/Representation Learning; Applications/Computational Biology and Bioinformatics,Poster
3364,Model-based targeted dimensionality reduction for neuronal population data,"Summarizing high-dimensional data using a small number of parameters is a ubiquitous first step in the analysis of neuronal population activity. Recently developed methods use ""targeted"" approaches that work by identifying multiple, distinct low-dimensional subspaces of activity that capture the population response to individual experimental task variables, such as the value of a presented stimulus or the behavior of the animal. These methods have gained attention because they decompose total neural activity into what are ostensibly different parts of a neuronal computation. However, existing targeted methods have been developed outside of the confines of probabilistic modeling, making some aspects of the procedures ad hoc, or limited in flexibility or interpretability. Here we propose a new model-based method for targeted dimensionality reduction based on a probabilistic generative model of the population response data.  The low-dimensional structure of our model is expressed as a low-rank factorization of a linear regression model. We perform efficient inference using a combination of expectation maximization and direct maximization of the marginal likelihood. We also develop an efficient method for estimating the dimensionality of each subspace. We show that our approach outperforms alternative methods in both mean squared error of the parameter estimates, and in identifying the correct dimensionality of encoding using simulated data. We also show that our method provides more accurate inference of low-dimensional subspaces of activity than a competing algorithm, demixed PCA.","Mikio Aoi*,Princeton University;Jonathan Pillow,Princeton University",mikioaoi@gmail.com;pillow@princeton.edu,"Neuroscience and Cognitive Science/Neural Coding*; Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA)",Poster
3366,On gradient regularizers for MMD GANs,"We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). Our method is based on studying the behavior of the optimized MMD, and constrains the gradient based on analytical results rather than an optimization penalty. Experimental results show that the proposed regularization leads to stable training and outperforms state-of-the art methods on image generation, including on 160 √ó 160 CelebA and 64 √ó 64 ImageNet.","Michael Arbel,UCL;Dougal Sutherland*,Gatsby Unit, UCL;Mikolaj Binkowski,Imperial College London;Arthur Gretton,Gatsby Computational Neuroscience Unit",michael.n.arbel@gmail.com;dougal@gmail.com;mikbinkowski@gmail.com;arthur.gretton@gmail.com,Deep Learning/Adversarial Networks*; Algorithms/Kernel Methods; Deep Learning/Generative Models,Poster
3368,Heterogeneous Multi-output Gaussian Process Prediction,"We present a novel extension of multi-output Gaussian processes for handling heterogeneous outputs. We assume that each output has its own likelihood function and use a vector-valued Gaussian process prior to jointly model the parameters in all likelihoods as latent functions. Our multi-output Gaussian process uses a covariance function with a linear model of coregionalisation form. Assuming conditional independence across the underlying latent functions together with an inducing variable framework, we are able to obtain tractable variational bounds amenable to stochastic variational inference.  We illustrate the performance of the model on synthetic data and two real datasets: a human behavioral study and a demographic high-dimensional dataset.","Pablo Moreno-Munoz*,Universidad Carlos III Madrid;Antonio Art√©s Rodr√≠guez,Universidad Carlos III de Madrid;Mauricio Alvarez,University of Sheffield",pmoreno@tsc.uc3m.es;antonio@tsc.uc3m.es;mauricio.alvarez@sheffield.ac.uk,Probabilistic Methods/Gaussian Processes,Spotlight
3370,Large-Scale Stochastic Sampling from the Probability Simplex,"Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference. These methods are based on sampling a discrete-time approximation to a continuous time process, such as the Langevin diffusion. When applied to distributions defined on a constrained space, such as the simplex, the time-discretisation error can dominate when we are near the boundary of the space. We demonstrate that while current SGMCMC methods for the simplex perform well in certain cases, they struggle with sparse simplex spaces; when many of the components are close to zero. However, most popular large-scale applications of Bayesian inference on simplex spaces, such as network or topic models, are  sparse. We argue that this poor performance is due to the biases of SGMCMC caused by the discretization error. To get around this, we propose the stochastic CIR process, which removes all discretization error and we prove that samples from the stochastic CIR process are asymptotically unbiased.  Use of the stochastic CIR process within a SGMCMC algorithm is shown to give substantially better performance for a topic model and a Dirichlet process mixture model than existing SGMCMC approaches.","Jack Baker*,Lancaster University;Paul Fearnhead,;Emily Fox,UW;Christopher Nemeth,University of Lancaster",j.baker1@lancaster.ac.uk;p.fearnhead@lancaster.ac.uk;ebfox@uw.edu;c.nemeth@lancaster.ac.uk,Probabilistic Methods/MCMC,Poster
3389,Policy Regret in Repeated Games,"The notion of ``policy regret'' in online learning is supposed to capture the reactions of the adversary to the actions taken by the learner, which more traditional notions such as external regret do not take into account.  We revisit this notion of policy regret, and first show that there are online learning settings in which policy regret and external regret are incompatible: any sequence of play which does well with respect to one must do poorly with respect to the other.  We then focus on the game theoretic setting, when the adversary is a self-interested agent.  In this setting we show that the external regret and policy regret are not in conflict, and in fact that a wide class of algorithms can ensure both as long as the adversary is also using such an algorithm.  We also define a new notion of equilibrium which we call a ``policy equilibrium'', and show that no-policy regret algorithms will have play which converges to such an equilibrium.  Relating this back to external regret, we show that coarse correlated equilibria (which no-external regret players will converge to) are a strict subset of policy equilibria.  So in game-theoretic settings every sequence of play with no external regret also has no policy regret, but the converse is not true.","Raman Arora,Johns Hopkins University;Michael Dinitz,JHU;Teodor Vanislavov Marinov*,Johns Hopkins University;Mehryar Mohri,NYU",arora@cs.jhu.edu;mdinitz@cs.jhu.edu;tmarino2@jhu.edu;mohri@cs.nyu.edu,Algorithms/Online Learning*; Theory/Game Theory and Computational Economics,Poster
3392,A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice,"In the $k$-nearest neighborhood model ($k$-NN), we are given a set of points $P$, and we shall answer queries $q$ by returning the $k$ nearest neighbors of $q$ in $P$ according to some metric. This concept is crucial in many areas of data analysis and data processing, e.g., computer vision, document retrieval and machine learning. Many $k$-NN algorithms have been published and implemented, but often the relation between parameters and accuracy of the computed $k$-NN is not explicit. We study property testing of $k$-NN graphs in theory and evaluate it empirically: given a point set $P \subset \mathbb{R}^\delta$ and a directed graph $G=(P,E)$, is $G$ a $k$-NN graph, i.e., every point $p \in P$ has outgoing edges to its $k$ nearest neighbors, or is it $\epsilon$-far from being a $k$-NN graph? Here, $\epsilon$-far means that one has to change more than an $\epsilon$-fraction of the edges in order to make $G$ a $k$-NN graph. We develop a randomized algorithm with one-sided error that decides this question, i.e., a property tester for the $k$-NN property, with complexity $O(\sqrt{n} k^2 / \epsilon^2)$ measured in terms of the number of vertices and edges it inspects, and we prove a lower bound of $\Omega(\sqrt{n})$. We evaluate our tester empirically on the $k$-NN models computed by various algorithms and show that it can be used to detect $k$-NN models with bad accuracy in significantly less time than the building time of the $k$-NN model.","Hendrik Fichtenberger*,TU Dortmund;Dennis Rohde,TU Dortmund",hendrik.fichtenberger@tu-dortmund.de;dennis.rohde@cs.tu-dortmund.de,Theory/Computational Complexity*; Algorithms/Classification; Theory/Learning Theory,Poster
3402,Banach Wasserstein GAN,"Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which  induces a notion of distance between probability distributions of images. So far the community has considered l^2 as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate the impact of the choice of norm on model performance and show state-of-the-art inception scores for non-progressive growing GANs on CIFAR-10.","Jonas Adler*,Royal Institute of Technology;Sebastian Lunz,University of Cambridge",jonasadl@kth.se;lunz@math.cam.ac.uk,Deep Learning/Adversarial Networks*; Deep Learning/Generative Models,Poster
3406,Provable Gaussian Embedding with One Observation,"Success of machine learning methods heavily relies on having an appropriate representation for data at hand. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about data. However, recently there has been a surge in approaches that learn how to encode the data automatically in a low dimensional space. Exponential family embedding provides a probabilistic framework for learning low-dimensional representation for various types of high-dimensional data. Though successful in practice, theoretical underpinnings for exponential family embeddings have not been established. In this paper, we study Gaussian embedding model and develop the first theoretical results for exponential family embedding models. First, we show that, under mild condition, the embedding structure can be learned from one observation by leveraging the parameter sharing between different contexts even though the data are dependent with each other.  Second, we study properties of two algorithms used for learning the embedding structure and establish convergence results for each of them. The first algorithm is based on a convex relaxation, while the other solved the non-convex formulation of the problem directly. Experiments demonstrate the effectiveness of our approach.","Ming Yu*,University of Chicago;Zhuoran Yang,;Tuo Zhao,Gatech;Mladen Kolar,University of Chicago Booth School of Business;Zhaoran Wang,Northwestern U",ming93@uchicago.edu;zy6@princeton.edu;tourzhao@gatech.edu;mkolar@chicagobooth.edu;zhaoranwang@gmail.com,Probabilistic Methods/Graphical Models*; Algorithms/Representation Learning; Applications/Signal Processing; Theory/Information Theo,Poster
3408,BRITS: Bidirectional Recurrent Imputation for Time Series,"Time series are widely used as signals in many classification/regression tasks. It is ubiquitous that time series contains many missing values. Given multiple correlated time series data, how to fill in missing values and to predict their class labels? Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. 
In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data.  Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation. BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data.
We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity.
Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies. ","Wei Cao*,Tsinghua University;Dong Wang,Duke University;Jian LI,Tsinghua University;Hao Zhou,Bytedance AI Lab;Lei Li,ByteDance AI Lab;Yitan Li,ByteDance.Inc",cao-w13@mails.tsinghua.edu.cn;wangdong2090@gmail.com;lapordge@gmail.com;haozhou0806@gmail.com;lileilab@bytedance.com;liyitan@bytedance.com,Deep Learning/Recurrent Networks*; Algorithms/Missing Data,Poster
3410,M-Walk: Learning to Walk in Graph  with Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given input query and a source node is an important problem in  applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem that has a known state transition model, but with sparse reward. To overcome the challenge, we develop a graph walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walking path) and map it into the Q-value, the policy and the state value. In order to effectively train the agent from sparse reward, we combine MCTS with the neural policy to generate trajectories with more positive rewards. From these trajectories, we improve the network in an off-policy manner using Q-learning, which indirectly improves the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy improvement step to learn the entire model. At testing stage, the MCTS is also applied with the neural policy to predict the target node. Experiment results on several graph-walking benchmarks show that we are able to learn better policies compared to other RL-based baseline methods, which are mainly based on policy gradient method. It also outperforms traditional KBC baselines.","Yelong Shen,Microsoft Research;Jianshu Chen*,Microsoft Research;Po-Sen Huang,Google DeepMind;Yuqing Guo,Microsoft Research;Jianfeng  Gao,Microsoft Research",yeshen@microsoft.com;chenjianshu@gmail.com;huang.person@gmail.com;yuqguo@microsoft.com;jfgao@microsoft.com,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Model-Based RL; Reinforcement Learning and Planning/Planni,Poster
3411,Extracting Relationships by Multi-Domain Matching,"In many biological and medical contexts, we construct a large labeled corpus by aggregating many sources to use in target prediction tasks.  Unfortunately, many of the sources may be irrelevant to our target task, so ignoring the structure of the dataset is detrimental.  This work proposes a novel approach, the Multiple Domain Matching Network (MDMN), to exploit this structure. MDMN embeds all data into a shared feature space while learning which domains share strong statistical relationships. These relationships are often insightful in their own right, and they allow domains to share strength without interference from irrelevant data. This methodology builds on existing distribution-matching approaches by assuming that source domains are varied and outcomes multi-factorial. Therefore, each domain should only match a relevant subset. Theoretical analysis shows that the proposed approach can have a tighter generalization bound than existing multiple-domain adaptation approaches.  Empirically, we show that the proposed methodology handles higher numbers of source domains (up to 21 empirically), and provides state-of-the-art performance on image, text, and multi-channel time series classification, including clinically relevant data of a novel treatment of Autism Spectrum Disorder.","Yitong Li*,Duke University;Michael Murias,Duke University;Geraldine Dawson,Duke University;David Carlson,Duke",yitong.li@duke.edu;michael.murias@duke.edu;geraldine.dawson@duke.edu;david.carlson@duke.edu,Neuroscience and Cognitive Science/Neuroscience,Poster
3415,Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses,"We present a general framework for designing efficient and scalable gradient computations for structured output prediction problems. These gradients can be used with algorithms such as back propagation for deep learning. While many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or $n$-gram loss, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\""{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computations for learning algorithms for two broad families of structured prediction loss functions, rational and tropical losses, which include the edit-distance and other string similarity measures as special cases.  Our methods cast the gradient computation as a shortest path problem on a weighted finite state transducer and allow training machine learning models (including neural networks) using complex structured losses which were heretofore intractable. We report experimental results confirming significant runtime improvement compared to direct methods.","Corinna Cortes,Google;Vitaly Kuznetsov,Google;Mehryar Mohri,NYU;Dmitry Storcheus*,Google Research;Scott Yang,D. E. Shaw & Co.",corinna@google.com;vitalyk@google.com;mohri@cs.nyu.edu;dstorcheus@google.com;yangs@cims.nyu.edu,Algorithms/Structured Prediction*; Applications/Natural Language Processing; Deep Learning/Efficient Training Methods,Poster
3427,Generative Probabilistic Novelty Detection with Adversarial Autoencoders,"Novelty detection is the problem of identifying whether a new data point is considered to be an inlier or an outlier. We assume that training data is available to describe only the inlier distribution. Recent approaches primarily leverage deep encoder-decoder network architectures to compute a reconstruction error that is used to either compute a novelty score or to train a one-class classifier. While we too leverage a novel network of that kind, we take a probabilistic approach and effectively compute how likely is that a sample was generated by the inlier distribution. We achieve this with two main contributions. First, we make the computation of the novelty probability feasible because we linearize the parameterized manifold capturing the underlying structure of the inlier distribution, and show how the probability factorizes and can be computed with respect to local coordinates of the manifold tangent space. Second, we improved the training of the autoencoder network. An extensive set of results show that the approach achieves state-of-the-art results on several benchmark datasets.","Stanislav Pidhorskyi*,West Virginia University;Ranya Almohsen,West Virginia University ;Gianfranco Doretto,West Virginia University",stpidhorskyi@mix.wvu.edu;ralmohse@mix.wvu.edu;gianfranco.doretto@mail.wvu.edu,Applications/Computer Vision*; Deep Learning/Adversarial Networks; Deep Learning/Deep Autoencoders; Deep Learning/Generati,Poster
3429,Diminishing Returns Shape Constraints for Interpretability and Regularization,"We investigate machine learning models that can provide diminishing returns and accelerating returns guarantees to capture prior knowledge or policies about how outputs should depend on inputs. Shape constraints are well-explored for one-dimensional models and generalized additive models (GAMs). We show that one can build flexible, nonlinear, multi-dimensional models using lattice functions with any combination of (ceterus paribus) concavity/convexity and monotonicity constraints on any subsets of features. We show better accuracy than shape-constrained GAMs, and more flexibility in shape constraint choice and training stability than for shape-constrained neural networks, which we also extend to handle the diminishing returns case. We demonstrate on real-world examples that additional shape constraints aid interpretability and can improve accuracy, especially when tuning-free regularization is useful.","Maya Gupta*,Google;Dara Bahri,Google;Andrew Cotter,Google;Kevin Canini,Google Research",mayagupta@google.com;dbahri@google.com;acotter@google.com;canini@google.com,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Regression; Applications/Denoising; Optimization; Theory/Regularization",Poster
3434,Scalable Hyperparameter Transfer Learning,"Bayesian optimization (BO) is a model-based approach for gradient-free black-box function optimization, such as hyperparameter optimization. Typically, BO relies on conventional Gaussian process (GP) regression, whose algorithmic complexity is cubic in the number of evaluations. As a result, GP-based BO cannot leverage large numbers of past function evaluations, for example, to warm-start related BO runs. We propose a multi-task adaptive Bayesian linear regression model for transfer learning in BO, whose complexity is linear in the function evaluations: one Bayesian linear regression model is associated to each black-box function optimization problem (or task), while transfer learning is achieved by coupling the models through a shared deep neural net. Experiments show that the neural net learns a representation suitable for warm-starting the black-box optimization problems and that BO runs can be accelerated when the target black-box function (e.g., validation loss) is learned together with other related signals (e.g., training loss). The proposed method was found to be at least one order of magnitude faster that methods recently published in the literature.","Valerio Perrone*,University of Warwick;Rodolphe Jenatton,Amazon;Matthias Seeger,Amazon;Cedric Archambeau,Amazon",v.perrone@warwick.ac.uk;jenatton@amazon.de;matthis@amazon.de;cedrica@amazon.com,Algorithms/AutoML*; Algorithms/Multitask and Transfer Learning,Poster
3436,Stochastic Nonparametric Event-Tensor Decomposition,"Tensor decompositions are fundamental tools for multiway data analysis. Existing approaches, however, ignore the valuable temporal information along with data, or simply discretize them into time steps so that important temporal patterns are easily missed. Moreover, most methods are limited to multilinear decomposition forms, and hence are unable to capture intricate, nonlinear relationships in data. To address these issues, we formulate event-tensors, to preserve the complete temporal information for multiway data, and propose a novel Bayesian nonparametric decomposition model. Our model can (1) fully exploit the time stamps to capture the critical, causal/triggering effects between the interaction events,  (2) flexibly estimate the complex relationships between the entities in tensor modes, and (3) uncover hidden structures from their temporal interactions. For scalable inference, we develop a doubly stochastic variational Expectation-Maximization algorithm to conduct an online decomposition. Evaluations on both synthetic and real-world datasets show that our model not only improves upon the predictive performance of existing methods, but also discovers interesting clusters underlying the data. ","Shandian Zhe*,University of Utah;Yishuai Du,University of Utah",zhe@cs.utah.edu;yishuaidu@gmail.com,Probabilistic Methods*; Applications/Matrix and Tensor Factorization; Probabilistic Methods/Bayesian Nonparametrics; P,Spotlight
3437,Scaling Gaussian Process Regression with Derivatives,"Gaussian processes (GPs) with derivatives are useful in many applications, including Bayesian optimization, implicit surface reconstruction, and terrain reconstruction. Fitting a GP to function values and derivatives at $n$ points in $d$ dimensions requires linear solves and log determinants with an ${n(d+1) \times n(d+1)}$ positive definite matrix -- leading to prohibitive $\mathcal{O}(n^3d^3)$ computations for standard direct methods. We propose $\mathcal{O}(nd)$ iterative solvers using fast matrix-vector multiplications (MVMs), together with pivoted Cholesky preconditioning that cuts the iterations to convergence by several orders of magnitude, allowing for fast kernel learning and prediction. Our approaches, together with dimensionality reduction, allows us to scale Bayesian optimization with derivatives to high-dimensional problems and large evaluation budgets.","David Eriksson*,Cornell University;Kun Dong,Cornell University;Eric Lee,Cornell University;David Bindel,Cornell University;Andrew Gordon Wilson,Cornell University",dme65@cornell.edu;kd383@cornell.edu;ehl59@cornell.edu;bindel@cs.cornell.edu;andrew@cornell.edu,Probabilistic Methods/Gaussian Processes,Poster
3440,Differentially Private Testing of Identity and Closeness of Discrete Distributions,"We study the fundamental problems of identity testing (goodness of fit), and closeness testing (two sample test) of distributions over $k$ elements, under differential privacy. While the problems have a long history in statistics,  finite sample bounds for these problems have only been established recently. 

In this work, we derive upper and lower bounds on the sample complexity of both the problems under $(\varepsilon, \delta)$-differential privacy. We provide optimal sample complexity algorithms for identity testing problem for all parameter ranges, and  the first results for closeness testing. Our closeness testing bounds are optimal in the sparse regime where the number of samples is at most $k$. 

Our upper bounds are obtained by privatizing non-private estimators for these problems. The non-private estimators are chosen to have small sensitivity. We propose a general framework to establish lower bounds on the sample complexity of statistical tasks under differential privacy. We show a bound on differentially private algorithms in terms of a coupling between the two hypothesis classes we aim to test. By constructing carefully chosen priors over the hypothesis classes, and using Le Cam's two point theorem we provide a general mechanism for proving lower bounds.  We believe that the framework can be used to obtain strong lower bounds for other statistical tasks under privacy.","Jayadev Acharya*,Cornell University;Ziteng Sun,Cornell University;Huanyu Zhang,Cornell University",acharya@cornell.edu;zs335@cornell.edu;hz388@cornell.edu,"Applications/Privacy, Anonymity, and Security*; Theory/Hardness of Learning and Approximations; Theory/Information Theory; Theory/Learning",Spotlight
3442,Bayesian Adversarial Learning,"Deep neural networks have been known to be vulnerable to adversarial attacks, raising lots of security concerns in the practical deployment. Popular defensive approaches can be formulated as a (distributionally) robust optimization problem, which minimizes a ``point estimate'' of worst-case loss derived from either per-datum perturbation or adversary data-generating distribution within certain pre-defined constraints. This point estimate ignores potential test adversaries that are beyond the pre-defined constraints. The model robustness might deteriorate sharply in the scenario of stronger test adversarial data. In this work, a novel robust training framework is proposed to alleviate this issue, Bayesian Robust Learning, in which a  distribution is put on the adversarial data-generating distribution to account for the uncertainty of the adversarial data-generating process. The uncertainty directly helps to consider the potential adversaries that are stronger than the point estimate in the cases of distributionally robust optimization. The uncertainty of model parameters is also incorporated to accommodate the full Bayesian framework. We design a scalable Markov Chain Monte Carlo sampling strategy to obtain the posterior distribution over model parameters. Various experiments are conducted to verify the superiority of BAL over existing adversarial training methods. ","Nanyang Ye,University of Cambridge;Zhanxing Zhu*,Peking University",yn272@cam.ac.uk;zhanxing.zhu@pku.edu.cn,Deep Learning,Poster
3443,Efficient Convex Completion of Coupled Tensors using Coupled Nuclear Norms,"Coupled norms have emerged as a convex method to solve coupled tensor completion.    A limitation with coupled norms is that they only induce low rankness using the multilinear rank of coupled tensors. In this paper, we introduce a new set of coupled norms known as coupled nuclear norms by constraining the CP rank of coupled tensors. We propose new coupled completion models using the coupled nuclear norms as regularizers, that can be optimized using computationally efficient optimization methods. We derive excess risk bounds for proposed coupled completion models and show that proposed norms lead to better performances.  Through simulation and real-data experiments, we demonstrate that proposed norms achieve better performances for coupled completion compared to existing coupled norms. ","Kishan Wimalawarne*,Kyoto University;Hiroshi Mamitsuka,""Kyoto University, Japan""",kishanwn@gmail.com;mami@kuicr.kyoto-u.ac.jp,Algorithms/Sparsity and Compressed Sensing*; Algorithms/Missing Data; Theory/Regularization,Poster
3444,Maximizing Induced Cardinality Under a Determinantal Point Process,"Determinantal point processes (DPPs) are well-suited to recommender systems where the goal is to generate collections of diverse, high-quality items. In the existing literature this is usually formulated as finding the mode of the DPP (the so-called MAP set). However, the MAP objective inherently assumes that the DPP models ""optimal"" recommendation sets, and yet obtaining such a DPP is nontrivial when there is no ready source of example optimal sets. In this paper we advocate an alternative framework for applying DPPs to recommender systems. Our approach assumes that the DPP simply models user engagements with recommended items, which is more consistent with how DPPs for recommender systems are typically trained.  With this assumption, we are able to formulate a metric that measures the expected number of items that a user will engage with.  We formalize this optimization of this metric as the Maximum Induced Cardinality (MIC) problem. Although the MIC objective is not submodular, we show that it can be approximated by a submodular function, and that empirically it is well-optimized by a greedy algorithm.","Jennifer Gillenwater*,Google;Alex Kulesza,Google Research;Sergei Vassilvtiskii,Google;Zelda Mariet,Massachusetts Institute of Technology",jengi@google.com;kulesza@google.com;sergeiv@google.com;zelda@csail.mit.edu,Optimization/Submodular Optimization*; Algorithms/Structured Prediction; Applications/Information Retrieval; Applications/Recommen,Poster
3449,Causal Inference with Noisy and Missing Covariates via Matrix Factorization,"Valid causal inference in observational studies often requires controlling for confounders. However, in practice measurements of confounders may be noisy, and can lead to biased estimates of causal effects. We show that we can reduce the bias caused by measurement noise using a large number of noisy measurements of the underlying confounders. We propose the use of matrix factorization to infer the confounders from noisy covariates, a flexible and principled framework that adapts to missing values, accommodates a wide variety of data types, and can augment a wide variety of causal inference methods. We bound the error for the induced average treatment effect estimator and show it is consistent in a linear regression setting, using Exponential Family Matrix Completion preprocessing. We demonstrate the effectiveness of the proposed procedure in numerical experiments  with both synthetic data and real clinical data.","Nathan Kallus,Cornell Tech;Xiaojie Mao*,Cornell University;Madeleine Udell,Cornell University",kallus@cornell.edu;xm77@cornell.edu;udell@cornell.edu,Probabilistic Methods/Causal Inference*; Algorithms/Missing Data; Algorithms/Unsupervised Learning; Applications/Matrix and Tensor F,Poster
3451,rho-POMDPs have Lipschitz-Continuous epsilon-Optimal Value Functions,"Many state-of-the-art algorithms for solving Partially Observable Markov Decision Processes (POMDPs) rely on turning the problem into a ‚Äúfully observable‚Äù problem‚Äîa belief MDP‚Äîand exploiting the piece-wise linearity and convexity (PWLC) of the optimal value function in this new state space (the belief simplex ‚àÜ). This approach has been extended to solving œÅ-POMDPs‚Äîi.e., for information-oriented criteria‚Äîwhen the reward œÅ is convex in ‚àÜ. General œÅ-POMDPs can also be turned into ‚Äúfully observable‚Äù problems, but with no means to exploit the PWLC property. In this paper, we focus on POMDPs and œÅ-POMDPs with Œª œÅ -Lipschitz reward function, and demonstrate that, for finite horizons, the optimal value function is Lipschitz-continuous. Then, value function approximators are proposed for both upper- and lower-bounding the optimal value function, which are shown to provide uniformly improvable bounds. This allows proposing two algorithms derived from HSVI which are empirically evaluated on various benchmark problems.","Mathieu Fehr,√âcole Normale Sup√©rieure;Olivier Buffet*,INRIA - LORIA;Vincent Thomas,Universit√© de Lorraine / LORIA;Jilles Dibangoye,INSA Lyon, INRIA",mathieu.fehr@ens.fr;olivier.buffet@loria.fr;vincent.thomas@loria.fr;jilles-steeve.dibangoye@insa-lyon.fr,Reinforcement Learning and Planning/Markov Decision Processes*; Reinforcement Learning and Planning/Planning; Reinforcement Learning and Planning/Reinforcemen,Poster
3452,Online Structure Learning for Feed-Forward and Recurrent Sum-Product Networks,"Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes a new online structure learning technique for feed-forward and recurrent SPNs. The algorithm is demonstrated on real-world datasets with continuous features for which it is not clear what network architecture might be best, including sequence datasets of varying length.","Abdullah Rashwan*,University of Waterloo;Agastya Kalra,University of Waterloo;Pascal Poupart,University of Waterloo;Prashant Doshi,University of Georgia;George Trimponias,Huawei;Wilson Hsu,University of Waterloo",arashwan@uwaterloo.ca;agastya.kalra@gmail.com;ppoupart@uwaterloo.ca;pdoshi@cs.uga.edu;george.trimponias@gmail.com;wwhsu@uwaterloo.ca,Algorithms/Online Learning*; Algorithms/Model Selection and Structure Learning; Algorithms/Unsupervised Learning; Deep Lear,Poster
3454,Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss,"Uncertainty sampling, a popular active learning algorithm, is used to reduce the amount of data required to learn a classifier, but it is observed in practice to converge to a variety of values depending on the initialization and sometimes to even better values than the convergence value of random sampling. In this work, we give a theoretical explanation of this phenomena, showing that uncertainty sampling on a convex (e.g., logistic) loss can be interpreted as performing a pre-conditioned stochastic gradient step on the zero-one loss. Experiments on synthetic and real datasets support this connection.","Stephen Mussmann*,Stanford University;Percy Liang,",mussmann@stanford.edu;pliang@cs.stanford.edu,Algorithms/Active Learning,Poster
3455,A Probabilistic U-Net for Segmentation of Ambiguous Images,"Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.","Simon Kohl*,German Cancer Research Center;Bernardino Romera-Paredes,DeepMind;Clemens Meyer,DeepMind;Jeffrey De Fauw,DeepMind;Joseph R. Ledsam,DeepMind;Klaus Maier-Hein,German Cancer Research Center;S. M. Ali Eslami,DeepMind;Danilo Jimenez Rezende,Google DeepMind;Olaf Ronneberger,DeepMind",simon.kohl@dkfz.de;brp@google.com;meyerc@google.com;defauw@google.com;jledsam@google.com;k.maier-hein@dkfz.com;aeslami@google.com;danilor@google.com;olafr@google.com,Applications/Image Segmentation*; Applications/Computer Vision; Deep Learning/CNN Architectures; Deep Learning/Generative Mod,Spotlight
3468,Unorganized Malicious Attacks Detection,"Recommender system has attracted much attention during the past decade. Many attack detection algorithms have been developed for better recommendations, mostly focusing on shilling attacks, where an attack organizer produces a large number of user profiles by the same strategy to promote or demote an item. This work considers a different attack style: unorganized malicious attacks, where attackers individually utilize a small number of user profiles to attack different items without any organizer. This attack style occurs in many real applications, yet relevant study remains open. We first formulate unorganized malicious attacks detection as a matrix completion problem, and propose the Unorganized Malicious Attacks detection (UMA) approach, based on the alternating splitting augmented Lagrangian method. We verify, both theoretically and empirically, the effectiveness of our proposed approach.","Ming Pang*,Nanjing University;Wei Gao,Nanjing Univ;Min Tao,Nanjing University;Zhi-Hua Zhou,Nanjing university",pangm@lamda.nju.edu.cn;gaow@lamda.nju.edu.cn;taom@nju.edu.cn;zhouzh@lamda.nju.edu.cn,"Applications/Recommender Systems*; Algorithms/Ranking and Preference Learning; Applications/Privacy, Anonymity, and Security",Poster
3470,Causal Inference via Kernel Deviance Measures,"Discovering the causal structure among a set of variables is a fundamental problem in many areas of science. In this paper, we propose Kernel Conditional Deviance for Causal Inference (KCDC) a fully nonparametric causal discovery method based on purely observational data. From a novel interpretation of the notion of asymmetry between cause and effect, we derive a corresponding asymmetry measure using the framework of reproducing kernel Hilbert spaces. Based on this, we propose three decision rules for causal discovery. We demonstrate the wide applicability of our method across a range of diverse synthetic datasets. Furthermore, we test our method on real-world time series data and the real-world benchmark dataset Tubingen Cause-Effect Pairs where we outperform existing state-of-the-art methods.","Jovana Mitrovic*,University of Oxford;Dino Sejdinovic,University of Oxford;Yee Whye Teh,University of Oxford",jovana.mitrovic@spc.ox.ac.uk;dino.sejdinovic@stats.ox.ac.uk;y.w.teh@stats.ox.ac.uk,Probabilistic Methods/Causal Inference*; Algorithms/Kernel Methods,Spotlight
3472,Bayesian Alignments of Warped Multi-Output Gaussian Processes,"We propose a novel Bayesian approach to modelling nonlinear alignments of time series based on latent shared information. We apply the method to the real-world problem of finding common structure in the sensor data of wind turbines introduced by the underlying latent and turbulent wind field. The proposed model allows for both arbitrary alignments of the inputs and non-parametric output warpings to transform the observations. This gives rise to multiple deep Gaussian process models connected via latent generating processes. We present an efficient variational approximation based on nested variational compression and show how the model can be used to extract shared information between dependent time series, recovering an interpretable functional decomposition of the learning problem. We show results for an artificial data set and real-world data of two wind turbines.","Markus Kaiser*,Technical University Munich;Clemens Otte,Siemens;Thomas Runkler,Technical University of Munich;Carl Henrik Ek,Bristol University",markus.kaiser@in.tum.de;clemens.otte@siemens.com;Thomas.Runkler@siemens.com;carlhenrik.ek@bristol.ac.uk,Probabilistic Methods/Gaussian Processes*; Probabilistic Methods/Hierarchical Models; Probabilistic Methods/Latent Variable Models; Proba,Poster
3481,Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks," Spiking neural networks (SNNs) are positioned to enable spatio-temporal information processing and ultra-low power event-driven neuromorphic hardware. However,  SNNs are yet to reach the same performances of conventional deep artificial neural networks (ANNs), a long-standing challenge due to complex dynamics and non-differentiable spike events encountered in training. The existing SNN error backpropagation (BP) methods are limited in terms of scalability, lack of proper handling of spiking discontinuities, and/or mismatch between the rate-coded loss function and computed gradient. We present a hybrid macro/micro level backpropagation algorithm (HM2-BP) for training multi-layer SNNs. The temporal effects are precisely captured by the proposed spike-train level post-synaptic potential (S-PSP) at the microscopic level.  The rate-coded errors are defined at the macroscopic level,  computed and back-propagated across both macroscopic and microscopic levels.  Different from existing BP methods, HM2-BP directly computes the gradient of the rate-coded loss function w.r.t tunable parameters. We evaluate the proposed HM2-BP algorithm by training deep fully connected and convolutional SNNs based on  the static MNIST [13] and dynamic neuromorphic N-MNIST [22] datasets. HM2-BP achieves an accuracy level of 99.49% and 98.88% for MNIST and N-MNIST, respectively, outperforming the best reported performances obtained from the existing SNN BP algorithms. It also achieves competitive performances surpassing those of conventional deep learning models when dealing with asynchronous spiking streams.","Yingyezhe Jin*,Texas A&M University;Wenrui Zhang,Texas A&M University;Peng Li,Texas A&M University",jyyz@tamu.edu;zhangwenrui@tamu.edu;pli@tamu.edu,Deep Learning*; Deep Learning/Biologically Plausible Deep Networks,Poster
3489,Gen-Oja: Simple & Efficient Algorithm for Streaming Generalized Eigenvector Computation,"In this paper, we study the problems of principle Generalized Eigenvector computation and Canonical Correlation Analysis in the stochastic setting. We propose a simple and efficient algorithm for these problems. We prove the global convergence of our algorithm, borrowing ideas from the theory of fast-mixing Markov chains and two-Time-Scale Stochastic Approximation, showing that it achieves the optimal rate of convergence. In the process, we develop tools for understanding stochastic processes with Markovian noise which might be of independent interest.","Kush Bhatia*,UC Berkeley;Aldo Pacchiano,UC Berkeley;Nicolas Flammarion,UC Berkeley;Peter Bartlett,""University of California, Berkeley"";Michael Jordan,UC Berkeley",kushbhatia@berkeley.edu;pacchiano@berkeley.edu;flammarion@berkeley.edu;bartlett@cs.berkeley.edu;jordan@cs.berkeley.edu,Optimization/Non-Convex Optimization*; Algorithms/Online Learning; Algorithms/Stochastic Methods; Theory/Learning Theory,Poster
3496,Efficient online algorithms for fast-rate regret bounds under sparsity,"We consider the online convex optimization problem. In the setting of arbitrary sequences and finite set of parameters, we establish a new fast-rate  quantile regret bound. Then we investigate the optimization into the $\ell_1$-ball by discretizing the parameter space. Our algorithm is projection free and we propose an efficient solution by restarting the algorithm on adaptive discretization grids. In the adversarial setting, we develop an algorithm that achieves several rates of convergence with different dependences on the sparsity of the objective. In the i.i.d. setting, we establish new risk bounds that are adaptive to the sparsity of the problem and to the regularity of the risk (ranging from a rate $1/\sqrt{T}$ for general convex risk to $1/T$ for strongly convex risk). These results generalize previous works on sparse online learning. They are obtained under a weak assumption on the risk (≈Åojasiewicz's assumption) that allows multiple optima which is crucial when dealing with degenerate situations.","Pierre Gaillard*,INRIA;Olivier Wintenberger,",pierre@gaillard.me;olivier.wintenberger@upmc.fr,Algorithms/Online Learning*; Algorithms/Sparsity and Compressed Sensing; Optimization/Convex Optimization; Theory/Learni,Poster
3497,GILBO: One Metric to Measure Them All,"We propose a simple, tractable lower bound on the mutual information contained
in the joint generative density of any latent variable generative model: the GILBO
(Generative Information Lower BOund). It offers a data-independent measure
of the complexity of the learned latent variable description, giving the log of
the effective description length. It is well-defined for both VAEs and GANs. We
compute the GILBO for 800 GAN s and VAE s each trained on four datasets (MNIST,
FashionMNIST, CIFAR-10 and CelebA) and discuss the results.","Alex Alemi*,Google;Ian Fischer,Google",alemi@google.com;iansf@google.com,Deep Learning/Generative Models*; Deep Learning/Adversarial Networks; Probabilistic Methods/Latent Variable Models; Theory/In,Spotlight
3499,Predictive Uncertainty Estimation via Prior Networks,"Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible data uncertainty and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through model uncertainty or as data uncertainty. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models distributional uncertainty. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST dataset, where they are found to outperform previous methods. Experiments on synthetic and MNIST data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty.","Andrey Malinin*,University of Cambirdge;Mark Gales,University of Cambridge",am969@cam.ac.uk;mjfg@eng.cam.ac.uk,Deep Learning*; Deep Learning/Predictive Models; Deep Learning/Supervised Deep Networks,Poster
3510,Dual Policy Iteration,"Recently, a novel class of Approximate Policy Iteration (API) algorithms have demonstrated impressive practical performance (e.g., ExIt from [1], AlphaGo-Zero from [2]). This new family of algorithms maintains, and alternately optimizes, two policies: a fast, reactive policy (e.g., a deep neural network) deployed at test time, and a slow, non-reactive policy (e.g., Tree Search), that can plan multiple steps ahead. The reactive policy is updated under supervision from the non-reactive policy, while the non-reactive policy is improved with guidance from the reactive policy. In this work we study this Dual Policy Iteration (DPI) strategy in an alternating optimization framework and provide a convergence analysis that extends existing API theory. We also develop a special instance of this framework which reduces the update of non-reactive policies to model-based optimal control using learned local models, and provides a theoretically sound way of unifying model-free and model-based RL approaches with unknown dynamics. We demonstrate the efficacy of our approach on various continuous control Markov Decision Processes.","Wen Sun*,Carnegie Mellon University; Geoffrey Gordon,(Carnegie Mellon University);Byron Boots,Georgia Institute of Technology;J. Bagnell,Carnegie Mellon University, USA",wensun@cs.cmu.edu;ggordon@cs.cmu.edu;bboots@cc.gatech.edu;dbagnell@ri.cmu.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
3518,A probabilistic population code based on neural samples,"Sensory processing is often characterized as implementing probabilistic inference: networks of neurons compute posterior beliefs over unobserved causes given the sensory inputs. How these beliefs are computed and represented by neural responses is much-debated (Fiser et al. 2010, Pouget et al. 2013). A central debate concerns the questions of whether neural responses represent samples of latent variables (Hoyer \& Hyvarinnen 2003) or parameters of their distributions (Ma et al. 2006) with efforts being made to distinguish between them (Grabska-Barwinska et al. 2013).
A separate debate addresses the question of whether neural responses are proportionally related to the encoded probabilities (Barlow 1969), or proportional to the logarithm of those probabilities (Jazayeri \& Movshon 2006, Beck et al. 2006, Beck et al. 2012). 
Here, we show that these alternatives -- contrary to common assumptions -- are not mutually exclusive and that the very same system can be compatible with all of them.
As a central analytical result, we show that modeling neural responses in area V1 as samples from a posterior distribution over latents in a linear Gaussian model of the image implies that those neural responses form a probabilistic population code (PPC, Beck et al. 2006). In particular, the posterior distribution over some experimenter-defined variable like ``orientation'' is part of the exponential family with sufficient statistics that are linear in the neural sampling-based firing rates.","Sabyasachi Shivkumar,University of Rochester;Richard D. Lange,University of Rochester;Ankani Chattoraj,University of Rochester;Ralf Haefner*,University of Rochester",sshivkum@ur.rochester.edu;rlange@ur.rochester.edu;achattor@ur.rochester.edu;ralf.haefner@gmail.com,Neuroscience and Cognitive Science/Neural Coding*; Probabilistic Methods/Latent Variable Models,Oral
3522,Manifold-tiling Localized Receptive Fields are Optimal in Similarity-preserving Neural Networks,"Many neurons in the brain, such as place cells in the rodent hippocampus, have localized receptive fields, i.e. they respond to a small neighborhood of stimulus space. What is the functional significance of such representations and how can they arise? Here, we propose that localized receptive fields emerge in similarity-preserving networks of rectifying neurons that learn low-dimensional manifolds populated by sensory inputs.  Numerical simulations of such networks on standard datasets yield manifold-tiling localized receptive fields. More generally, we show analytically that, for data lying on symmetric manifolds, optimal solutions of objectives, from which similarity-preserving networks are derived, have localized receptive fields. Therefore, nonnegative similarity-preserving mapping (NSM) implemented by neural networks can model representations of continuous manifolds in the brain.","Anirvan Sengupta,Rutgers University;Cengiz Pehlevan*,Flatiron Institute;Mariano Tepper,Flatiron Institute;Alex Genkin,NYU;Dmitri Chklovskii,Flatiron Institute",anirvans.physics@gmail.com;cpehlevan@flatironinstitute.org;mtepper@flatironinstitute.org;alexander.genkin@gmail.com;dchklovskii@flatironinstitute.org,Neuroscience and Cognitive Science/Neural Coding*; Neuroscience and Cognitive Science/Plasticity and Adaptation,Poster
3533,On the Convergence and Robustness of Training GANs with Regularized Optimal Transport,"Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized Optimal Transport (OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10 images.  Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.","Maziar Sanjabi*,USC;Jimmy Ba,;Meisam Razaviyayn,USC;Jason Lee,USC",maziar.sanjabi@gmail.com;jimmy@cs.toronto.edu;razaviya@usc.edu;jasonlee@marshall.usc.edu,Deep Learning/Adversarial Networks*; Deep Learning/Generative Models; Optimization/Non-Convex Optimization,Poster
3534,Model-Agnostic Private Learning,"We design differentially private learning algorithms that are agnostic to the learning model assuming access to limited amount of unlabeled public data. First, we give a new differentially private algorithm for answering a sequence of $m$ online classification queries (given by a sequence of $m$ unlabeled public feature vectors) based on a private training set. Our private algorithm follows the paradigm of subsample-and-aggregate, in which any generic non-private learner is trained on disjoint subsets of the private training set, then for each classification query, the votes of the resulting classifiers ensemble are aggregated in a differentially private fashion. Our private aggregation is based on a novel combination of distance-to-instability framework \cite{ST13} and the sparse-vector technique \cite{DNRRV09,HR10}.  We show that our algorithm makes a conservative use of the privacy budget. In particular, if the underlying non-private learner yields classification error at most $\alpha\in (0, 1)$, then our construction answers more queries than what is implied by a straightforward application of advanced composition theorems from differential privacy; and by at least a factor of $1/\alpha$ in some cases. Next, we apply the knowledge transfer technique to construct a private learner that outputs a classifier, which can be used to answer unlimited number of queries. In the (agnostic) PAC model, we analyze our construction and prove upper bounds on the sample complexity for both the realizable and the non-realizable cases. As in non-private sample complexity, our bounds are completely characterized by the VC dimension of the concept class. ","Raef Bassily*,The Ohio State University;Abhradeep Guha Thakurta,University of California Santa Cruz;Om Dipakbhai Thakkar,Boston University",rbbassily@gmail.com;guhathakurta.abhradeep@gmail.com;omthkkr@gmail.com,"Theory/Learning Theory*; Applications/Privacy, Anonymity, and Security",Oral
3538,Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders,"Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For example, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model. In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity. We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints. Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.","Tengfei Ma,IBM Research;Jie Chen*,IBM Research;Cao Xiao,IBM Research ",Tengfei.Ma1@ibm.com;chenjie@us.ibm.com;cxiao@us.ibm.com,Deep Learning/Generative Models*; Deep Learning/Deep Autoencoders,Poster
3539,Provably Correct Automatic Sub-Differentiation for Qualified Programs,"The \emph{Cheap Gradient Principle}~\citep{Griewank:2008:EDP:1455489} --- the computational cost of computing a $d$-dimensional vector of  partial derivatives of a scalar function is nearly the same (often within a factor of $5$)  as that of simply computing the scalar function itself --- is of central importance in optimization; it allows us to quickly obtain (high-dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing sub-derivatives: widely used ML libraries, including TensorFlow and PyTorch, do \emph{not} correctly compute (generalized) sub-derivatives even on simple differentiable examples. This work considers the question: is there a \emph{Cheap Sub-gradient Principle}?  Our main result shows that, under certain restrictions on our library of non-smooth functions (standard in non-linear programming), provably correct generalized sub-derivatives can be computed at a computational cost that is within a (dimension-free) factor of $6$ of the cost of computing the scalar function itself.  ","Sham Kakade,University of Washington;Jason Lee*,USC",sham@cs.washington.edu;jasonlee@marshall.usc.edu,Optimization/Non-Convex Optimization*; Algorithms/AutoML; Deep Learning/Optimization for Deep Networks,Poster
3541,"Deep Homogeneous Mixture Models: Representation, Separation, and Approximation","At their core, many unsupervised learning models provide a compact representation of homogeneous density mixtures, but their similarities and differences are not always clearly understood. In this work, we formally establish the relationships among latent tree graphical models (including special cases such as hidden Markov models and tensorial mixture models), hierarchical tensor formats and sum-product networks. Based on this connection, we then give a unified treatment of exponential separation in \emph{exact} representation size between deep mixture architectures and shallow ones. In contrast, for \emph{approximate} representation, we show that the conditional gradient algorithm can approximate any homogeneous mixture within $\epsilon$ accuracy by combining $O(1/\epsilon^2)$ ``shallow'' architectures, where the hidden constant may decrease (exponentially) with respect to the depth. Our experiments on both synthetic and real datasets confirm the benefits of depth in density estimation.","Priyank Jaini*,University of Waterloo;Pascal Poupart,University of Waterloo;Yaoliang Yu,University of Waterloo",pjaini@uwaterloo.ca;ppoupart@uwaterloo.ca;yaoliang.yu@uwaterloo.ca,Algorithms/Density Estimation*; Deep Learning/Generative Models; Probabilistic Methods/Graphical Models; Probabilistic Methods,Poster
3543,"Parameters as interacting particles: asymptotic scaling, convexity, and error of neural networks"," The performance of neural networks on high-dimensional data distributions suggests that it may be possible to parameterize a representation of a \emph{given} high-dimensional function with controllably small errors, potentially outperforming standard interpolation methods.  We demonstrate, both theoretically and numerically, that this is indeed the case.  We map the parameters of a neural network to a system of particles relaxing with an interaction potential determined by the loss function.  We show that in the limit that the number of parameters $n$ is large, the landscape of the mean-squared error becomes convex and the representation error in the function scales as $O(n^{-1})$.  As a consequence, we rederive the universal approximation theorem for neural networks but we additionally prove that the optimal representation can be achieved through stochastic gradient descent, the algorithm ubiquitously used for parameter optimization in machine learning.  In the asymptotic regime, we study the fluctuations around the optimal representation and show that they arise at a scale $O(n^{-1})$, for suitable choices of the batch size.  These fluctuations in the landscape demonstrate the necessity of the noise inherent in stochastic gradient descent and our analysis provides a precise scale for tuning this noise.  Our results apply to both single and multi-layer neural networks, as well as standard kernel methods like radial basis functions.  From our insights, we extract several practical guidelines for large scale applications of neural networks, emphasizing the importance of both noise and quenching, in particular.","Grant Rotskoff*,New York University;Eric Vanden-Eijnden,New York University",rotskoff@cims.nyu.edu;eve2@cims.nyu.edu,Theory/Large Deviations and Asymptotic Analysis*; Theory/Statistical Physics of Learning,Poster
3550,Multitask Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies,"We introduce a new RL problem where the agent is required to execute a given subtask graph which describes a set of subtasks and their dependency. Unlike existing approaches that explicitly describe what the agent should do, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform a complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSS) which encodes the subtask graph using a recursive neural network. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy to pre-train our NSS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform a complex reasoning to find the optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSS can be further improved by combining it with MCTS.","SUNGRYULL SOHN*,UNIVERSITY OF MICHIGAN;Junhyuk Oh,U Michigan;Honglak Lee,University of Michingan Ann Arbor, USA",srsohn@umich.edu;junhyuk@umich.edu;honglak@eecs.umich.edu,Algorithms/Multitask and Transfer Learning*; Deep Learning; Reinforcement Learning and Planning; Reinforcement Learning and Planning/Reinforce,Poster
3551,A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks,"Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is the fundamental requirement to deploy a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in extreme cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that {the proposed method} enjoys broader usage by applying it to class incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.","Kimin Lee*,KAIST;Kibok Lee,University of Michigan;Honglak Lee,University of Michingan Ann Arbor, USA;Jinwoo Shin,KAIST",kiminlee@kaist.ac.kr;kibok@umich.edu;honglak@eecs.umich.edu;jinwoos@kaist.ac.kr,Deep Learning,Spotlight
3572,End-to-End Differentiable Physics for Learning and Control,"We present a differentiable physics engine that can be integrated as a module in deep neural networks for end-to-end learning.  As a result, structured physics knowledge can be embedded into larger systems, allowing them, for example, to match observations by performing precise simulations, while achieves high sample efficiency.  Specifically, in this paper we demonstrate how to perform backpropagation analytically through a physical simulator defined via a linear complementarity problem.  Unlike traditional finite difference methods, such gradients can be computed analytically, which allows for greater flexibility of the engine. Through experiments in diverse domains, we highlight the system's ability to learn physical parameters from data, efficiently match and simulate observed visual behavior, and readily enable control via gradient-based planning methods.","Filipe de Avila Belbute-Peres*,Carnegie Mellon University;Kevin Smith,MIT;Kelsey Allen,MIT;Joshua Tenenbaum,MIT;Zico Kolter,Carnegie Mellon University",filiped@cs.cmu.edu;k2smith@mit.edu;krallen@mit.edu;jbt@mit.edu;zkolter@cs.cmu.edu,Deep Learning*; Optimization; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning an,Spotlight
3574,BRUNO: A Deep Recurrent Model for Exchangeable Data,"We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection.","Iryna Korshunova*,Ghent University;Jonas Degrave,U. Ghent;Ferenc Huszar,Twitter;Yarin Gal,University of OXford;Arthur Gretton,Gatsby Computational Neuroscience Unit;Joni Dambre,Ghent University",iryna.korshunova@ugent.be;jonas.degrave@ugent.be;ferenc.huszar@gmail.com;yarin.gal@cs.ox.ac.uk;arthur.gretton@gmail.com;joni.dambre@ugent.be,Probabilistic Methods*; Deep Learning/Few-Shot Learning Approaches; Deep Learning/Generative Models; Deep Learning ->,Poster
3582,Stimulus domain transfer in recurrent models for large scale cortical population prediction on video," To better understand the representations in visual cortex, we need to generate better predictions of neural activity in awake animals presented with their ecological input: natural video. Despite recent advances in models for static images, models for predicting responses to natural video are scarce and standard linear-nonlinear models perform poorly. We developed a new deep recurrent network architecture that predicts inferred spiking activity of thousands of mouse V1 neurons simultaneously recorded with two-photon microscopy, while accounting for confounding factors such as the animal's gaze position and brain state changes related to running state and pupil dilation. Powerful system identification models provide an opportunity to gain insight into cortical functions through {\em in silico} experiments that can subsequently be tested in the brain. However, in many cases this approach requires that the model is able to generalize to stimulus statistics that it was not trained on, such as band-limited noise and other parameterized stimuli. We investigated these domain transfer properties in our model and find that our model trained on natural images is able to correctly predict the orientation tuning of neurons in responses to artificial noise stimuli. Finally, we show that we can fully generalizing from movies to noise and maintain high predictive performance on both by fine-tuning the final layer's weights on a network otherwise trained on natural movies. The converse, however, is not true.","Fabian Sinz*,;Alexander Ecker,University of T√ºbingen;Paul Fahey,Bayl;Edgar Walker,Baylor College of Medicine;Erick Cobos,Baylor College of Medicine;Emmanouil Froudarakis,Baylor College of Medicine;Dimitri Yatsenko,Baylor College of Medicine;Xaq Pitkow,Rice;Jacob Reimer,Baylor College of Medicine;Andreas Tolias,Baylor College of Medicine",sinz@bcm.edu;alexander.ecker@uni-tuebingen.de;paul.fahey@bcm.edu;eywalker@bcm.edu;emcobost@gmail.com;froudara@bcm.edu;dvyatsen@bcm.edu;xaq@rice.edu;reimer@bcm.edu;astolias@bcm.edu,Neuroscience and Cognitive Science/Neuroscience*; Neuroscience and Cognitive Science/Neural Coding,Poster
3585,Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction,"Machine understanding of complex images is a key goal of artificial intelligence. One challenge underlying this task is that visual scenes contain multiple inter-related objects, and that global context plays an important role in interpreting the scene. A natural modeling framework for capturing such effects is structured prediction, which optimizes over complex labels, while modeling within-label interactions. However, it is unclear what principles should guide the design of a structured prediction model that utilizes the power of deep learning components. Here we propose a design principle for such architectures that follows from a natural requirement of permutation invariance. We prove a necessary and sufficient characterization for architectures that follow this invariance, and discuss its implication on model design. Finally, we show that the resulting model achieves new state of the art results on the Visual Genome scene graph labeling benchmark, outperforming all recent approaches.","Roei Herzig*,Tel Aviv University;Moshiko Raboh,Tel Aviv University;Gal Chechik,;Jonathan Berant,Tel Aviv University;Amir Globerson,Tel Aviv University, Google",roeiherz@gmail.com;shikorab@gmail.com;gal.chechik@gmail.com;joberant@cs.tau.ac.il;amir.globerson@gmail.com,Algorithms/Structured Prediction*; Applications/Visual Scene Analysis and Interpretation,Poster
3588,Distributed Multi-Player Bandits - a Game of Thrones Approach,"We consider a multi-armed bandit game where N players compete for K arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are independent and identically distributed. Performance is measured using the expected sum of regrets, compared to optimal assignment of arms to players. We assume that each player only knows her actions and the reward she received each turn. Players cannot observe the actions of other players, and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O\left(\log^{2}T\right) . This is the first algorithm to achieve a poly-logarithmic regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible.  ","Ilai Bistritz*,Bar-Ilan University;Amir Leshem,Bar-Ilan University",ilaibist@gmail.com;leshem.amir2@gmail.com,Algorithms/Bandit Algorithms*; Theory/Game Theory and Computational Economics,Poster
3599,Efficient Loss-Based Decoding On Graphs For Extreme Classification,"In extreme classification problems, learning algorithms are required to map instances to labels from an extremely large label set.
  We build on a recent extreme classification framework with logarithmic time and space (LTLS), and on a general approach for error correcting output coding (ECOC) with loss-based decoding, and introduce a flexible and efficient approach accompanied by theoretical bounds.
  Our framework employs output codes induced by graphs, for which we show how to perform efficient loss-based decoding to potentially improve accuracy.
  In addition, our framework offers a tradeoff between accuracy, model size and prediction time.
  We show how to find the sweet spot of this tradeoff using only the training data.
Our experimental study demonstrates the validity of our assumptions and claims,  and shows that our method is competitive with state-of-the-art algorithms.","Itay Evron*,Technion;Edward Moroshko,Technion;Koby Crammer,Technion",evron.itay@gmail.com;edward.moroshko@gmail.com;koby@ee.technion.ac.il,Algorithms/Classification,Poster
3600,Chaining Mutual Information and Tightening Generalization Bounds,"Bounding the generalization error of learning algorithms has a long history, that yet falls short in explaining various generalization successes including those of deep learning. Two important difficulties are (i) exploiting the dependencies between hypotheses, (ii) exploiting the dependencies between the algorithm's input and output. Progress on the first point was made with the chaining method, used in the VC dimension bound. More recently, progress on the second point was made with the mutual information method by Russo and Zou '15. Yet, these two methods are currently disjoint. In this paper, we introduce a technique to combine chaining and mutual information methods, to obtain a generalization bound that is both algorithmic-dependent and that exploits dependencies between hypotheses. We provide an example in which our bound significantly outperforms both the chaining and the mutual information method. As a corollary, we tighten Dudley's inequality under knowledge that a learning algorithm chooses its output from a small subset of hypotheses with high probability; an assumption motivated by the performance of SGD discussed in Zhang et al.\ '17. ","Amir Asadi*,Princeton University;Emmanuel Abbe,;Sergio Verd√∫,Princeton University",aasadi@princeton.edu;eabbe@princeton.edu;verdu@princeton.edu,Theory/Learning Theory*; Theory/Information Theory,Poster
3602,Implicit Probabilistic Integrators for ODEs,"We introduce a family of implicit probabilistic integrators for initial value problems (IVPs), taking as a starting point the multistep Adams‚ÄìMoulton method. The implicit construction allows for dynamic feedback from the forthcoming time-step, in contrast to previous probabilistic integrators, all of which are based on explicit methods. We begin with a concise survey of the rapidly-expanding field of probabilistic ODE solvers. We then introduce our method, which builds on and adapts the work of Conrad et al. (2016) and Teymur et al. (2016), and provide a rigorous proof of its well-definedness and convergence. We discuss the problem of the calibration of such integrators and suggest one approach. We give an illustrative example highlighting the effect of the use of probabilistic integrators‚Äîincluding our new method‚Äîin the setting of parameter inference within an inverse problem.","Onur Teymur*,Imperial College London;Han Cheng Lie,Freie Universit√§t Berlin;Tim Sullivan,Free University of Berlin;Ben Calderhead,Imperial College London",ot1013@ic.ac.uk;hlie@math.fu-berlin.de;sullivan@zib.de;b.calderhead@imperial.ac.uk,Probabilistic Methods*; Probabilistic Methods/Bayesian Theory; Probabilistic Methods/Gaussian Processes,Poster
3603,Learning Attentional Communication for Multi-Agent Cooperation,"Communication could potentially be an effective way for multi-agent cooperation. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. When there is a large number of agents, agents hardly differentiate valuable information that helps cooperative decision making from globally shared information. Therefore, communication barely help, and could even impair the learning of multi-agent cooperation. Predefined communication architectures, on the other hand, restrict communication among agents and thus restrain potential cooperation. To tackle these difficulties, in this paper, we propose an attentional communication model that learns when communication is needed and how to integrates shared information for cooperative decision making. Our model leads to efficient and effective communication for large-scale multi-agent cooperation. Empirically, we show the strength of our model in various cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies than existing methods.","Jiechuan Jiang,Peking University;Zongqing Lu*,Peking University",jiechuan.jiang@pku.edu.cn;zongqing.lu@pku.edu.cn,Reinforcement Learning and Planning/Multi-Agent RL,Poster
3607,"Training Deep Models Faster with Robust, Approximate Importance Sampling","In theory, importance sampling speeds up stochastic gradient algorithms for supervised learning by prioritizing training instances. In practice, the cost of computing importances greatly limits the impact of importance sampling. We propose a robust, approximate importance sampling procedure (RAIS) for stochastic gradient descent. By approximating the ideal sampling distribution using robust optimization, RAIS provides much of the benefit of exact importance sampling with drastically reduced overhead.  Empirically, we find RAIS-SGD and standard SGD follow similar learning curves, but RAIS moves faster through these paths.","Tyler Johnson*,University of Washington;Carlos Guestrin,University of Washington	",tbjohns@washington.edu;guestrin@cs.washington.edu,Deep Learning/Efficient Training Methods*; Deep Learning/Optimization for Deep Networks,Poster
3615,Bandit Learning with Implicit Feedback,"Implicit feedback, such as user clicks, although abundant in online information service systems, does not provide substantial evidence on users' evaluation of system's output. Such incomplete supervision inevitably misleads model estimation, especially in a bandit learning setting where the feedback is acquired on the fly. In this work, we study a contextual bandit problem with implicit feedback by modeling the feedback as a composition of user result examination and relevance judgment. Since users' examination behavior is unobserved, we introduce latent variables to model it. We perform Thompson sampling on top of variational Bayesian inference for arm selection and model update. Rigorous upper regret bound analysis of the proposed algorithm proves its feasibility of learning from implicit feedback; and extensive empirical evaluations on click logs collected from a major MOOC platform further demonstrate its learning effectiveness in practice. ","Yi Qi*,Tsinghua University;Qingyun Wu,University of Virginia;Hongning  Wang,University of Virginia;Jie Tang,Tsinghua University;Maosong Sun,",qi-y16@mails.tsinghua.edu.cn;qw2ky@virginia.edu;hw5x@virginia.edu;jietang@tsinghua.edu.cn;sms@mail.tsinghua.edu.cn,Algorithms/Bandit Algorithms*; Applications/Recommender Systems; Probabilistic Methods/Variational Inference,Poster
3632,Unsupervised Text Style Transfer using Language Models as Discriminators,"Binary classifiers are employed as discriminators in GAN-based unsupervised style transfer models to ensure that transferred sentences are similar to sentences in the target domain. One difficulty with the binary discriminator is that error signal is sometimes insufficient to train the model to produce rich-structured language. In this paper, we propose a technique of using a target domain language model as the discriminator to provide richer, token-level feedback during the learning process. Because our language model scores sentences directly using a product of locally normalized probabilities, it offers more stable and more useful training signal to the generator. We train the generator to minimize the negative log likelihood (NLL) of generated sentences evaluated by a language model. By using continuous approximation of the discrete samples, our model can be trained using back-propagation in an end-to-end way. Moreover, we find empirically with a language model as a structured discriminator, it is possible to eliminate the adversarial training steps using negative samples, thus making training more stable. We compare our model with previous work using convolutional neural networks (CNNs) as discriminators and show our model outperforms them significantly in three tasks including word substitution decipherment, sentiment modification and related language translation.","Zichao  Yang*,Carnegie Mellon University;Zhiting Hu,Carnegie Mellon University;Chris Dyer,DeepMind;Eric Xing,Petuum Inc. and CMU;Taylor Berg-Kirkpatrick,Carnegie Mellon University",yangtze2301@gmail.com;zhitingh@cs.cmu.edu;cdyer@google.com;epxing@cs.cmu.edu;tberg@cs.cmu.edu,Applications/Natural Language Processing*; Algorithms/Unsupervised Learning,Poster
3634,Relational recurrent neural networks,"Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a Relational Memory Core (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (BoxWorld & Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.","Adam Santoro*,DeepMind;Ryan Faulkner,DeepMind;David Raposo,DeepMind;Jack Rae,Deepmind;Mike Chrzanowski,DeepMind;Theophane Weber,DeepMind;Daan Wierstra,Google DeepMind;Oriol Vinyals,DeepMind;Razvan Pascanu,Google Deepmind;Timothy Lillicrap,DeepMind",adamsantoro@google.com;rfaulk@google.com;draposo@google.com;jwrae@google.com;chrzanowskim@google.com;theophane@google.com;wierstra@google.com;vinyals@google.com;razp@google.com;countzero@google.com,Deep Learning/Memory-Augmented Neural Networks*; Deep Learning/Recurrent Networks,Poster
3643,Streaming~Kernel~PCA~with~$\tilde{O}(\sqrt{n})$~Random~Features,"We study the statistical and computational aspects of kernel principal component analysis using random Fourier features and show that under mild assumptions, $O(\sqrt{n} \log n)$ features suffices to achieve $O(1/\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient streaming algorithm based on classical Oja's algorithm that achieves this rate","Enayat Ullah*,Johns Hopkins University;Poorya Mianjy,Johns Hopkins University;Teodor Vanislavov Marinov,Johns Hopkins University;Raman Arora,Johns Hopkins University",enayat@jhu.edu;mianjy@jhu.edu;tmarino2@jhu.edu;arora@cs.jhu.edu,Algorithms/Kernel Methods*; Theory/Learning Theory,Poster
3646,Exploring Sparse Features in Deep Reinforcement Learning towards Fast Disease Diagnosis,"This paper proposes a policy gradient method with two techniques, {\em reward shaping} and {\em reconstruction}, to improve the performance of online symptom checking. Reward shaping can guide the search towards better directions. Reconstruction can guide the agent to learn correlations between features. Together, they can find symptom queries that can yield positive responses from a patient with high probability.  Consequently, using these techniques a symptom checker can obtain much improved diagnoses.","Yu-Shao Peng*,HTC Research;Kai-Fu Tang,HTC Research;Hsuan-Tien Lin,National Taiwan University;Edward Chang,",ys_peng@htc.com;kevin_tang@htc.com;htlin@csie.ntu.edu.tw;Edward_Chang@htc.com,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Exploration; Reinforcement Learning and Planning/Reinforce,Poster
3652,Bayesian Model-Agnostic Meta-Learning,"Learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning due to the model uncertainty inherent in the problem. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, the method is capable of learning complex uncertainty structure beyond a point estimate or a simple Gaussian approximation. In addition, a robust Bayesian meta-update mechanism with a new meta-loss prevents overfitting during meta-update. Remaining an efficient gradient-based meta-learner, the method is also  model-agnostic and simple to implement. Experiment results show the accuracy and robustness of the proposed method in various tasks: sinusoidal regression, image classification, active learning, and reinforcement learning.","Jaesik Yoon,Sap;Taesup Kim,Universit√© de Montr√©al;Ousmane Dia,ElementAI;Sungwoong Kim,Kakao Brain;Yoshua Bengio,Universite de Montreal;Sungjin Ahn*,Rutgers University",jaesik817@gmail.com;taesup.kim@umontreal.ca;ousmane@elementai.com;allan.kim01@kakaobrain.com;yoshua.umontreal@gmail.com;sjn.ahn@gmail.com,Deep Learning/Meta-Learning*; Deep Learning/Few-Shot Learning Approaches; Probabilistic Methods,Spotlight
3662,Disconnected Manifold Learning for Generative  Adversarial Networks,"Real images often lie on a union of disjoint manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work, we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold, and investigate how sample quality, mode collapse and local convergence are affected by this. Next, we show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator GANs. Finally, we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs, its consequences in practice, and the effectiveness of our proposed modifications in alleviating these issues.","Mahyar Khayatkhoei*,Rutgers University;Maneesh Singh,Verisk Analytics;Ahmed Elgammal,-",m.khayatkhoei@rutgers.edu;maneesh.singh@verisk.com;elgammal@cs.rutgers.edu,Deep Learning/Adversarial Networks*; Deep Learning/Generative Models; Deep Learning/Meta-Learning,Poster
3666,Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces,"Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform the tasks of spoken word classification and translation, and the experimental results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.","Yu-An Chung*,MIT CSAIL;Wei-Hung Weng,MIT;Schrasing Tong,MIT CSAIL;James Glass,MIT",andyyuan@mit.edu;ckbjimmy@mit.edu;st9@mit.edu;glass@mit.edu,Applications/Audio and Speech Processing*; Algorithms/Representation Learning; Algorithms/Unsupervised Learning; Applications/Natural,Spotlight
3669,Learning Signed Determinantal Point Processes through the Principal Minor Assignment Problem,"Symmetric determinantal point processes (DPP) are a class of probabilistic models that encode the random selection of items that have a repulsive behavior. They have attracted a lot of attention in machine learning, where returning diverse sets of items is sought for. Sampling and learning these symmetric DPP's is pretty well understood. In this work, we consider a new class of DPP's, which we call signed DPP's, where we relax the repulsive behavior and allow some pairs of items to attract each other. We set the ground for learning signed DPP's through a method of moments, by solving the so called principal assignment problem for a class of matrices $K$ that satisfy $K_{i,j}=\pm K_{j,i}$, $i\neq j$, in polynomial time.","Victor-Emmanuel Brunel*,Massachusetts Institute of Technology",victoremmanuel.brunel@gmail.com,Theory/Learning Theory*; Theory/Computational Complexity; Theory/Frequentist Statistics,Poster
3678,Out-of-Distribution Detection using Multiple Semantic Label Representations,"Deep Neural Networks are powerful models that attained remarkable results on a variety of tasks. These models are shown to be extremely efficient when training and test data are drawn from the same distribution. However, it is not clear how a network will act when it is fed with an out-of-distribution example. In this work, we consider the problem of out-of-distribution detection in neural networks. We propose to use multiple semantic dense representations instead of sparse representation as the target label. Specifically, we propose to use several word representations obtained from different corpora or architectures as target labels. We evaluated the proposed model on computer vision, and speech commands detection tasks and compared it to previous methods. Results suggest that our method compares favorably with previous work. Besides, we present the efficiency of our approach for detecting wrongly classified and adversarial examples.","Gabi Shalev*,Dept. of Computer Science, Bar-Ilan University;Yossi Adi,Bar-Ilan University;Joseph Keshet,Dept. of Computer Science, Bar-Ilan University",shalev.gabi@gmail.com;yossiadidrum@gmail.com;jkeshet@cs.biu.ac.il,Deep Learning/Supervised Deep Networks*; Deep Learning/Embedding Approaches,Poster
3682,Stochastic Chebyshev Gradient Descent for Spectral Optimization,"A large class of machine learning techniques requires the solution of optimization problems involving spectral functions of parametric matrices, e.g. log-determinant and nuclear norm. Unfortunately, computing the gradient of a spectral function is generally of cubic complexity, as such gradient descent methods are rather expensive for optimizing objectives involving the spectral function. Thus, one naturally turns to stochastic gradient methods in hope that they will provide a way to reduce or altogether avoid the computation of full gradients. However, here a new challenge appears: there is no straightforward way to compute unbiased stochastic gradients for spectral functions. In this paper, we develop unbiased stochastic gradients for spectral-sums, an important subclass of spectral functions. Our unbiased stochastic gradients are based on combining randomized trace estimators with stochastic truncation of the Chebyshev expansions. A careful design of the truncation distribution allows us to offer distributions that are variance-optimal, which is crucial for fast and stable convergence of stochastic gradient methods. We further leverage our proposed stochastic gradients to devise stochastic methods for objective functions involving spectral-sums, and rigorously analyze their convergence rate. The utility of our methods is demonstrated in numerical experiments.","Insu Han*,KAIST;Haim Avron,Tel Aviv University;Jinwoo Shin,KAIST",hawki17@kaist.ac.kr;haimav@post.tau.ac.il;jinwoos@kaist.ac.kr,Optimization*; Algorithms/Stochastic Methods,Spotlight
3690,"Revisiting $(\epsilon, \gamma, \tau)$-similarity learning for domain adaptation","Similarity learning is an active research area in machine learning that tackles the problem of finding a similarity function tailored to an observable data sample in order to achieve efficient classification. This learning scenario has been generally formalized by the means of a $(\epsilon, \gamma, \tau)-$good similarity learning framework in the context of supervised classification and has been shown to have strong theoretical guarantees. In this paper, we propose to extend the theoretical analysis of similarity learning to the domain adaptation setting, a particular situation occurring when the similarity is learned and then deployed on samples following different probability distributions. We give a new definition of an $(\epsilon, \gamma)-$good similarity for domain adaptation and prove several results quantifying the performance of a similarity function on a target domain after it has been trained on a source domain. We particularly show that if the source domain support contains that of the target then principally new domain adaptation learning bounds can be proved.","Sofien Dhouib*,CREATIS UMR CNRS 5220;Ievgen Redko,Hubert Curien Laboratory",dhouib@creatis.insa-lyon.fr;ievgen.redko@insa-lyon.fr,Algorithms/Similarity and Distance Learning*; Theory/Learning Theory,Spotlight
3692,How to tell when a clustering is (approximately) correct using convex relaxations,"We introduce a generic method to obtain guarantees of near-optimality and uniqueness (up to small perturbations) for a clustering. This 
method can be instantiated for a variety of clustering loss functions for which convex relaxations exist. Obtaining the guarantees amounts to 
solving a convex optimization problem. We demonstrate the practical relevance of this method by obtaining distribution free guarantees for the K-means clustering 
problem  on realistic data sets. The guarantees do not depend on any distributional assumptions, but they depend on the data set at hand. 
They exist only when the data is  clusterable.","Marina Meila*,University of Washington",mmp@stat.washington.edu,Algorithms/Clustering*; Optimization/Combinatorial Optimization; Optimization/Convex Optimization,Poster
3695,"Constant Regret, Generalized Mixability, and Mirror Descent","We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting, and for the right choice of loss function and ``mixing'' algorithm, it is possible for the learner to achieve a constant regret regardless of the number of prediction rounds. For example, a constant regret can be achieved for \emph{mixable} losses using the \emph{aggregating algorithm}. The \emph{Generalized Aggregating Algorithm} (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies), which reduce to the aggregating algorithm when using the \emph{Shannon entropy}. For a given entropy $\Phi$, losses for which constant regret is possible using the GAA are called $\Phi$-mixable. Which losses are $\Phi$-mixable was previously left as an open question. We fully characterize $\Phi$-mixability and answer other open questions posed by \cite{Reid2015}. Additionally, by leveraging the connection between the \emph{mirror descent algorithm} and the update step of the GAA, we suggest a new \emph{adaptive} generalized aggregating algorithm and analyze its performance in terms of the regret bound.","Zakaria Mhammedi*,The Australian National University;Robert Williamson,Australian National University",zak.mhammedi@anu.edu.au;Bob.Williamson@anu.edu.au,Theory/Learning Theory*; Algorithms/Online Learning,Spotlight
3698,A Bayesian Approach to Generative Adversarial Imitation Learning,"Generative adversarial training for imitation learning has shown promising results on high-dimensional and continuous control tasks. This paradigm is based on reducing the imitation learning problem to the density matching problem, where the agent iteratively refines the policy to match the empirical state-action visitation frequency of the expert demonstration. Although this approach has shown to robustly learn to imitate even with scarce demonstration, one must still address the inherent challenge that collecting trajectory samples in each iteration is a costly operation. To address this issue, we first propose a Bayesian formulation of generative adversarial imitation learning (GAIL), where the imitation policy and the cost function are represented as stochastic neural networks. Then, we show that we can significantly enhance the sample efficiency of GAIL leveraging the predictive density of the cost, on an extensive set of imitation learning tasks with high-dimensional states and actions.","Wonseok Jeon*,KAIST;Seokin Seo,KAIST;Kee-Eung Kim,KAIST",wsjeon@ai.kaist.ac.kr;siseo@ai.kaist.ac.kr;kekim@cs.kaist.ac.kr,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Spotlight
3707,Plug-in Estimation in High-Dimensional Linear Inverse Problems: A Rigorous Analysis,"Estimating a vector $\mathbf{x}$ from noisy linear measurements $\mathbf{Ax+w}$ often requires use of prior knowledge or structural constraints
on $\mathbf{x}$ for accurate reconstruction. Several recent works have considered combining linear least-squares estimation with a generic or plug-in ``denoiser"" function that can be designed in a modular manner based on the prior knowledge about $\mathbf{x}$. While these methods have shown excellent performance, it has been difficult to obtain rigorous performance guarantees. This work considers plug-in denoising combined with the recently-developed Vector Approximate Message Passing (VAMP) algorithm, which is itself derived via Expectation Propagation techniques. It shown that the mean squared error of this ``plug-in""  VAMP can be exactly predicted for a large class of high-dimensional random $\Abf$ and denoisers. The method is illustrated in image reconstruction and parametric bilinear estimation.","Alyson Fletcher*,UCLA;Parthe Pandit,UCLA;Sundeep Rangan,NYU;Subrata Sarkar,Ohio State;Phillip Schniter,Ohio State",akfletcher@g.ucla.edu;parthepandit@g.ucla.edu;srangan@nyu.edu;sarkar.51@osu.edu;schniter.1@osu.edu,Theory/Statistical Physics of Learning*; Algorithms/Sparsity and Compressed Sensing; Applications/Signal Processing,Poster
3708,Constrained Cross-Entropy Method for Safe Reinforcement Learning,"We study a safe reinforcement learning problem in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well-suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then we give sufficient conditions on the properties of this differential equation to guarantee the convergence of the proposed algorithm. At last, we show with simulation experiments that the proposed algorithm can effectively learn feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions.","Min Wen*,University of Pennsylvania;Ufuk Topcu,University of Texas at Austin",wenm@seas.upenn.edu;utopcu@utexas.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
3717,Multi-Agent Generative Adversarial Imitation Learning,"Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments.
We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.","Jiaming Song*,Stanford;Hongyu Ren,Stanford University;Dorsa Sadigh,Stanford;Stefano  Ermon,Stanford University",jiaming.tsong@gmail.com;hyren@cs.stanford.edu;dorsa@cs.stanford.edu;ermon@cs.stanford.edu,Reinforcement Learning and Planning/Multi-Agent RL*; Deep Learning/Adversarial Networks; Deep Learning/Generative Models; Reinforcement Learning an,Poster
3719,Adaptive Learning with Unknown Information Flows,"An agent facing sequential decisions that are characterized by partial feedback needs to strike a balance between maximizing immediate payoffs based on available information, and acquiring new information that may be essential for maximizing future payoffs. This trade-off is captured by the multi-armed bandit (MAB) framework that has been studied and applied when at each time epoch payoff observations are collected on the actions that are selected at that epoch. In this paper we introduce a new, generalized MAB formulation in which additional information on each arm may appear arbitrarily throughout the decision horizon, and study the impact of such information flows on the achievable performance and the design of efficient decision-making policies. By obtaining matching lower and upper bounds, we characterize the (regret) complexity of this family of MAB problems as a function of the information flows. We introduce an adaptive exploration policy that, without any prior knowledge of the information arrival process, attains the best performance (in terms of regret rate) that is achievable when the information arrival process is a priori known. Our policy uses dynamically customized virtual time indexes to endogenously control the exploration rate based on the realized information arrival process.","Yonatan Gur,Stanford GSB;Ahmadreza Momeni*,Stanford University",ygur@stanford.edu;amomenis@stanford.edu,Algorithms/Bandit Algorithms*; Algorithms/Online Learning; Algorithms/Stochastic Methods; Reinforcement Learning and Planning,Poster
3722,Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks,"Electronic health records provide a rich source of data for machine learning methods to learn dynamic treatment responses over time. However, any direct estimation is hampered by the presence of time-dependent confounding, where actions taken are dependent on time-varying variables related to the outcome of interest. Drawing inspiration from marginal structural models, a class of methods in epidemiology which use propensity weighting to adjust for time-dependent confounders, we introduce the Recurrent Marginal Structural Network - a sequence-to-sequence architecture for forecasting a patient's expected response to a series of planned treatments. Using simulations of a state-of-the-art pharmacokinetic-pharmacodynamic (PK-PD) model of tumor growth, we demonstrate the ability of our network to accurately learn unbiased treatment responses from observational data ‚Äì even under changes in the policy of treatment assignments ‚Äì and performance gains over benchmarks.","Bryan Lim*,University of Oxford;Ahmed Alaa,University of California, Los Angeles;Mihaela van der Schaar,University of California, Los Angeles",bryan.lim@eng.ox.ac.uk;a7med3laa@hotmail.com;mihaelaucla@gmail.com,Deep Learning/Recurrent Networks*; Deep Learning/Predictive Models,Poster
3723,Generative modeling for protein structures,"Analyzing the structure and function of proteins is a key part of understanding biology at the molecular and cellular level. In addition, a major engineering challenge is to design new proteins in a principled and methodical way. Current computational modeling methods for protein design are slow and often require human oversight and intervention. Here, we apply Generative Adversarial Networks (GANs) to the task of generating protein structures, toward application in fast de novo protein design. We encode protein structures in terms of pairwise distances between alpha-carbons on the protein backbone, which eliminates the need for the generative model to learn translational and rotational symmetries. We then introduce a convex formulation of corruption-robust 3D structure recovery to fold the protein structures from generated pairwise distance maps, and solve these problems using the Alternating Direction Method of Multipliers. We test the effectiveness of our models by predicting completions of corrupted protein structures and show that the method is capable of quickly producing biochemically viable solutions. ","Namrata Anand*,Stanford University;Possu  Huang,Stanford University",namrataa@stanford.edu;possu@stanford.edu,Applications/Computational Biology and Bioinformatics*; Deep Learning/CNN Architectures; Deep Learning/Deep Autoencoders,Poster
3724,Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo,"Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to directly sample from it. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.","Marton Havasi*,University of Cambridge;Jose-Miguel Hernandez-Lobato,University of Cambridge;Juan Jos√© Murillo Fuentes,Universidad de Sevilla",mh740@cam.ac.uk;jmh233@cam.ac.uk;murillo@us.es,Probabilistic Methods/Gaussian Processes*; Deep Learning/Predictive Models; Probabilistic Methods/Latent Variable Models; Probabilistic M,Poster
3725,Knowledge Distillation by On-the-Fly Native Ensemble,"Knowledge distillation is effective to train the small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a high-capacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) learning strategy for one-stage online distillation. Specifically, ONE only trains a single multi-branch network while simultaneously establishing a strong teacher on-the-fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance of a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.","Xu Lan*,Queen Mary University of London;Xiatian Zhu,Queen Mary University, London, UK;Shaogang Gong,Queen Mary University of London",x.lan@qmul.ac.uk;eddy@visionsemantics.com;s.gong@qmul.ac.uk,Deep Learning/Efficient Training Methods*; Algorithms/Boosting and Ensemble Methods; Algorithms/Classification,Poster
3737,Non-Adversarial Mapping with VAEs,"The study of cross-domain mapping without supervision has recently attracted much attention. Much of the recent progress was enabled by the use of adversarial training as well as cycle constraints. The practical difficulty of adversarial training motivates research into non-adversarial methods. In a recent paper, Hoshen and Wolf, showed that cross-domain mapping is possible without the use of cycles or GANs. Although promising, their approach suffers from several drawbacks including costly inference and an optimization variable for every training example preventing their method from using large training sets. We present an alternative approach which is able to achieve non-adversarial mapping using a novel form of Variational Auto-Encoder. Our method is much faster at inference time, is able to leverage large datasets and has a simple interpretation.","Yedid Hoshen*,Facebook AI Research (FAIR)",yedid.hoshen@gmail.com,Algorithms/Unsupervised Learning*; Algorithms/Similarity and Distance Learning; Deep Learning/Adversarial Networks; Deep Learning,Poster
3743,Generalisation in humans and deep neural networks,"We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.","Robert Geirhos*,University of T√ºbingen;Carlos R. M. Temme,University of T√ºbingen;Jonas Rauber,University of T√ºbingen;Heiko H. Sch√ºtt,University of T√ºbingen;Matthias Bethge,University of T√ºbingen;Felix A. Wichmann,University of T√ºbingen",robert.geirhos@bethgelab.org;car.mete@yahoo.de;jonas.rauber@bethgelab.org;heiko.schuett@uni-tuebingen.de;matthias.bethge@bethgelab.org;felix.wichmann@uni-tuebingen.de,Neuroscience and Cognitive Science*; Neuroscience and Cognitive Science/Cognitive Science; Neuroscience and Cognitive Science/Visua,Poster
3748,Towards Text Generation with Adversarially Learned Neural Outlines,"Recent progress in deep generative models has been fueled by two paradigms -- autoregressive and adversarial models. We propose a combination of both approaches with the goal of learning generative models of text. Our method first produces a high-level outline and then generates words sequentially, conditioning on both the outline and the previous output. We generate outlines with an adversarial model trained to approximate the distribution of sentences in a latent space induced by general-purpose sentence encoders. This provides strong, informative conditioning for the autoregressive stage. Our qualitative results show that this generative procedure yields natural-looking sentences and interpolations. Quantitative results suggest that conditioning information from generated outlines effectively guides the autoregressive model to produce realistic samples even at high temperatures with multinomial sampling.","Sandeep Subramanian*,MILA;Sai Rajeshwar,University of Montreal;Alessandro Sordoni,Maluuba;Adam Trischler,Microsoft Research;Aaron Courville,Universite de Montreal;Christopher Pal,√âcole Polytechnique de Montr√©al ",sandeep.subramanian.1@umontreal.ca;rajsai24@gmail.com;alsordon@microsoft.com;adam.trischler@microsoft.com;aaron.courville@umontreal.ca;christopher.pal@polymtl.ca,Deep Learning/Generative Models*; Algorithms/Representation Learning; Applications/Natural Language Processing; Deep Learning ->,Poster
3751,cpSGD: Communication-efficient and differentially-private distributed SGD," Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For $d$ variables and $n \approx d$ clients, the proposed method uses $\cO(\log \log(nd))$ bits of communication per client per coordinate and ensures constant privacy.

We also improve previous analysis of the \emph{Binomial mechanism} showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest.","Naman Agarwal,princeton;Ananda Suresh*,Google;Felix Yu,Google;Sanjiv Kumar,Google Research;Brendan McMahan,Google",namana@princeton.edu;theertha@google.com;felixyu@google.com;sanjivk@google.com;mcmahan@google.com,"Theory/Information Theory*; Applications/Privacy, Anonymity, and Security",Spotlight
3760,Blackbox Matrix√óMatrix Gaussian Process Inference,"Despite numerous advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on recent trends in machine learning hardware. In this paper, we present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM uses a modified batched version of the conjugate gradients algorithm to derive all terms required for training and inference in a single call. Adapting this algorithm to complex models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM utilizes a specialized preconditioner that substantially speeds up convergence. In experiments, we show that BBMM efficiently utilizes GPU hardware, speeding up GP inference by an order of magnitude on a variety of popular GP models compared to existing approaches.","Jacob Gardner*,Cornell University;Geoff Pleiss,Cornell University;Kilian Weinberger,Cornell University;David Bindel,Cornell University;Andrew Gordon Wilson,Cornell University",jrg365@cornell.edu;gp346@cornell.edu;kqw4@cornell.edu;bindel@cs.cornell.edu;andrew@cornell.edu,Probabilistic Methods/Gaussian Processes,Spotlight
3765,Diffusion Maps for Textual Network Embedding,"Textual network embedding leverages rich text information associated with the network to learn low-dimensional vectorial representations of vertices.
Rather than using typical natural language processing (NLP) approaches, recent research exploits the relationship of texts on the same edge to graphically embed text. However, these models neglect to measure the complete level of connectivity between any two texts in the graph. We present diffusion maps for textual network embedding (DMTE), integrating global structural information of the graph to capture the semantic relatedness between texts, with a diffusion-convolution operation applied on the text inputs. In addition, a new objective function is designed to efficiently preserve the high-order proximity using the graph diffusion. Experimental results show that the proposed approach outperforms state-of-the-art methods on the vertex-classification and link-prediction tasks.","Xinyuan Zhang*,Duke University;Yitong Li,Duke University;Dinghan Shen,Duke University;Lawrence Carin Duke,CS",xy.zhang@duke.edu;yitong.li@duke.edu;dinghan.shen@duke.edu;lcarin@duke.edu,Applications/Natural Language Processing*; Applications/Network Analysis,Spotlight
3771,"Edward2: Simple, Dynamic, Accelerated","We describe Edward2, a probabilistic programming language (PPL) extending Edward. Edward2 distills the core of Edward down to a single abstraction‚Äîthe
random variable‚Äîwhile expanding its feature set. By blurring the line between modeling and computation, Edward2 enables numerous applications not possible in existing PPLs: the grammar VAE; learning to learn by variational inference by gradient descent; and GPU-accelerated NUTS. In a benchmark on VAEs, Edward2 sees a 5x speedup running on TPUs compared to GPU. In a benchmark on NUTS, Edward2 sees a 20x speedup over Stan and 7x over PyMC3.","Dustin Tran*,Columbia University;Matthew Hoffman,Google;Dave Moore,Google;Christopher Suter,Google;Srinivas Vasudevan,Google;Alexey Radul,Google;Matthew Johnson,Google Brain;Rif A. Saurous,",dustin@cs.columbia.edu;mhoffman@google.com;davmre@google.com;cgs@google.com;srvasude@google.com;axch@google.com;mattjj@google.com;rif@google.com,"Data, Competitions, Implementations, and Software/Software Toolkits*; Deep Learning/Generative Models",Poster
3774,VideoCapsuleNet: A Simplified Network for Action Detection,"The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue which makes the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss.","Kevin Duarte*,University of Central Florida;Yogesh Rawat,University of Central Florida;Mubarak Shah,University of Central Florida",kevin_duarte@knights.ucf.edu;yogesh@crcv.ucf.edu;shah@crcv.ucf.edu,Deep Learning*; Applications/Activity and Event Recognition; Applications/Computer Vision; Applications/Vid,Poster
3779,Rectangular Bounding Process," Stochastic partition models divide a multi-dimensional space into a number of rectangular regions, such that the data within each region exhibit certain types of homogeneity. Due to the nature of their partition strategy, existing partition models may create many unnecessary divisions in sparse regions when trying to describe data in dense regions. To avoid this problem we introduce a new parsimonious partition model -- the Rectangular Bounding Process (RBP) -- to efficiently partition multi-dimensional spaces, by employing a bounding strategy to enclose data points within rectangular bounding boxes. Unlike existing approaches, the RBP possesses several attractive theoretical properties that make it a powerful nonparametric partition prior on a hypercube. In particular, the RBP is self-consistent and as such can be directly extended from a finite hypercube to infinite (unbounded) space. We apply RBP to regression trees and relational models as a flexible partition prior. The experimental results validate the merit of RBP {in rich yet parsimonious expressiveness} compared to the state-of-the-art methods.","Xuhui Fan*,UNSW;Bin Li,Fudan University;Scott SIsson,",Xuhui.Fan@unsw.edu.au;libin@fudan.edu.cn;Scott.Sisson@unsw.edu.au,Probabilistic Methods/Bayesian Nonparametrics,Poster
3780,Improved Algorithms for Collaborative PAC Learning,"We study a recent model of collaborative PAC learning where $k$ players with $k$ different tasks collaborate to learn a single classifier that works for all tasks. Previous work showed that when there is a classifier that has very small errors on all tasks, there is a collaborative algorithm that finds a single classifier for all tasks and it uses $O((\ln (k))^2)$ times the sample complexity to learn a single task. In this work, we design new algorithms for both the realizable and the non-realizable settings using only $O(\ln (k))$ times the sample complexity to learn a single task. The sample complexity upper bounds of our algorithms match previous lower bounds and in some range of parameters, are even better than previous algorithms that are allowed to output different classifiers for different tasks.","Huy Nguyen,Princeton;Lydia Zakynthinou*,Northeastern University",hlnguyen@cs.princeton.edu;zakynthinou.l@husky.neu.edu,Theory*; Theory/Learning Theory,Poster
3784,Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding,"Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps.
This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state.
We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly long-term dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.","Nan Rosemary Ke*,Mila, University of Montreal;Anirudh Goyal,University of Montreal;Olexa Bilaniuk,MILA, Polytechnique Montreal;Jonathan Binas,MILA, Montreal;Michael Mozer,University of Colorado;Christopher Pal,√âcole Polytechnique de Montr√©al ;Yoshua Bengio,Universite de Montreal",rosemary.nan.ke@gmail.com;anirudhgoyal9119@gmail.com;obilaniu@gmail.com;jbinas@gmail.com;mozer@colorado.edu;christopher.pal@polymtl.ca;yoshua.umontreal@gmail.com,Neuroscience and Cognitive Science/Cognitive Science*; Deep Learning/Biologically Plausible Deep Networks; Deep Learning/Recurrent Networks,Spotlight
3785,Communication Compression for Decentralized Training,"Optimizing distributed learning systems is an art
of balancing between computation and communication.
There have been two lines of research that try to
deal with slower networks: {\em communication 
compression} for
low bandwidth networks, and {\em decentralization} for
high latency networks. In this paper, We explore
a natural question: {\em can the combination
of both techniques lead to
a system that is robust to both bandwidth
and latency?}

Although the system implication of such combination
is trivial, the underlying theoretical principle and
algorithm design is challenging:  unlike centralized algorithms, simply compressing
{\rc exchanged information,
even in an unbiased stochastic way, 
within the decentralized network would accumulate the error and cause divergence.} 
In this paper, we develop
a framework of quantized, decentralized training and
propose two different strategies, which we call
{\em extrapolation compression} and {\em difference compression}.
We analyze both algorithms and prove 
both converge at the rate of $O(1/\sqrt{nT})$ 
where $n$ is the number of workers and $T$ is the
number of iterations, matching the convergence rate for
full precision, centralized training. We validate 
our algorithms and find that our proposed algorithm outperforms
the best of merely decentralized and merely quantized
algorithm significantly for networks with {\em both} 
high latency and low bandwidth.","Hanlin Tang*,University of Rochester;Shaoduo Gan,ETH Zurich;Ce Zhang,ETH;Tong Zhang,Tecent AI Lab;Ji  Liu,University of Rochester",tanghl1994@gmail.com;sgan@inf.ethz.ch;ce.zhang@inf.ethz.ch;tongzhang@tongzhang-ml.org;ji.liu.uwisc@gmail.com,Optimization*; Optimization/Non-Convex Optimization,Poster
3786,Depth-Limited Solving for Imperfect-Information Games,"A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.","Noam Brown*,CMU;Tuomas Sandholm,Carnegie Mellon University;Brandon Amos,CMU",noamb@cs.cmu.edu;sandholm@cs.cmu.edu;bamos@cs.cmu.edu,Theory/Game Theory and Computational Economics*; Applications/Game Playing; Reinforcement Learning and Planning/Multi-Agent RL,Poster
3789,Training Deep Neural Networks with 8-bit Floating Point Numbers,"The state-of-the-art hardware platforms for training deep neural networks are moving from traditional single precision (32-bit) computations towards 16 bits of precision - in large part due to the high energy efficiency and smaller bit storage associated with using reduced-precision representations. However, unlike inference, training with numbers represented with less than 16 bits has been challenging due to the need to maintain fidelity of the gradient computations during back-propagation. Here we demonstrate, for the first time, the successful training of deep neural networks using 8-bit floating point numbers while fully maintaining the accuracy on a spectrum of deep learning models and datasets. In addition to reducing the data and computation precision to 8 bits, we also successfully reduce the arithmetic precision for additions (used in partial product accumulation and weight updates) from 32 bits to 16 bits through the introduction of a number of key ideas including chunk-based accumulation and floating point stochastic rounding. The use of these novel techniques lays the foundation for a new generation of hardware training platforms with the potential for 2-4 times improved throughput over today's systems.","Naigang Wang*,IBM T. J. Watson Research Center;Jungwook Choi,IBM Research;Daniel Brand,IBM Research;Chia-Yu Chen,IBM Research;Kailash Gopalakrishnan,IBM Watson",nwang@us.ibm.com;choij@us.ibm.com;danbrand@us.ibm.com;cchen@us.ibm.com;kailash@us.ibm.com,Applications/Hardware and Systems*; Deep Learning/Efficient Training Methods,Poster
3793,Scalar Posterior Sampling with Applications ,"We propose a practical  non-episodic PSRL algorithm that unlike recent state-of-the-art PSRL algorithms  uses a deterministic,  model-independent episode switching schedule. Our algorithm termed deterministic schedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space complexity.  We prove a Bayesian regret bound under mild assumptions.  Our result is more generally applicable to multiple parameters and continuous state action problems.  We compare our algorithm with state-of-the-art PSRL algorithms on standard discrete and continuous problems from the literature.  Finally, we show how the assumptions of our algorithm satisfy a sensible  parameterization  for a  large class of problems in sequential recommendations.","Georgios Theocharous*,""Adobe Research, USA"";Zheng Wen,Adobe Research;Yasin Abbasi-Yadkori,Adobe Research;Nikos Vlassis,Netflix",theochar@adobe.com;zwen@adobe.com;abbasiya@adobe.com;nikos.vlassis@gmail.com,Reinforcement Learning and Planning*; Algorithms/Bandit Algorithms; Algorithms/Online Learning; Applications/Recommender Systems,Poster
3797,Understanding Batch Normalization,"Batch normalization is a ubiquitous deep learning technique that normalizes activations in intermediate layers. It is associated with improved accuracy and faster learning, but despite its enormous success there is little consensus regarding why it works. We aim to rectify this and take an empirical approach to understanding batch normalization. Our primary observation is that the higher learning rates that batch normalization enables have a regularizing effect that dramatically improves generalization of normalized networks, which is both demonstrated empirically and motivated theoretically. We show how both activations and gradient information becomes less input dependent across spatial dimensions and examples within a mini-batch for deep unnormalized networks, and show how this limit possible learning rates. Motivated by recent results in random matrix theory, we argue that this ill-conditioning is due to fluctuations in random initialization, shedding new light on classical initialization schemes and their consequences.","Johan Bj√∂rck*,Cornell;Carla Gomes,Cornell University;Bart Selman,Cornell University;Kilian Weinberger,Cornell University",johanbjorck@hotmail.com;gomes@cs.cornell.edu;selman@cs.cornell.edu;kilianweinberger@cornell.edu,Deep Learning,Poster
3816,Adversarial Scene Editing: Automatic Object Removal from Weak Supervision,"While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets.
In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only.","Rakshith Shetty*,Max Planck Institute of Informatics;Mario Fritz,Max-Planck-Institut f√ºr Informatik;Bernt Schiele,MPI",rshetty@mpi-inf.mpg.de;mfritz@mpi-inf.mpg.de;schiele@mpi-inf.mpg.de,Deep Learning/Generative Models*; Algorithms/Semi-Supervised Learning; Algorithms/Unsupervised Learning; Applications/Compute,Poster
3818,Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples,"Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible features/attributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional causality inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the gut-feeling part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94% detection accuracy for 7 different kinds of attacks with 9.91% false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55% accuracy with 23.3% false positives.","Guanhong Tao*,Purdue University;Shiqing Ma,Purdue University;Yingqi Liu,Purdue University;Xiangyu Zhang,Purdue University",taog@purdue.edu;ma229@purdue.edu;liu1751@purdue.edu;xyzhang@cs.purdue.edu,"Applications*; Applications/Privacy, Anonymity, and Security",Spotlight
3826,On Neuronal Capacity,"  We define the capacity of a learning machine to be the logarithm of the number (or volume) of the functions it can implement. We review known results, and derive new results, estimating the capacity of several neuronal models:  linear and polynomial threshold gates, linear and polynomial threshold gates with constrained weights (binary weights, positive weights), and ReLU neurons. We also derive capacity estimates and bounds for fully recurrent networks and layered feedforward networks.","Pierre Baldi*,UC Irvine;Roman Vershynin,UCI",pfbaldi@ics.uci.edu;rvershyn@uci.edu,Theory*; Applications/Hardware and Systems; Neuroscience and Cognitive Science/Neural Coding; Theory ->,Oral
3831,Breaking the Activation Function Bottleneck through Adaptive Parameterization,"Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and Wikitext-2 word-modeling tasks while using fewer parameters and converging in half as many iterations.","Sebastian Flennerhag*,University of Manchester;Hujun Yin,University of Manchester ;John Keane,University of Manchester;Mark Elliot,University of Manchester",sebastianflennerhag@gmail.com;hujun.yin@manchester.ac.uk;john.keane@manchester.ac.uk;mark.elliot@manchester.ac.uk,Deep Learning/Recurrent Networks*; Deep Learning/Supervised Deep Networks,Poster
3833,Learning Loop Invariants for Program Verification,"A fundamental problem in program verification concerns inferring loop invariants. The problem is undecidable and even practical instances are challenging. Inspired by how human experts construct loop invariants, we propose a reasoning framework that constructs the solution by multi-step decision making and querying an external program graph memory block. By training with reinforcement learning, we are able to capture rich program features and avoid the need for ground truth solutions as supervision. Compared to previous learning tasks in domains with graph-structured data, we address unique challenges, such as a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed. We evaluate our approach on a suite of 133 benchmark problems and compare it to three state-of-the-art systems. It solves 97 problems compared to 73 by a stochastic search-based system, 77 by a symbolic constraint solver, and 100 by a decision tree learning-based system. Moreover, the strategy learned can be generalized to new programs: compared to solving new instances from scratch, the pre-trained agent is more sample efficient in finding solutions.","Xujie Si,University of Pennsylvania;Hanjun Dai*,Georgia Tech;Mukund Raghothaman,University of Pennsylvania;Mayur Naik,University of Pennsylvania;Le Song,Ant Financial & Georgia Institute of Technology ",xsi@seas.upenn.edu;hanjundai@gatech.edu;rmukund@seas.upenn.edu;mhnaik@seas.upenn.edu;lsong@cc.gatech.edu,Deep Learning/Program Induction*; Deep Learning/Attention Models; Deep Learning/Embedding Approaches; Reinforcement Learning and,Spotlight
4842,Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization,"There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn audio and video features from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve state-of-the-art performance on established audio classification benchmarks (DCASE2014 and ESC-50), while our visual stream provides a very effective initialization to significantly improve the performance of video-based action recognition models (our self-supervised pretraining yields a remarkable gain in accuracy of 16.7% on UCF101 and 13.0% on HMDB51, compared to learning from scratch).","Bruno Korbar,Dartmouth College;Du Tran,Facebook;Lorenzo Torresani*,Dartmouth College",Bruno.Korbar.18@dartmouth.edu;trandu@fb.com;lt@dartmouth.edu,Algorithms/Unsupervised Learning*; Applications/Activity and Event Recognition; Applications/Computer Vision; Applications/Spe,Poster
4848,Towards Robust Interpretability with Self-Explaining Neural Networks,"Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.","David Alvarez-Melis*,MIT;Tommi Jaakkola,-",dalvmel@mit.edu;tommi@csail.mit.edu,"Applications/Fairness, Accountability, and Transparency*; Deep Learning/Visualization or Exposition Techniques for Deep Networks",Poster
4852,Deep State Space Models for Time Series Forecasting,"We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from millions of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.","Syama Sundar Rangapuram*,Amazon;Matthias Seeger,Amazon;Jan Gasthaus,Amazon Research;Lorenzo Stella,Amazon;Bernie Wang,Amazon;Tim Januschowski,Amazon",r.shyamsundar@gmail.com;matthis@amazon.de;gasthaus@amazon.com;stellalo@amazon.de;yuyawang@amazon.com;tjnsch@amazon.de,Probabilistic Methods*; Applications/Time Series Analysis; Deep Learning/Recurrent Networks,Poster
4855,Constrained Graph Variational Autoencoders for Molecule Design,"Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on the use of graphs to represent chemical molecules, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization.	Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is more successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.","Qi Liu,Microsoft Research;Miltiadis Allamanis,Microsoft Research;Marc Brockschmidt,Microsoft;Alexander Gaunt*,Microsoft Research",qiliu@u.nus.edu;miallama@microsoft.com;mabrocks@microsoft.com;algaunt@microsoft.com,Algorithms/Structured Prediction*; Deep Learning/Deep Autoencoders; Deep Learning/Generative Models,Poster
4859,Learning Libraries of Subroutines for Neurally‚ÄìGuided Bayesian Program Learning,"  Successful approaches to program induction require a hand-engineered
  domain-specific language (DSL), constraining the space of allowed
  programs and imparting prior knowledge of the domain.  We contribute
  a program induction algorithm that learns a DSL while
  jointly training a neural network to efficiently search for programs
  in the learned DSL.  We use our model to synthesize functions on lists,
  edit text, and solve symbolic regression problems, showing how the
  model learns a domain-specific library of program components for
  expressing solutions to problems in the domain.","Kevin Ellis*,MIT;Lucas Morales,MIT;Mathias Sabl√©-Meyer,MIT, ENS Paris-Saclay;Armando Solar-Lezama,MIT;Joshua Tenenbaum,MIT",ellisk@mit.edu;lucasem@mit.edu;mathias.sable-meyer@ens-cachan.fr;asolar@csail.mit.edu;jbt@mit.edu,Deep Learning/Program Induction,Spotlight
4861,Neural Architecture Optimization,"Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods. Furthermore, the computational resource is 10 times fewer than typical methods based on RL and EA.","Renqian Luo,USTC;Fei Tian*,Microsoft Research;Tao Qin,Microsoft Research Asia;Enhong Chen,University of Science and Technology of China;Tieyan Liu,",lrq@mail.ustc.edu.cn;fetia@microsoft.com;taoqin@microsoft.com;cheneh@ustc.edu.cn;tyliu@microsoft.com,Algorithms/AutoML*; Applications/Computer Vision; Applications/Natural Language Processing; Deep Learning; Deep Le,Poster
4866,Preference Based Adaptation for Learning Objectives,"In many real-world learning tasks, it is hard to directly optimize the true performance measures, meanwhile choosing the right surrogate objectives is also difficult. Under this situation, it is desirable to incorporate an optimization of objective process into the learning loop based on weak modeling of the relationship between the true measure and the objective. In this work, we discuss the task of objective adaptation, in which the learner iteratively adapts the learning objective to the underlying true objective based on the preference feedback from an oracle. We show that when the objective can be linearly parameterized, this preference based learning problem can be solved by utilizing the dueling bandit model. A novel sampling based algorithm DL^2M is proposed to learn the optimal parameter, which enjoys strong theoretical guarantees and efficient empirical performance. To avoid learning a hypothesis from scratch after each objective function update, a boosting based hypothesis adaptation approach is proposed to efficiently adapt any pre-learned element hypothesis to the current objective. We apply the overall approach to multi-label learning, and show that the proposed approach achieves significant performance under various multi-label performance measures.","Yao-Xiang Ding*,Nanjing University;Zhi-Hua  Zhou,Nanjing University",dingyx@lamda.nju.edu.cn;zhouzh@nju.edu.cn,Algorithms/Online Learning*; Algorithms/Bandit Algorithms; Algorithms/Boosting and Ensemble Methods; Algorithms/Multitas,Poster
4880,Distributed $k$-Clustering for Data with Heavy Noise,"	In this paper, we consider the $k$-center/median/means clustering with outliers problems (or the $(k, z)$-center/median/means problems) in the distributed setting.  Most previous distributed algorithms have their communication costs linearly depending on $z$, the number of outliers.  Recently Guha et al. overcame this dependence issue by considering bi-criteria approximation algorithms that output solutions with $2z$ outliers.  For the case where $z$ is large, the extra $z$ outliers discarded by the algorithms might be too large, considering that the data gathering process might be costly. In this paper, we improve the number of outliers to the best possible $(1+\epsilon)z$, while maintaining the $O(1)$-approximation ratio and independence of communication cost on $z$.  The problems we consider include the $(k, z)$-center problem, and $(k, z)$-median/means problems in Euclidean metrics. Implementation of the our algorithm for $(k, z)$-center shows that it outperforms many previous algorithms, both in terms of the communication cost and quality of the output solution. ","Shi Li*,University at Buffalo;Xiangyu Guo,University at Buffalo",shil@buffalo.edu;xiangyug@buffalo.edu,Optimization*; Algorithms/Clustering; Optimization/Combinatorial Optimization,Spotlight
4881,Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo,"A key task in Bayesian machine learning is sampling from distributions that are only specified up to a partition function (i.e., constant of proportionality). One prevalent example of this is sampling posteriors in parametric 
distributions, such as latent-variable generative models.  However sampling (even very approximately) can be #P-hard.

Classical results (going back to Bakry and Emery) on sampling focus on log-concave distributions, and show a natural Markov chain called Langevin diffusion mix in polynomial time.  However, all log-concave distributions are uni-modal, while in practice it is very common for the distribution of interest to have multiple modes.
In this case, Langevin diffusion suffers from torpid mixing. 

We address this problem by combining Langevin diffusion with simulated tempering. The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution. We analyze this Markov chain for a mixture of (strongly) log-concave distributions of the same shape. In particular, our technique applies to the canonical multi-modal distribution: a mixture of gaussians (of equal variance). Our algorithm efficiently samples from these distributions given only access to the gradient of the log-pdf. To the best of our knowledge, this is the first result that proves fast mixing for multimodal distributions. ","Holden Lee*,Princeton University;Andrej Risteski,MIT;Rong Ge,Duke University",holdenl@princeton.edu;risteski@mit.edu;rongge@cs.duke.edu,Theory*; Probabilistic Methods/MCMC,Poster
4890,A General Method for Amortizing Variational Filtering,"We introduce a general-purpose, theoretically-grounded, and simple method for performing filtering variational inference in dynamical latent variable models, which we refer to as the variational filtering EM algorithm. The algorithm is derived from the variational objective in the filtering setting and naturally consists of a Bayesian prediction-update loop, with updates performed using any desired optimization method. A computationally efficient implementation of the algorithm is provided, using iterative amortized inference models to perform inference optimization. Through empirical evaluations with several deep dynamical latent variable models on a variety of sequence data sets, we demonstrate that this simple filtering scheme compares favorably against previously proposed filtering methods in terms of inference performance, thereby improving model quality.","Joe Marino*,California Institute of Technology;Milan Cvitkovic,California Institute of Technology;Yisong Yue,Caltech",jmarino@caltech.edu;mcvitkov@caltech.edu;yyue@caltech.edu,Deep Learning/Efficient Inference Methods*; Deep Learning/Generative Models,Poster
4891,A Reduction for Efficient LDA Topic Reconstruction,"We present a novel approach for LDA (Latent Dirichlet Allocation) topic reconstruction. The main technical idea is to show that the distribution over the documents generated by LDA can be transformed into a distribution for a much simpler generative model in which documents are generated from {\em the same set of topics} but have a much simpler structure: documents are single topic and topics are chosen uniformly at random. Furthermore, this reduction is approximation preserving, in the sense that approximate distributions-- the only ones we can hope to compute in practice-- are mapped into approximate distribution in the simplified world. This opens up the possibility of efficiently reconstructing LDA topics in a roundabout way. Compute an approximate document distribution from the given corpus, transform it into an approximate distribution for the single-topic world, and run a reconstruction algorithm in the uniform, single topic world-- a much simpler task than direct LDA reconstruction. Indeed, we show the viability of the approach by giving very simple algorithms for a generalization of two notable cases that have been studied in the literature, $p$-separability and Gibbs sampling for matrix-like topics.","Matteo Almanza,Sapienza University;Flavio Chierichetti*,Sapienza University;Alessandro Panconesi,Sapienza, University of Rome;Andrea Vattani,Spiketrap",matteojug@gmail.com;flavio@di.uniroma1.it;ale@di.uniroma1.it;avattani@cs.ucsd.edu,Algorithms*; Algorithms/Missing Data; Deep Learning/Generative Models; Probabilistic Methods/Latent Vari,Poster
4892,Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data,"Continuous-time Bayesian networks (CTBNs) constitute a general and powerful framework for modeling continuous-time stochastic processes on networks. This makes them particularly attractive for learning the directed structures among interacting entities. However, if the available data is incomplete, one needs to simulate the prohibitively complex CTBN dynamics. Existing approximation techniques, such as sampling and low-order variational methods, either scale unfavorably in system size, or are unsatisfactory in terms of accuracy. Inspired by recent advances in statistical physics, we present a new approximation scheme based on cluster-variational methods significantly improving upon existing variational approximations. We can then analytically marginalize CTBN parameters, as these are of secondary importance for structure learning. This recovers a scalable scheme for direct structure learning from incomplete and noisy time-series data. Our approach outperforms existing methods in terms of scalability.","Dominik Linzner*,TU Darmstadt;Heinz Koeppl,TU Darmstadt",dominik.linzner@bcs.tu-darmstadt.de;heinz.koeppl@bcs.tu-darmstadt.de,Probabilistic Methods/Variational Inference*; Probabilistic Methods/Graphical Models,Poster
4893,RenderNet: A deep convolutional network for differentiable rendering from 3D shapes,"Traditional computer graphics rendering pipeline is designed for procedurally generating 2D quality images from 3D shapes with high performance. The non-differentiability due to discrete operations such as visibility computation makes it hard to explicitly correlate rendering parameters and the resulting image, posing a significant challenge for inverse rendering tasks. Recent work on differentiable rendering achieves differentiability either by designing surrogate gradients for non-differentiable operations or via an approximate but differentiable renderer. These methods, however, are still limited when it comes to handling occlusion and restricted to particular rendering effects. We present RenderNet, a differentiable rendering convolutional network with a novel projection unit that can render 2D images from 3D shapes. Spatial occlusion and shading calculation are automatically encoded in the network. Our experiments show that RenderNet can successfully learn to implement different shaders, and can be used in inverse rendering tasks to estimate shape, pose, lighting and texture from a single image.","Thu Nguyen-Phuoc*,University of Bath;Chuan Li,Lambda Labs;Stephen Balaban,Lambda Labs;Yongliang Yang,University of Bath",tnp23@bath.ac.uk;c@lambdal.com;s@lambdal.com;strongyang@gmail.com,Applications/Computer Vision*; Deep Learning/CNN Architectures,Poster
4901,Robust Hypothesis Testing Using Wasserstein Uncertainty Sets,"We develop a novel computationally efficient and general framework for robust hypothesis testing. The new framework features a new way to construct uncertainty sets under the null and the alternative distributions, which are sets centered around the empirical distribution defined via Wasserstein metric, thus our approach is data-driven and free of distributional assumptions. We develop a convex safe approximation of the minimax formulation and show that such approximation renders a nearly-optimal detector among the family of all possible tests. By exploiting the structure of the least favorable distribution, we also develop a tractable reformulation of such approximation, with complexity independent of the dimension of observation space and can be nearly sample-size-independent in general. Real-data example using human activity data demonstrated the excellent performance of the new robust detector. ","RUI GAO*,GEORGIA TECH;Liyan Xie,Georgia Tech;Yao Xie,Georgia Tech;Huan Xu,Georgia Inst. of Technology",rgao32@gatech.edu;lxie49@gatech.edu;yao.xie@isye.gatech.edu;huan.xu@isye.gatech.edu,Optimization*; Theory/Frequentist Statistics; Theory/Information Theory,Spotlight
4902,Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks,It has been shown that deep neural network (DNN) based classifiers are vulnerable to human-imperceptive adversarial perturbations which can cause DNN classifiers to output wrong predictions with high confidence. We propose an unsupervised learning approach to detect adversarial inputs without any knowledge of attackers. Our approach tries to capture the intrinsic properties of a DNN classifier and uses them to detect adversarial inputs. The intrinsic properties used in this study are the output distributions of the hidden neurons in a DNN classifier presented with natural images. Our approach can be easily applied to any DNN classifiers or combined with other defense strategy to improve robustness. Experimental results show that our approach demonstrates state-of-the-art robustness in defending black-box and gray-box attacks.,"Zhihao Zheng,Brandeis University;Pengyu Hong*,Brandeis University",zhihaozh@brandeis.edu;hongpeng@brandeis.edu,Deep Learning,Poster
4906,Monte-Carlo Tree Search for Constrained POMDPs,"Monte-Carlo Tree Search (MCTS) has been successfully applied to very large POMDPs, a standard model for stochastic sequential decision-making problems. However, many real-world problems inherently have multiple goals, where multi-objective formulations are more natural. The constrained POMDP (CPOMDP) is such a model that maximizes the reward while constraining the cost, extending the standard POMDP model. To date, solution methods for CPOMDPs assume an explicit model of the environment, and thus are hardly applicable to large-scale real-world problems. In this paper, we present CC-POMCP (Cost-Constrained POMCP), an online MCTS algorithm for large CPOMDPs that leverages the optimization of LP-induced parameters and only requires a black-box simulator of the environment. In the experiments, we demonstrate that CC-POMCP converges to the optimal stochastic action selection in CPOMDP and pushes the state-of-the-art by being able to scale to very large problems.","Jongmin Lee*,KAIST;Geon-Hyeong Kim,KAIST;Pascal Poupart,University of Waterloo;Kee-Eung Kim,KAIST",jmlee@ai.kaist.ac.kr;ghkim@ai.kaist.ac.kr;ppoupart@uwaterloo.ca;kekim@cs.kaist.ac.kr,Reinforcement Learning and Planning/Planning*; Reinforcement Learning and Planning/Markov Decision Processes,Poster
4911,Learning to Repair Software Vulnerabilities with Generative Adversarial Networks,"Motivated by the problem of automated repair of software vulnerabilities, we propose an adversarial learning approach that maps from one discrete source domain to another target domain without requiring paired labeled examples or source and target domains to be bijections. We demonstrate that the proposed adversarial learning approach is an effective technique for repairing software vulnerabilities, performing close to seq2seq approaches that require labeled pairs. The proposed Generative Adversarial Network approach is application-agnostic in that it can be applied to other problems similar to code repair, such as grammar correction or sentiment translation.","Jacob Harer*,Boston University;Onur Ozdemir,Draper;Tomo Lazovich,Lightmatter;Christopher Reale,Draper;Rebecca Russell,Draper;Louis Kim,Draper;Sang Chin,Boston University",jharer@bu.edu;oozdemir@syr.edu;tomo@lightmatter.ai;creale@draper.com;rrussell@draper.com;lkim@draper.com;spchin@cs.bu.edu,Deep Learning/Adversarial Networks*; Applications/Natural Language Processing; Applications/Text Analysis,Poster
4923,Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation,"Neural Machine Translation (NMT) has achieved remarkable progress with the quick evolvement of model structures. In this paper, we propose the concept of layer-wise coordination for NMT, which explicitly coordinates the learning of hidden representations of the encoder and decoder together layer by layer, gradually from low level to high level. Furthermore, we share the parameters of each layer between the encoder and decoder to regularize and coordinate the learning. Experiments show that combined with the state-of-the-art Transformer model, layer-wise coordination achieves improvements on three IWSLT and two WMT translation tasks. More specifically, our method achieves 34.43 and 29.01 BLEU score on WMT16 English-Romanian and WMT14 English-German tasks, outperforming the Transformer baseline.","Tianyu He,University of Science and Technology of China;Xu Tan*,Microsoft Research;Yingce Xia,University of Science and Technology of China;Di He,Peking University;Tao Qin,Microsoft Research Asia;Zhibo Chen,University of Science and Technology of China;Tieyan Liu,",hetianyu@mail.ustc.edu.cn;xuta@microsoft.com;yingce.xia@gmail.com;dihe@microsoft.com;taoqin@microsoft.com;chenzhibo@ustc.edu.cn;tyliu@microsoft.com,Applications/Natural Language Processing*; Deep Learning,Poster
4925,Dirichlet belief networks as structured topic prior,"Recently, on the success of deep learning, considerable research effort has been devoted to developing deep architectures for topic models. Although several deep models have been proposed to learn better topic proportions of documents,
how to leverage the benefits of deep structures for learning word distributions of topics has not yet been rigorously studied. Here we propose a new multi-layer generative process on word distributions of topics, where each layer consists of a set of topics and each topic is drawn from a mixture of the topics in the layer above. As the topics in all the layers can be directly interpreted by words, the proposed model is able to discover interpretable topic hierarchies. As an self-contained module, our model can be flexibly adapted to different kinds of topic models to improve their modelling accuracy and interpretability. Extensive experiments on real corpora demonstrated the advantages of the proposed model.","He Zhao*,Monash University;Lan Du,Monash University;Wray Buntine,Monash University;Mingyuan Zhou,University of Texas at Austin",he.zhao@monash.edu;Lan.Du@monash.edu;wray.buntine@monash.edu;Mingyuan.Zhou@mccombs.utexas.edu,Probabilistic Methods/Topic Models*; Applications/Text Analysis,Poster
4930,Stochastic Expectation Maximization with Variance Reduction,"Expectation-Maximization (EM) is a popular tool for learning latent variable models, but the vanilla batch EM does not scale to large data sets because the whole data set is needed at every E-step. Stochastic Expectation Maximization (sEM) reduces the cost of E-step by stochastic approximation. However, sEM has a slower asymptotic convergence rate than batch EM, and requires a decreasing sequence of step sizes, which is difficult to tune. In this paper, we propose a variance reduced stochastic EM (sEM-vr) algorithm inspired by variance reduced stochastic gradient descent algorithms. We show that sEM-vr has the same exponential asymptotic convergence rate as batch EM. Moreover, sEM-vr only requires a constant step size to achieve this rate, which alleviates the burden of parameter tuning. We compare sEM-vr with batch EM, sEM and other algorithms on Gaussian mixture models and probabilistic latent semantic analysis, and sEM-vr converges significantly faster than these baselines.","Jianfei Chen*,Tsinghua University;Jun Zhu,Tsinghua University;Yee Whye Teh,University of Oxford;Tong Zhang,",chrisjianfeichen@gmail.com;dcszj@tsinghua.edu.cn;y.w.teh@stats.ox.ac.uk;bradymzhang@tencent.com,Probabilistic Methods/Latent Variable Models*; Optimization/Non-Convex Optimization,Poster
4941,Submodular Maximization via Gradient Ascent: The Case of Deep Submodular   Functions,"We study the problem of maximizing deep submodular functions (DSFs) subject to a matroid constraint. DSFs are an expressive class of submodular functions that include, as strict subfamilies, the facility location, weighted coverage, and sums of concave composed with modular functions. We use a strategy similar to the continuous greedy approach~\cite{calinescu2007maximizing}, but we show that the multilinear extension of any DSF has a natural and computationally attainable concave relaxation that we can optimize using gradient ascent. Our results show a guarantee of \max_{0<\delta<1}(1-\epsilon-\delta-e^{-\delta^2O(k)}) with a running time of O(\nicefrac{n^2}{\epsilon^2}) plus time for pipage rounding to recover a discrete solution, where k is the rank of the matroid constraint. This bound is often better than the standard 1-1/e guarantee of the continuous greedy algorithm, but runs much faster. Our bound also holds even for fully curved (c=1) functions where the guarantee of 1-c/e degenerates to 1-1/e where $c$ is the curvature of f.  We perform computational experiments that support our theoretical results.","Wenruo Bai*,""University of Washington, Seattle"";William Noble,University of Washington;Jeff Bilmes,UW",wrbai@uw.edu;wnoble@uw.edu;bilmes@uw.edu,Optimization/Submodular Optimization*; Optimization/Combinatorial Optimization; Optimization/Convex Optimization,Poster
4944,The challenge of realistic music generation: modelling raw audio at scale,"Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.","Sander Dieleman*,DeepMind;Aaron van den Oord,DeepMind;Karen Simonyan,DeepMind",sedielem@google.com;avdnoord@google.com;simonyan@google.com,Applications/Music Modeling and Analysis*; Deep Learning/Deep Autoencoders; Deep Learning/Generative Models,Poster
4946,Spectral Signatures in Backdoor Attacks on Deep Nets,"A recent line of work has uncovered a new form of data poisoning: so-called backdoor attacks. These attacks are particularly dangerous because they do not affect a network's behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by an adversary's planted perturbation.

In this paper, we identify a new property of all known backdoor attacks, which we call spectral signatures. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards a principled understanding of backdoor attacks.","Brandon Tran*,MIT;Jerry Li,MIT;Aleksander Madry,MIT",btran115@mit.edu;jerryzli@mit.edu;madry@mit.edu,"Applications/Privacy, Anonymity, and Security*; Algorithms/Spectral Methods; Deep Learning",Poster
4951,Reward learning from human preferences and demonstrations in Atari,"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we need humans to communicate an objective to the agent directly. In this work, we combine two approaches to this problem: learning from expert demonstrations and learning from trajectory preferences. We use both to train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games. Additionally, we investigate the fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.","Jan Leike,DeepMind;Borja Ibarz*,DeepMind;Dario Amodei,OpenAI;Geoffrey Irving,OpenAI;Shane Legg,",leike@google.com;bibarz@google.com;damodei@openai.com;irving@openai.com;legg@google.com,Reinforcement Learning and Planning/Reinforcement Learning*; Algorithms/Ranking and Preference Learning; Deep Learning,Poster
4952,Approximate Knowledge Compilation by Online Collapsed Importance Sampling,"We introduce collapsed compilation, a novel approximate inference algorithm for discrete probabilistic graphical models. 
It is a collapsed sampling algorithm that incrementally selects which variable
to sample next based on the partial sample obtained so far. This online
collapsing, together with knowledge compilation inference on the remaining
variables, naturally exploits local structure and context-specific independence
in the distribution. These properties are naturally exploited in exact inference, but are difficult to harness for approximate inference.
Moreover, by having a partially compiled circuit available
during sampling, collapsed compilation has access to a highly-effective proposal
distribution for importance sampling. Our experimental evaluation shows that
collapsed compilation performs well on standard benchmarks. In particular, when the
amount of exact inference is equally limited, collapsed
compilation is competitive with the state of the art.","Tal Friedman*,UCLA;Guy Van den Broeck,UCLA",tal@cs.ucla.edu;guyvdb@cs.ucla.edu,Probabilistic Methods/Graphical Models*; Algorithms/Relational Learning,Oral
4959,Neural Arithmetic Logic Units,"Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.","Andrew Trask*,Google DeepMind;Felix Hill,Deepmind;Scott Reed,DeepMind;Jack Rae,Deepmind;Chris Dyer,DeepMind;Phil Blunsom,",liamtrask@gmail.com;felixhill@google.com;reedscot@google.com;jwrae@google.com;cdyer@google.com;pblunsom@google.com,Deep Learning*; Deep Learning/Memory-Augmented Neural Networks; Deep Learning/Recurrent Networks,Poster
4965,Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training,"Distributed training of deep nets is an important technique  to address some of the present day computing challenges like memory consumption and computational demands. Classical distributed approaches, synchronous or asynchronous, are based on the parameter server architecture, i.e., worker nodes compute gradients which are communicated to the parameter server while  updated parameters are returned. Recently, distributed training with AllReduce operations gained popularity as well. While many of those operations seem appealing, little is reported about wall-clock training time improvements. In this paper, we carefully analyze the AllReduce based setup, propose timing models which include communication, latency and compute time, and demonstrate that a pipelined training with a width of two combines the best of both synchronous and asynchronous training. Specifically, for a setup consisting of a four-node GPU cluster we show wall-clock time training improvements of up to 5.4x compared to conventional approaches.","Youjie Li*,UIUC;Mingchao Yu,University of Southern California;Songze Li,University of Southern California;Salman Avestimehr,University of Southern California;Nam Sung Kim,University of Illinois at Urbana-Champaign;Alexander Schwing,UIUC",li238@illinois.edu;fishermanymc@gmail.com;songzeli@usc.edu;avestimehr@gmail.com;nam.sung.kim@gmail.com;aschwing@illinois.edu,Applications/Communication- or Memory-Bounded Learning,Poster
4972,Improved Expressivity Through Dendritic Neural Networks,"A typical biological neuron, such as a pyramidal neuron of neocortex receives thousands of afferent synaptic inputs on its dendrite tree and sends the efferent axonal output downstream. In typical artificial neural networks, dendrite trees are modeled as linear structures that funnel weighted synaptic inputs to the cell bodies. However, numerous experimental and theoretical studies have shown that dendritic arbors are far more than such simple linear accumulators. That is, synaptic inputs can actively modulate their neighboring synaptic activities, therefore the dendritic structures are highly nonlinear. In this study, we model such local nonlinearity of dendritic trees with our Dendritic Neural Network (DENN) structure and apply this to typical machine learning tasks. Equipped with localized non-linearities, DENNs can attain greater model expressivity than regular neural networks while maintain efficient network inference. Such strength is evidenced by higher fitting power when we train DENNs with supervised machine learning tasks. We also empirically show that the locality structure can give DENNs a boost to their generalization ability, exemplified by outranking naive deep neural network architectures when tested on 121 classification tasks from the UCI machine learning repository.","Xundong Wu*,Hangzhou Dianzi University;Xiangwen Liu,Hangzhou Dianzi University;Wei Li,Hangzhou Dianzi University;Qing Wu,Hangzhou Dianzi University",wuxundong@gmail.com;liuxiangwenhz@gmail.com;hduweili@gmail.com;wuq@hdu.edu.cn,Deep Learning*; Neuroscience and Cognitive Science/Neuroscience,Poster
4974,Efficient Anomaly Detection via Matrix Sketching,"We consider the problem of finding anomalies in high-dimensional data using popular PCA based anomaly scores.  The naive algorithms for computing these scores explicitly compute the PCA of the covariance matrix which uses space quadratic in the dimensionality of the data. We give the first streaming algorithms that use space that is linear or sublinear in the dimension. We prove general results showing that \emph{any} sketch of a matrix that satisfies a certain operator norm guarantee can be used to approximate these scores. We instantiate these results with powerful matrix sketching techniques such as Frequent Directions and random projections to derive efficient and practical algorithms for these problems, which we validate over real-world data sets. Our main technical contribution is to prove matrix perturbation inequalities for operators arising in the computation of these measures.","Vatsal Sharan*,Stanford University;Parikshit Gopalan,VMware Research;Udi Wieder,VMware Research",vsharan@stanford.edu;pgopalan@vmware.com;uwieder@vmware.com,"Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA)*; Applications/Communication- or Memory-Bounded Learning",Poster
4975,Learning to Specialize with Knowledge Distillation for Visual Question Answering,"Visual Question Answering (VQA) is a notoriously challenging problem because it involves various heterogeneous tasks defined by questions within a unified framework. Learning specialized models for individual types of tasks is intuitively attracting but surprisingly difficult; it is not straightforward to outperform naive independent ensemble approaches. We present a principled algorithm to learn specialized models with knowledge distillation under a multiple choice learning framework. The training examples are dynamically assigned to a subset of models for specializing their functionality. The assigned and non-assigned models are learned to predict ground-truth answers and imitate their own base models before specialization, respectively. Our approach alleviates the problem of data deficiency, which is a critical limitation in existing frameworks on multiple choice learning, and allows each model to learn its own specialized expertise without forgetting general knowledge by knowledge distillation. Our experiments show that the proposed algorithm achieves the superior performances compared to naive ensemble methods and other baselines in VQA. Our framework is also effective for more general tasks, e.g., image classification with a large number of labels, which is known to be difficult under existing multiple choice learning schemes.","Jonghwan Mun*,POSTECH;Kimin Lee,KAIST;Jinwoo Shin,KAIST;Bohyung Han,Seoul National University",choco1916@postech.ac.kr;kiminlee@kaist.ac.kr;jinwoos@kaist.ac.kr;bhhan@snu.ac.kr,Applications/Visual Question Answering*; Algorithms/Boosting and Ensemble Methods,Poster
4976,A Lyapunov-based Approach to Safe Reinforcement Learning,"In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints.
In particular, besides optimizing performance it is crucial to guarantee the \emph{safety} of an agent during training as well as deployment (e.g. a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware).
To incorporate safety in RL, we derive algorithms under the framework of Constrained Markov decision problems (CMDPs), an extension of the standard Markov decision problems (MDPs) augmented with constraints on expected cumulative costs.
Our approach hinges on a novel \emph{Lyapunov} method.  
We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local, linear constraints.
Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. 
To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.","Yinlam Chow*,DeepMind;Ofir Nachum,Google;Edgar Duenez-Guzman,Google DeepMind;Mohammad Ghavamzadeh,FAIR",yinlamchow@google.com;ofirnachum@google.com;eaduenez@gmail.com;mohammad.ghavamzadeh@inria.fr,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
4977,Credit Assignment For Collective Multiagent RL With Global Rewards,"Scaling decision theoretic planning to large multiagent systems is challenging due to uncertainty and partial observability in the environment. We focus on a multiagent planning model subclass, relevant to urban settings, where agent interactions are dependent on their ``collective influence'' on each other, rather than their identities. Unlike previous work, we address a general setting where system reward is not decomposable among agents. We develop collective actor-critic RL approaches for this setting, and address the problem of multiagent credit assignment, and computing low variance policy gradient estimates that result in faster convergence to high quality solutions. We also develop difference rewards based credit assignment methods for the collective setting. Empirically our new approaches provide significantly better solutions than previous methods in the presence of global rewards on two real world problems modeling taxi fleet optimization and multiagent patrolling, and a synthetic grid navigation domain. ","Duc Thien Nguyen*,Singapore Management University;Akshat Kumar,Singapore Management University;Hoong Chuin Lau,Singapore Management University",dtnguyen.2014@phdis.smu.edu.sg;akshatkumar@smu.edu.sg;hclau@smu.edu.sg,Reinforcement Learning and Planning/Multi-Agent RL*; Reinforcement Learning and Planning/Markov Decision Processes; Reinforcement Learning and Plannin,Poster
4978,Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes,"We consider stochastic gradient descent (SGD) for least-squares regression with potentially several passes over the data. While several passes have been widely reported to perform practically better in terms of predictive performance on unseen data, the existing theoretical analysis of SGD suggests that a single pass is statistically optimal. While this is true for low-dimensional easy problems, we show that for hard problems, multiple passes lead to statistically optimal predictions while single pass does not; we also show that in these hard models, the optimal number of passes over the data increases with sample size. In order to define the notion of hardness and show that our predictive performances are optimal, we consider potentially infinite-dimensional models and notions typically associated to kernel methods, namely, the decay of eigenvalues of the covariance matrix of the features and the complexity of the optimal predictor as measured through the covariance matrix.
We illustrate our results on synthetic experiments with non-linear kernel methods and on a classical benchmark with a linear model.","Loucas Pillaud-Vivien*,INRIA - Ecole Normale Sup√©rieure;Alessandro Rudi,INRIA, Ecole Normale Superieure;Francis Bach,INRIA - Ecole Normale Sup√©rieure",loucas.pillaud-vivien@inria.fr;alessandro.rudi@inria.fr;francis.bach@inria.fr,Algorithms/Stochastic Methods*; Algorithms/Kernel Methods; Algorithms/Regression; Optimization/Convex Optimization; Theory,Poster
4982,Does mitigating ML's impact disparity require treatment disparity?,"Following precedent in employment discrimination law, two notions of disparity are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently;
algorithms exhibit impact disparity when outcomes differ across subgroups (even unintentionally). Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning processes (DLPs). Here, the sensitive feature is used during training but a group-blind classifier is produced. In this paper, we show that: (i) when sensitive and (nominally) nonsensitive features are correlated, DLPs will indirectly implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) when group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) in general, DLPs provide suboptimal trade-offs between accuracy and impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs. ","Zachary  Lipton*,Carnegie Mellon University;Julian McAuley,UCSD;Alexandra Chouldechova,CMU",zlipton@cmu.edu;jmcauley@eng.ucsd.edu;achould@cmu.edu,"Applications/Fairness, Accountability, and Transparency",Poster
4984,Proximal Graphical Event Models,"Event datasets include events that occur irregularly over the timeline and are prevalent in numerous domains. We introduce proximal graphical event models (PGEM) as a representation of such datasets. PGEMs belong to a broader family of models that characterize relationships between various types of events, where the rate of occurrence of an event type depends only on whether or not its parents have occurred in the most recent history. The main advantage over the state of the art models is that they are entirely data driven and do not require additional inputs from the user, which can require knowledge of the domain such as choice of basis functions or hyperparameters in graphical event models. We theoretically justify our learning of  optimal windows for parental history and the choices of parental sets, and the algorithm are sound and complete in terms of parent structure learning.  We present additional efficient heuristics for learning PGEMs from data, demonstrating their effectiveness on synthetic and real datasets.","Debarun Bhattacharjya,IBM Research;Dharmashankar Subramanian*,IBM Research;Tian Gao,IBM Research",debarunb@us.ibm.com;dharmash@us.ibm.com;tgao@us.ibm.com,Probabilistic Methods/Graphical Models*; Algorithms; Algorithms/Adaptive Data Analysis; Algorithms/Model Selection and Structure Learni,Spotlight
4998,Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor Environments,"We propose a Bayesian decision making framework for control of Markov Decision Processes (MDPs) with unknown dynamics and large, possibly continuous, state, action, and parameter spaces in data-poor environments. Most of the existing adaptive controllers for MDPs with unknown dynamics are based on the reinforcement learning framework and rely on large data sets acquired by sustained direct interaction with the system or via a simulator. This is not feasible in many applications, due to ethical, economic, and physical constraints. The proposed framework addresses the data poverty issue by decomposing the problem into an offline planning stage that does not rely on sustained direct interaction with the system or simulator and an online execution stage. In the offline process, parallel Gaussian process temporal difference (GPTD) learning techniques are employed for near-optimal Bayesian approximation of the expected discounted reward over a sample drawn from the prior distribution of unknown parameters. In the online stage, the action with the maximum expected return with respect to the posterior distribution of the parameters is selected. This is achieved by an approximation of the posterior distribution using a Markov Chain Monte Carlo (MCMC) algorithm, followed by constructing multiple Gaussian processes over the parameter space for efficient prediction of the means of the expected return at the MCMC sample. The effectiveness of the proposed framework is demonstrated using a simple dynamical system model with continuous state and action spaces, as well as a more complex model for a metastatic melanoma gene regulatory network observed through noisy synthetic gene expression data.","Mahdi Imani*,Texas A&M University;Seyede Fatemeh Ghoreishi,Texas A&M University;Ulisses M. Braga-Neto,Texas A&M University",m.imani88@tamu.edu;f.ghoreishi88@tamu.edu;ulisses@tamu.edu,Reinforcement Learning and Planning/Markov Decision Processes*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Poster
4999,Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,"Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated 
distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset. ","Yuanzhi Li*,Princeton;Yingyu Liang,University of Wisconsin Madison",yuanzhil@cs.princeton.edu;yliang@cs.wisc.edu,Theory/Learning Theory*; Deep Learning/Optimization for Deep Networks; Optimization/Non-Convex Optimization; Theory ->,Spotlight
5004,Hamiltonian Variational Auto-Encoder,"Variational Auto-Encoders (VAE) have become very popular techniques to perform
inference and learning in latent variable models as they allow us to leverage the rich
representational power of neural networks to obtain flexible approximations of the
posterior of latent variables as well as tight evidence lower bounds (ELBO). Com-
bined with stochastic variational inference, this provides a methodology scaling to
large datasets. However, for this methodology to be practically efficient, it is neces-
sary to obtain low-variance unbiased estimators of the ELBO and its gradients with
respect to the parameters of interest. While the use of Markov chain Monte Carlo
(MCMC) techniques such as Hamiltonian Monte Carlo (HMC) has been previously
suggested to achieve this [23, 26], the proposed methods require specifying reverse
kernels which have a large impact on performance. Additionally, the resulting
unbiased estimator of the ELBO for most MCMC kernels is typically not amenable
to the reparameterization trick. We show here how to optimally select reverse
kernels in this setting and, by building upon Hamiltonian Importance Sampling
(HIS) [17], we obtain a scheme that provides low-variance unbiased estimators of
the ELBO and its gradients using the reparameterization trick. This allows us to
develop a Hamiltonian Variational Auto-Encoder (HVAE). This method can be
re-interpreted as a target-informed normalizing flow [20] which, within our context,
only requires a few evaluations of the gradient of the sampled likelihood and trivial
Jacobian calculations at each iteration.","Anthony Caterini*,University of Oxford;Arnaud Doucet,Oxford University;Dino Sejdinovic,University of Oxford",anthony.caterini@stats.ox.ac.uk;doucet@stats.ox.ac.uk;dino.sejdinovic@stats.ox.ac.uk,Probabilistic Methods/Variational Inference*; Deep Learning/Deep Autoencoders; Probabilistic Methods/Latent Variable Models; Probabilistic M,Poster
5005,Modelling and unsupervised learning of symmetric deformable object categories,"We look at the problem of learning the structure of categories of symmetric visual objects from raw images, without manual supervision. We show that we can capture the intuitive notion of symmetry in natural objects, which often clashes with its classical mathematical definition, by looking at the symmetry not of geometric shapes, but of object deformations. We do so by building on the recently-introduced object frame representation and show how the latter can be extended to capture symmetries, mapping them to simple transformation groups in representation space. An advantage of the original object frame is that it is amenable to unsupervised learning. We show that our formulation leads to a direct generalization of this learning strategy that allows learning the symmetries of natural objects also in an unsupervised manner. Finally, we show that our formulation provides an explanation of the ambiguities in pose recovery that arise from certain symmetries and we provide a way of discounting such ambiguities in learning.","James Thewlis*,University of Oxford;Hakan Bilen,University of Edinburgh;Andrea Vedaldi,Oxford University",jdt@robots.ox.ac.uk;hbilen@ed.ac.uk;vedaldi@robots.ox.ac.uk,Applications/Computer Vision*; Algorithms/Representation Learning; Algorithms/Unsupervised Learning,Poster
5007,Sequential Monte Carlo for probabilistic graphical models via twisted targets,"Approximate inference in probabilistic graphical models (PGMs) can be grouped into deterministic methods and Monte-Carlo-based methods. The former can often provide accurate and rapid inferences, but are typically associated with biases that are hard to quantify. The latter enjoy asymptotic consistency, but can suffer from high computational costs. In this paper we present a way of bridging the gap between deterministic and stochastic inference. Specifically, we suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which can leverage the output from deterministic inference methods. While generally applicable, we show explicitly how this can be done with loopy belief propagation, expectation propagation, and Laplace approximations. The resulting algorithm can be viewed as a post-correction of the biases associated with these methods and, indeed, numerical results show clear improvements over the baseline deterministic methods as well as over ""plain"" SMC.	","Fredrik Lindsten*,Uppsala University;Jouni Helske,Link√∂ping University;Matti Vihola,University of Jyv√§skyl√§",fredrik.lindsten@it.uu.se;jouni.helske@liu.se;matti.vihola@iki.fi,Probabilistic Methods*; Probabilistic Methods/Graphical Models; Probabilistic Methods/MCMC,Spotlight
5008,Statistical mechanics of low-rank tensor decomposition,"Often, large, high dimensional datasets collected across multiple modalities can be organized as a higher order tensor. Low-rank tensor decomposition then arises as a powerful and widely used tool to discover simple low dimensional structures underlying such data. However, we currently lack a theoretical understanding of the algorithmic behavior of low-rank tensor decomposition. We derive Bayesian approximate message passing (AMP) algorithms for recovering arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic mean field theory to precisely characterize their performance.   Our theory reveals the existence of phase transitions between easy, hard and impossible inference regimes, and displays an excellent match with simulations.  Moreover it reveals several qualitative surprises compared to the behavior of symmetric, cubic tensor decomposition. Finally, we compare our AMP algorithm to the most commonly used algorithm, the alternating least squares (ALS), and demonstrate that AMP significantly outperforms ALS in the presence of noise.","Jonathan Kadmon*,Stanford University;Surya Ganguli,",kadmonj@stanford.edu;sganguli@stanford.edu,Theory/Statistical Physics of Learning*; Applications/Matrix and Tensor Factorization; Probabilistic Methods/Belief Propagation,Poster
5015,Variational Bayesian Monte Carlo,"Many probabilistic models of interest in scientific computing and machine learning have expensive, black-box likelihoods that prevent the application of standard techniques for Bayesian inference, such as MCMC, which would require access to the gradient or a large number of likelihood evaluations.
We introduce here a novel sample-efficient inference framework, Variational Bayesian Monte Carlo (VBMC). VBMC combines variational inference with Gaussian-process based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective.
Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence, useful for model selection.
We demonstrate VBMC both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to D = 10), VBMC performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations, unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive, black-box likelihoods.","Luigi Acerbi*,University of Geneva",luigi.acerbi@gmail.com,Probabilistic Methods/Gaussian Processes*; Algorithms/Active Learning; Neuroscience and Cognitive Science/Neuroscience; Probabilistic Met,Poster
5026,Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion,"There is growing interest in combining model-free and model-based approaches in reinforcement learning with the goal of achieving the high performance of model-free algorithms with low sample complexity. This is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency.","Jacob Buckman*,Google;Danijar Hafner,Google;George Tucker,Google Brain;Eugene Brevdo,Google;Honglak Lee,Google",jacobbuckman@gmail.com;mail@danijar.com;gjt@google.com;ebrevdo@google.com;honglak@google.com,Reinforcement Learning and Planning/Model-Based RL*; Reinforcement Learning and Planning; Reinforcement Learning and Planning/Decision and Control; Re,Oral
5033,Efficient Online Portfolio with Logarithmic Regret,"We study the decades-old problem of online portfolio management and propose the first algorithm with logarithmic regret that is not based on Cover's Universal Portfolio algorithm and admits much faster implementation. Specifically Universal Portfolio enjoys optimal regret $\mathcal{O}(N\ln T)$ for $N$ financial instruments over $T$ rounds, but requires log-concave sampling and has a large polynomial running time. Our algorithm, on the other hand, ensures a slightly larger but still logarithmic regret of $\mathcal{O}(N^2(\ln T)^4)$, and is based on the well-studied Online Mirror Descent framework with a novel regularizer that can be implemented via standard optimization methods in time $\mathcal{O}(TN^{2.5})$ per round. The regret of all other existing works is either polynomial in $T$ or has a potentially unbounded factor such as the inverse of the smallest price relative.","Haipeng Luo,University of Southern California;Chen-Yu Wei*,University of Southern California;Kai Zheng,Peking University",haipeng.luo@gmail.com;chenyu.wei@usc.edu;zhengk92@pku.edu.cn,Algorithms/Online Learning*; Theory/Learning Theory,Spotlight
5034,Algorithms and Theory for Multiple-Source Adaptation,"This work includes a number of novel contributions for the multiple-source adaptation problem. We present new normalized solutions with strong theoretical guarantees for the cross-entropy loss and other similar losses. We also provide new guarantees that hold in the case where the conditional probabilities for the source domains are distinct. Moreover, we give new algorithms for determining the distribution-weighted combination solution for the cross-entropy loss and other losses. We report the results of a series of experiments with real-world datasets. We find that our algorithm outperforms competing approaches by producing a single robust model that performs well on  any target mixture distribution. Altogether, our theory, algorithms, and empirical results provide a full solution for the multiple-source adaptation problem with very practical benefits.","Judy Hoffman,UC Berkeley;Mehryar Mohri,NYU;Ningshan Zhang*,NYU",jhoffman@eecs.berkeley.edu;mohri@cs.nyu.edu;nzhang@stern.nyu.edu,Algorithms/Multitask and Transfer Learning*; Applications/Computer Vision; Applications/Object Recognition; Applications/Text Analysis;,Poster
5037,Online Reciprocal Recommendation with Theoretical Performance Guarantees,"A reciprocal recommendation problem is one where the goal of learning is not just to predict a user's preference towards a passive item (e.g., a book), but to recommend the targeted user on one side another user from the other side such that a mutual interest between the two exists. The problem thus is sharply different from the more traditional items-to-users recommendation, since a good match requires meeting the preferences of both users. We initiate a rigorous theoretical investigation of the reciprocal recommendation task in a specific framework of sequential learning. We point out general limitations, formulate reasonable assumptions enabling effective learning and, under these assumptions, we design and analyze a computationally efficient algorithm that uncovers mutual likes at a pace comparable to those achieved by a clearvoyant algorithm knowing all user preferences in advance. Finally, we validate our algorithm against synthetic and real-world datasets, showing improved empirical performance over simple baselines.","Claudio Gentile,INRIA;Nikos Parotsidis,University of Rome Tor Vergata;Fabio Vitale*,University of Lille",cla.gentile@gmail.com;nikos.parotsidis@uniroma2.it;fabio.vitale@inria.fr,Algorithms/Online Learning*; Algorithms/Active Learning; Algorithms/Collaborative Filtering; Applications/Recommender Sy,Poster
5038,The promises and pitfalls of Stochastic Gradient Langevin Dynamics,"Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC algorithm for Bayesian learning from large scale datasets. While SGLD with decreasing step sizes converges weakly to the posterior distribution, the algorithm is often used with a constant step size in practice and has demonstrated spectacular successes in machine learning tasks. The current practice is to set the step size inversely proportional to N where N is the number of training samples. As N becomes large, we show that the SGLD algorithm has an invariant probability measure which significantly departs from the target posterior and behaves like as Stochastic Gradient Descent (SGD). This difference is inherently due to the high variance of the stochastic gradients. Several strategies have been suggested to reduce this effect; among them, SGLD Fixed Point (SGLDFP) uses carefully designed control variates to reduce the variance of the stochastic gradients. We show that SGLDFP gives approximate samples from the posterior distribution, with an accuracy comparable to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear in the number of data points. We provide a detailed analysis of the Wasserstein distances between LMC, SGLD, SGLDFP and SGD and explicit expressions of the means and covariance matrices of their invariant distributions. Our findings are supported by limited numerical experiments.","Nicolas Brosse*,Ecole Polytechnique, Palaiseau, FRANCE;Alain Durmus,ENS;Eric Moulines,Ecole Polytechnique",nbrosse@gmail.com;alain.durmus@cmla.ens-cachan.fr;eric.moulines@polytechnique.edu,Probabilistic Methods/MCMC,Poster
5039,How SGD selects the global minima in over-parameterized learning: A stability perspective,"The question of which global minima are accessible by an stochastic gradient decent (SGD)  algorithm with specific learning rate and batch size is
studied from the perspective of numerical stability.  The concept of non-uniformity is introduced, which, together with sharpness,
characterizes the stability property of a global minimum and hence the accessibility of a particular SGD algorithm to that global minimum.
In particular, this analysis shows that  learning rate and batch size play different roles in minima selection. 
Extensive empirical results seem to correlate well with the theoretical findings and provide further support to these  claims.","Lei Wu*,Peking University;Chao Ma,Princeton University;Weinan E,Princeton University",leiwu@pku.edu.cn;chaom@princeton.edu;weinan@math.princeton.edu,Optimization*; Algorithms/Dynamical Systems; Deep Learning/Optimization for Deep Networks; Theory/Learning,Poster
5042,Differentiable MPC for End-to-end Planning and Control,"In this paper we present foundations for using model predictive control (MPC) as a differentiable policy class in reinforcement learning.    Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the solver.  Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning in a larger system. We empirically show results in an imitation learning setting, demonstrating that we can recover the underlying dynamics and cost more efficiently and reliably than with a generic neural network policy class.","Brandon Amos*,CMU;Ivan Jimenez,Georgia Tech;Jacob Sacks,Georgia Institute of Technology;Byron Boots,Georgia Institute of Technology;Zico Kolter,Carnegie Mellon University",bamos@cs.cmu.edu;ijimenez3@gatech.edu;jsacks@gatech.edu;bboots@cc.gatech.edu;zkolter@cs.cmu.edu,Reinforcement Learning and Planning*; Optimization/Convex Optimization; Reinforcement Learning and Planning/Decision and Control; Re,Poster
5045,Bilevel learning of the Group Lasso structure,"Regression with group-sparsity penalty plays a central role in high-dimensional prediction problems. However, most of existing methods require the group structure to be known a priori. In practice, this strong assumption often results in a degradation of the prediction performance. To circumvent this issue, we present a method to estimate the group structure by means of a continuous bilevel optimization problem where the data is split into training and validation sets. Our approach relies on an approximation where the lower problem is replaced by a smooth dual forward-backward scheme with Bregman distances. We provide guarantees regarding its convergence to the exact problem and demonstrate the well behaviour of the method on synthetic experiments. Finally, a preliminary application to genes expression data is tackled in order to unveil functional groups. ","Jordan Frecon*,Istituto Italiano di Tecnologia;Saverio Salzo,Istituto Italiano di Tecnologia;Massimiliano Pontil, 	Istituto Italiano di Tecnologia & University College London",jordan.frecon@iit.it;saverio.salzo@iit.it;massimiliano.pontil@iit.it,Algorithms/Model Selection and Structure Learning*; Algorithms/Multitask and Transfer Learning; Algorithms/Regression; Algorithms/Structured Pr,Spotlight
5046,Generative Adversarial Examples,"Adversarial examples are typically constructed by perturbing an existing data point, and current defense methods are focused on guarding against this type of attack. In this paper, we propose a new class of adversarial examples that are synthesized entirely from scratch using a conditional generative model. We first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over inputs. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that this new kind of adversarial inputs, which we call Generative Adversarial Examples, are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that generative adversarial examples can easily bypass strong adversarial training and certified defense methods which can foil existing adversarial attacks.","Yang Song*,Stanford University;Rui Shu,Stanford University;Nate Kushman,Microsoft Research;Stefano  Ermon,Stanford University",songyang@stanford.edu;ruishu@stanford.edu;nate@kushman.org;ermon@cs.stanford.edu,"Applications/Privacy, Anonymity, and Security*; Deep Learning/Adversarial Networks; Deep Learning/Generative Models",Poster
5047,Information-theoretic Limits for Community Detection in Network Models,"We analyze the information-theoretic limits for the recovery of node labels in several network models. This includes the Stochastic Block Model, the Exponential Random Graph Model, the Latent Space Model, the Directed Preferential Attachment Model, and the Directed Small-world Model. For the Stochastic Block Model, the non-recoverability condition depends on the probabilities of having edges inside a community, and between different communities. For the Latent Space Model, the non-recoverability condition depends on the dimension of the latent space, and how far and spread are the communities in the latent space. For the Directed Preferential Attachment Model and the Directed Small-world Model, the non-recoverability condition depends on the ratio between homophily and neighborhood size. We also consider dynamic versions of the Stochastic Block Model and the Latent Space Model.","Chuyang Ke*,Purdue University;Jean Honorio,Purdue",cke@purdue.edu;jhonorio@purdue.edu,Theory/Learning Theory*; Theory/Information Theory,Poster
5053,Learning Conditioned Graph Structures for Interpretable Visual Question Answering,"Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on  higher level image representations, which allow to capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain promising results with 66.18\% accuracy and demonstrate the interpretability of the proposed method. Code can be found at github.com/aimbrain/vqa-project.","Will Norcliffe-Brown*,AimBrain;Efstathios Vafeias,AimBrain;Sarah Parisot,AimBrain",will.norcliffe@aimbrain.com;stathis@aimbrain.com;sarah.parisot@huawei.com,Applications/Visual Question Answering*; Deep Learning/Attention Models,Poster
5057,Distributionally Robust Graphical Models,"In many structured prediction problems, complex relationships between variables are compactly defined using graphical structures. The most prevalent graphical prediction methods ---probabilistic graphical models and large margin methods--- have their own distinct strengths but also come with significant drawbacks. Conditional random fields (CRFs) are Fisher consistent, but they do not permit integration of customized loss functions into their learning process. Large-margin models, such as structured support vector machines (SSVM), have the flexibility to incorporate customized loss metrics, but lack Fisher consistency guarantees. We present adversarial graphical models (AGM), a distributionally robust approach for constructing a predictor that performs robustly for a class of data distributions defined using a graphical structure. Our approach enjoys both the flexibility of incorporating customized loss functions into its design as well as the statistical guarantee of Fisher consistency. We present exact learning and prediction algorithms for AGM requiring similar time complexity as existing graphical models and show its practical benefits in our experiments.","Rizal Fathony*,University of Illinois at Chicago;Ashkan Rezaei,University of Illinois at Chicago;Mohammad Ali Bashiri,University of Illinois at Chicago;Xinhua Zhang,UIC;Brian Ziebart,UIC",rfatho2@uic.edu;arezae4@uic.edu;mbashi4@uic.edu;zhangx@uic.edu;bziebart@uic.edu,Probabilistic Methods/Graphical Models*; Algorithms/Structured Prediction,Poster
5066,Transfer Learning with Neural AutoML,"We reduce the computational cost of Neural AutoML with transfer learning. AutoML relieves human effort by automating the design of ML algorithms. Neural AutoML has become popular for the design of deep learning architectures, however, this method has a high computation cost. To address this we propose Transfer Neural AutoML that uses knowledge from prior tasks to speed up network design. We extend RL-based architecture search methods to support parallel training on multiple tasks and then transfer the search strategy to new tasks.
On language and image classification data, Transfer Neural AutoML reduces convergence time over single-task training by over an order of magnitude on many tasks.","Catherine Wong,;Neil Houlsby*,Google;Yifeng Lu,;Andrea Gesmundo,Google",wong.catherine.mail@gmail.com;neilhoulsby@gmail.com;yifenglu@google.com;agesmundo@google.com,Algorithms/Multitask and Transfer Learning*; Algorithms/AutoML,Poster
5067,Stochastic Primal-Dual Method for Empirical Risk Minimization with O(1) Per-Iteration Complexity,"Regularized empirical risk minimization problem with linear predictor appears frequently in machine learning. In this paper, we propose a new stochastic primal-dual method to solve this class of problems. Different from existing methods, our proposed methods only require O(1) operations in each iteration. We also develop a variance-reduction variant of the algorithm that converges linearly. Numerical experiments suggest that our methods are faster than existing ones such as proximal SGD, SVRG and SAGA on high-dimensional problems.","Conghui Tan*,The Chinese University of Hong Kong;Tong Zhang,Tecent AI Lab;Shiqian Ma,;Ji  Liu,University of Rochester",chtan@se.cuhk.edu.hk;tongzhang@tongzhang-ml.org;sqma@math.ucdavis.edu;ji.liu.uwisc@gmail.com,Optimization/Convex Optimization,Poster
5077,On preserving non-discrimination when combining expert advice,"We study the design of online learning algorithms that, when run on members of different groups, do not discriminate against some group. We consider the most basic question in such a setting: how can we design an online learning algorithm that, given access to individually non-discriminatory predictors, guarantees the classical no-regret property and overall non-discrimination at the same time? We show a strong impossibility result for this goal with respect to ""equal opportunity"" that requires equal false negative rates across groups. On the positive side, we show that for another notion of non-discrimination, ""equalized error rates"", such a guarantee is achievable.","Avrim Blum,Toyota Technological Institute of Chicago;Suriya Gunasekar,TTI Chicago;Thodoris Lykouris*,Cornell University;Nathan Srebro,Toyota Technical Institute of Chicago",avrim@ttic.edu;suriya@ttic.edu;teddlyk@cs.cornell.edu;nati@ttic.edu,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Online Learning",Poster
5086,"Learning to Play With Intrinsically-Motivated, Self-Aware Agents","Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation.  Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a ""world-model"" network that learns to predict the dynamic consequences of the agent's actions.  Simultaneously, we train a separate explicit ""self-model"" that allows the agent to track the error map of its world-model. It then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering.  Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks.  Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in realistic physical environments.","Nick Haber*,Stanford University;Damian Mrowca,Stanford University;Stephanie Wang,Stanford University;Li Fei-Fei,Stanford University;Daniel Yamins,Stanford University",nhaber@stanford.edu;mrowca@stanford.edu;Steph17@stanford.edu;feifeili@cs.stanford.edu;yamins@stanford.edu,Neuroscience and Cognitive Science/Cognitive Science*; Algorithms/Active Learning; Algorithms/Representation Learning; Algorithms/Unsupervised Lea,Poster
5094,Scaling provable adversarial defenses,"Recent work has developed methods for learning deep network classifiers that are \emph{provably} robust to norm-bounded adversarial perturbation; however, these methods are currently only possible for relatively small feedforward networks.  In this paper, in an effort to scale these approaches to substantially larger models, we extend previous work in three main directly.  First, we present a technique for extending these training procedures to much more general networks, with skip connections (such as ResNets) and general nonlinearities; the approach is fully modular, and can be implemented automatically analogously to automatic differentiation. Second, in the specific case of $\ell_\infty$ adversarial perturbations and networks with ReLU nonlinearities, we adopt a nonlinear random projection for training, which scales \emph{linearly} in the number of hidden units (previous approached scaled quadratically).  Third, we show how to further improve robust error through cascade models.  On both MNIST and CIFAR data sets, we train classifiers that improve substantially on the state of the art in provable robust adversarial error bounds: from 5.8% to 3.1% on MNIST  (with $\ell_\infty$ perturbations of $\epsilon=0.1$), and from 80% to 36.4% on CIFAR (with $\ell_\infty$ perturbations of $\epsilon=2/255$).","Eric Wong*,Carnegie Mellon University;Frank Schmidt,Robert Bosch GmbH;Jan Hendrik Metzen,Robert Bosch GmbH;Zico Kolter,Carnegie Mellon University",ericwong@cs.cmu.edu;frank.r.schmidt@de.bosch.com;JanHendrik.Metzen@de.bosch.com;zkolter@cs.cmu.edu,Deep Learning*; Deep Learning/Adversarial Networks,Poster
5104,Deep Network for the Integrated 3D Sensing of Multiple People in Natural Images," We present a feed-forward, multitask, end-to-end trainable system for the integrated 2d localization, as well as 3d pose and shape estimation, of multiple people in monocular images. The challenge is the formal modeling of the problem that intrinsically requires discrete and continuous computation (e.g. grouping people vs. predicting 3d pose). The model identifies human body structures (joints and limbs) in images, groups them based on 2d and 3d information fused using learned scoring functions, and optimally aggregates such responses into partial or complete 3d human skeleton hypotheses under kinematic tree constraints, but without knowing in advance the number of people in the scene and their visibility relations. We design a single multi-task deep neural network with differentiable stages where the person grouping problem is formulated as an integer program based on learned body part scores parameterized by both 2d and 3d information. This avoids suboptimality resulting from separate 2d and 3d reasoning, with grouping performed based on the combined information. The calculation can be formally described as a linear binary integer program with globally optimal solution. The final predictive stage of 3d pose and shape is based on a learned attention process where information from different human body parts is optimally fused. State-of-the-art results are obtained in large scale datasets like Human3.6M and Panoptic. ","Andrei Zanfir*,IMAR and Lund University;Elisabeta Marinoiu,IMAR;Mihai Zanfir,IMAR;Alin-Ionut Popa,IMAR;Cristian Sminchisescu,Lund University",andrei.zanfir@imar.ro;elisabeta.marinoiu@imar.ro;mihai.zanfir@imar.ro;alin.popa@imar.ro;cristian.sminchisescu@math.lth.se,"Applications/Computer Vision*; Applications/Body Pose, Face, and Gesture Analysis; Deep Learning/CNN Architectures",Spotlight
5106,Almost Optimal Algorithms for Linear Stochastic Bandits with Heavy-Tailed Payoffs,"In linear stochastic bandits, it is commonly assumed that payoffs are with sub-Gaussian noises. In this paper, under a weaker assumption on noises, we study the problem of linear stochastic bandits with heavy-tailed payoffs (LinBET), where the distributions have finite moments of order $1+\epsilon$, for some $\epsilon\in (0,1]$. We rigorously analyze the regret lower bound of LinBET as $\Omega(T^{\frac{1}{1+\epsilon}})$, implying that finite moments of order 2 (i.e., finite variances) yield the bound of $\Omega(\sqrt{T})$, with $T$ being the total number of rounds to play bandits. The provided lower bound also indicates that the state-of-the-art algorithms for LinBET are far from optimal. By adopting median of means with a well-designed allocation of decisions and truncation based on historical information, we elaborately develop two novel bandit algorithms, where the regret upper bounds match the lower bound up to polylogarithmic factors. To the best of our knowledge, we are the first to solve LinBET optimally in the sense of the polynomial order on $T$.  Our proposed algorithms are evaluated based on synthetic datasets, and outperform the state-of-the-art results.","Han Shao,The Chinese University of Hong Kong;Xiaotian Yu*,The Chinese University of Hong Kong;Irwin King,The Chinese University of Hong Kong;Michael Lyu,The Chinese University of Hong Kong",hshao@cse.cuhk.edu.hk;xtyu@cse.cuhk.edu.hk;king@cse.cuhk.edu.hk;lyu@cse.cuhk.edu.hk,Algorithms/Bandit Algorithms*; Algorithms/Online Learning,Spotlight
5112,Data-dependent PAC-Bayes priors via differential privacy,"The Probably Approximately Correct (PAC) Bayes framework (McAllester, 1999) can incorporate knowledge about the learning algorithm and (data) distribution through the use of distribution-dependent priors, yielding tighter generalization bounds on data-dependent posteriors. Using this flexibility, however, is difficult, especially when the data distribution is presumed to be unknown. We show how an Œµ-differentially private data-dependent prior yields a valid PAC-Bayes bound, and then show how non-private mechanisms for choosing priors obtain the same generalization bound provided they are close in 2-Wasserstein distance to an Œµ-differentially private mechanism. As an application of this result, we show that a Gaussian prior mean chosen via stochastic gradient Langevin dynamics (SGLD; Welling and Teh, 2011) leads to a valid PAC-Bayes bound, despite SGLD only converging in 2-Wasserstein distance to an Œµ-differentially private mechanism. We study our data-dependent bounds empirically, and show that they can be nonvacuous even when other distribution-dependent bounds are vacuous.","Gintare Karolina  Dziugaite*,University of Cambridge;Daniel Roy,University of Toronto",gkd22@cam.ac.uk;droy@utstat.toronto.edu,Theory/Learning Theory*; Deep Learning; Theory,Poster
5118,Deep Poisson gamma dynamical systems,"We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially observed multivariate count data, improving previously proposed models by not only mining deep hierarchical latent structure from the data, but also capturing both first-order and long-range temporal dependencies. Using sophisticated but simple-to-implement data augmentation techniques, we derived closed-form Gibbs sampling update equations by first backward and upward propagating auxiliary latent counts, and then forward and downward sampling latent variables. Moreover, we develop stochastic gradient MCMC inference that is scalable to very long multivariate count time series. Experiments on both synthetic and a variety of real-world data demonstrate that the proposed model not only has excellent predictive performance, but also provides highly interpretable multilayer latent structure to represent hierarchical and temporal information propagation.","Dandan Guo,National Laboratory of Radar Signal Processing, Xidian University;Bo Chen,Xidian University;Hao Zhang,Xidian University;Mingyuan Zhou*,University of Texas at Austin",gdd_xidian@126.com;bchen@mail.xidian.edu.cn;zhanghao_xidian@163.com;Mingyuan.Zhou@mccombs.utexas.edu,Probabilistic Methods/Latent Variable Models*; Applications/Matrix and Tensor Factorization; Applications/Time Series Analysis; Probabilistic,Poster
5123,Continuity vs. Injectivity in Dimensionality Reduction: a Quantitative Topology View,"In this paper, 
we investigate Dimensionality reduction (DR) maps in an information retrieval setting from a quantitative topology point of view.
In particular, 
we show that no DR maps can achieve perfect precision and perfect recall simultaneously. 
Thus a continuous DR map must have imperfect precision.
We further prove an upper bound on the precision of Lipschitz continuous DR maps.
While precision is a natural measure in an information retrieval setting, 
it does not measure `how' wrong the retrieved data is.
We therefore propose a new measure based on Wasserstein distance that comes with theoretical guarantee in terms of a lower bound.  
A key technical step in our proofs is a particular optimization problem of the $L_2$-Wasserstein distance over a constrained set of distributions.
We provide a complete solution to this optimization problem, 
which can be of independent interest on the technical side.
Lastly, simulation confirms the tightness of the bound.","Yik Chau Lui*,BorealisAI;Gavin Weiguang Ding,Borealis AI;Ruitong Huang,Borealis AI;Robert McCann,University of Toronto",kry.lui.work@gmail.com;gavin.w.ding@gmail.com;ruitong@ualberta.ca;mccann@math.toronto.edu,Theory/Learning Theory*; Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning,Poster
5128,Teaching Inverse Reinforcement Learners via Features and Demonstrations,"Inverse reinforcement learning (IRL) algorithms enable a learning agent to train a desired behaviour by first estimating the reward function optimized by demonstrations provided by an expert, and then find an optimal policy for that estimated reward function. The typical assumption is that the learning agent knows the features that the true reward function depends on. In this paper, we study IRL in the setting where this is not the case, i.e., where there is a mismatch between the world views of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk garantuee that the learner is able to find a near-optimal policy using variants of standard IRL algorithms. Based on these findings, we suggest a teaching scheme in which the expert can update the learner‚Äôs world view in order to decrease the teaching risk, and thus ultimately enable her to find a near-optimal policy.","Luis Haug*,ETH Zurich;Sebastian Tschiatschek,Microsoft Research;Adish Singla,MPI-SWS",lhaug@inf.ethz.ch;sebastian.tschiatschek@microsoft.com;adishs@mpi-sws.org,Reinforcement Learning and Planning/Planning*; Reinforcement Learning and Planning/Model-Based RL,Poster
5129,Wasserstein Distributionally Robust Kalman Filtering,"We study a distributionally robust mean square error estimation problem over a nonconvex Wasserstein ambiguity set containing only normal distributions. We show that the optimal estimator and the least favorable distribution form a Nash equilibrium. Despite the non-convex nature of the ambiguity set, we prove that the estimation problem is equivalent to a tractable convex program. We further devise a Frank-Wolfe algorithm for this convex program whose direction-searching subproblem can be solved in a quasi-closed form. Using these ingredients, we introduce a distributionally robust Kalman filter that hedges against model risk.","Soroosh Shafiee*,EPFL;Viet Anh Nguyen,EPFL;Daniel Kuhn,EPFL;Peyman Mohajerin Esfahani,Delft University of Technology",soroosh.shafiee@epfl.ch;viet-anh.nguyen@epfl.ch;daniel.kuhn@epfl.ch;P.MohajerinEsfahani@tudelft.nl,Optimization/Convex Optimization*; Reinforcement Learning and Planning/Decision and Control,Spotlight
5133,Generalisation of structural knowledge in the Hippocampal-Entorhinal system,"A central problem to understanding intelligence is the concept of generalisation. This allows previously learnt structure to be exploited to solve tasks in novel situations differing in their particularities. We take inspiration from neuroscience, specifically the Hippocampal-Entorhinal system (containing place and grid cells), known to be important for generalisation. We propose that to generalise structural knowledge, the representations of the structure of the world, i.e. how entities in the world relate to each other, need to be separated from representations of the entities themselves. We show, under these principles, artificial neural networks embedded with hierarchy and fast Hebbian memory, can learn the statistics of memories, generalise structural knowledge, and also exhibit neuronal representations mirroring those found in the brain. We experimentally support model assumptions, showing a preserved relationship between grid and place cells across environments.","James Whittington*,University of Oxford;Timothy Muller,University of Oxford;Shirely Mark,University College London;Caswell Barry,University College London;Tim Behrens,(organization)",james.whittington@magd.ox.ac.uk;timothymuller127@gmail.com;s.mark@ucl.ac.uk;caswell.barry@ucl.ac.uk;behrens@fmrib.ox.ac.uk,Neuroscience and Cognitive Science*; Algorithms/Multitask and Transfer Learning; Deep Learning/Few-Shot Learning Approaches; Deep L,Oral
5135,"Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization","  We suggest a general oracle-based framework that captures parallel
  stochastic optimization in different parallelization settings
  described by a dependency graph, and derive generic lower bounds 
  in terms of this graph.  We then use the framework and derive lower
  bounds to study several specific parallel optimization settings,
  including delayed updates and parallel processing with intermittent
  communication.  We highlight gaps between lower and upper bounds on
  the oracle complexity, and cases where the ``natural'' algorithms
  are not known to be optimal.","Blake Woodworth*,TTI-Chicago;Jialei Wang,U Chicago;Adam Smith,Boston University;Brendan McMahan,Google;Nathan Srebro,Toyota Technical Institute of Chicago",blake@ttic.edu;jialei@uchicago.edu;ads22@bu.edu;mcmahan@google.com;nati@ttic.edu,Optimization/Convex Optimization*; Algorithms/Stochastic Methods; Theory/Learning Theory,Spotlight
5137,Adversarial Regularizers in Inverse Problems,"Inverse Problems in medical imaging and computer vision are traditionally solved using purely model-based methods. Among those variational regularization models are one of the most popular approaches. We propose a new framework for applying data-driven approaches to inverse problems, using a neural network as a regularization functional. The network learns to discriminate between the distribution of ground truth images and the distribution of unregularized reconstructions. Once trained, the network is applied to the inverse problem by solving the corresponding variational problem. Unlike other data-based approaches for inverse problems, the algorithm can be applied even if only unsupervised training data is available. Experiments demonstrate the potential of the framework for denoising on the BSDS dataset and for computer tomography reconstruction on the LIDC dataset.","Sebastian Lunz*,University of Cambridge;Carola Schoenlieb,Cambridge University;Ozan √ñktem,Royal Institute of Technology",lunz@math.cam.ac.uk;cbs31@cam.ac.uk;ozan@kth.se,Applications/Denoising*; Deep Learning/Adversarial Networks,Poster
5138,Clustering Redemption‚ÄìBeyond the Impossibility of Kleinberg‚Äôs Axioms,"Kleinberg (2002) stated three axioms that any clustering procedure should satisfy and showed there is no clustering procedure that simultaneously satisfies all three. One of these, called the consistency axiom, requires that when the data is modified in a helpful way, i.e. if points in the same cluster are made more similar and those in different ones made less similar, the algorithm should output the same clustering. To circumvent this impossibility result, research has focused on considering clustering procedures that have a clustering quality measure (or a cost) and showing that a modification of Kleinberg‚Äôs axioms that takes cost into account lead to feasible clustering procedures. In this work, we take a different approach, based on the observation that the consistency axiom fails to be  satisfied when the ‚Äúcorrect‚Äù number of clusters changes. We modify this axiom by making use of cost functions to determine the correct number of clusters, and require that consistency holds only if the number of clusters remains unchanged. We show that single linkage satisfies the modified axioms, and if the input is well-clusterable, some popular procedures such as k-means also satisfy the axioms, taking a step towards explaining the success of these objective functions for guiding the design of algorithms.","Vincent Cohen-Addad,Paris Sorbonne;Varun Kanade*,Oxford;Frederik Mallmann-Trenn,MIT",vcohen@di.ens.fr;varunk@cs.ox.ac.uk;mallmann@mit.edu,Algorithms/Clustering*; Theory/Learning Theory,Poster
5143,Co-teaching: Robust Training Deep Neural Networks with Extremely Noisy Labels,"It is challenging to train deep neural networks robustly with noisy labels,
as the capacity of deep neural networks is so high that they can totally over-fit on these noisy labels. In this paper, motivated by the memorization effects of deep networks, which shows networks fit clean instances first and then noisy ones, we present a new paradigm called ``\textit{Co-teaching}'' combating with noisy labels. We train two networks simultaneously. First, in each mini-batch data, each network filters noisy instances based on memorization effects. Then, it teaches the remained instances to its peer network for updating the parameters. Empirical results on benchmark datasets demonstrate that, the robustness of deep learning models trained by Co-teaching approach is much superior than that of state-of-the-art methods.","Bo Han*,RIKEN / University of Technology Sydney;Quanming Yao,4Paradigm;Xingrui Yu,University of Technology Sydney;Gang Niu,RIKEN;Miao Xu,RIKEN;Weihua Hu,The University of Tokyo/RIKEN;Ivor Tsang,University of Technology Sydney;Masashi Sugiyama,RIKEN/The University of Tokyo",bo.han@riken.jp;qyaoaa@connect.ust.hk;xingruiyu_upc@hotmail.com;gang.niu@riken.jp;miao.xu@riken.jp;hu@ms.k.u-tokyo.ac.jp;ivor.tsang@uts.edu.au;sugi@k.u-tokyo.ac.jp,Deep Learning*; Algorithms/Semi-Supervised Learning,Poster
5144,An Event-Based Framework for Task Specification and Control,"The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose inverse event-based control, which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.","Justin Fu,UC Berkeley;Avi Singh*,UC Berkeley;Dibya Ghosh,UC Berkeley;Larry Yang,UC Berkeley;Sergey Levine,UC Berkeley",justinfu@berkeley.edu;avisingh@cs.berkeley.edu;dibya.ghosh@berkeley.edu;larrywyang@berkeley.edu;svlevine@eecs.berkeley.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Deep Learning,Poster
5148,A convex program for bilinear inversion of sparse vectors,"We consider the bilinear inverse problem of recovering two vectors,  x in R^L and w in R^L, from their entrywise product. We consider the case where x and w have known signs and are sparse with respect to known dictionaries of size K and N, respectively.  Here,  K and N may be larger than, smaller than, or equal to L.  We introduce L1-BranchHull, which is a convex program posed in the natural parameter space and does not require an approximate solution or initialization in order to be stated or solved. We study the case where x and w are S1- and S2-sparse with respect to a random dictionary and present a recovery guarantee that only depends on the number of measurements as L > Omega(S1+S2)(log(K+N))^2. Numerical experiments verify that the scaling constant in the theorem is not too large.  One application of this problem is the sweep distortion removal task in dielectric imaging, where one of the signals is a nonnegative reflectivity, and the other signal lives in a known subspace, for example that given by dominant wavelet coefficients. We also introduce a variants of L1-BranchHull for the purposes of tolerating noise and outliers, and for the purpose of recovering piecewise constant signals.  We provide an ADMM implementation of these variants and show they can extract piecewise constant behavior from real images.","Alireza Aghasi,Georgia State University;Ali Ahmed,Information Technology University;Paul Hand,Northeastern University;Babhru Joshi*,Rice University",aaghasi@gsu.edu;ali.ahmed@itu.edu.pk;p.hand@northeastern.edu;bj12@rice.edu,Algorithms/Sparsity and Compressed Sensing*; Applications/Information Retrieval; Applications/Source Separation; Optimization/Convex Opt,Poster
5151,Adversarial Multiple Source Domain Adaptation,"While domain adaptation has been actively researched, most algorithms focus on the single-source-single-target adaptation setting. In this paper we propose new generalization bounds and algorithms under both classification and regression settings for unsupervised multiple source domain adaptation. Our theoretical analysis naturally leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose multisource domain adversarial networks (MDAN) that approach domain adaptation by optimizing task-adaptive generalization bounds. To demonstrate the effectiveness of MDAN, we conduct extensive experiments showing superior adaptation performance on both classification and regression problems: sentiment analysis, digit classification, and vehicle counting. ","Han Zhao*,Carnegie Mellon University;Shanghang Zhang,Carnegie Mellon University;Guanhang Wu,Carnegie Mellon University; Jos√© M. F. Moura,Carnegie Mellon University;Joao Paulo Costeira,Instituto Superior Tecnico; Geoffrey Gordon,(Carnegie Mellon University)",han.zhao@cs.cmu.edu;shanghaz@andrew.cmu.edu;guanhanw@andrew.cmu.edu;moura@andrew.cmu.edu;jpc@isr.ist.utl.pt;ggordon@cs.cmu.edu,Deep Learning*; Applications/Computer Vision; Deep Learning/Adversarial Networks,Poster
5153,Neural Tangent Kernel: Convergence and Generalization in Neural Networks,"At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.

We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping.

Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.","Arthur Jacot*,EPFL;Clement Hongler,EPFL;Franck Gabriel,EPFL & Imperial College London",arthur@netopera.net;clement.hongler@epfl.ch;franckr.gabriel@gmail.com,Deep Learning/Optimization for Deep Networks*; Algorithms/Kernel Methods; Deep Learning; Optimization; Optimization/Non-Convex Optimization;,Spotlight
5172,A statistical model for graph partitioning with high-dimensional covariates,"We provide the first information theoretical tight analysis for inference of latent community structure given a sparse graph along with high dimensional node covariates, correlated with the same latent communities. Our work bridges recent theoretical breakthroughs in detection of latent community structure without nodes covariates and a large body of empirical work using diverse heuristics for combining node covariates with graphs for inference. The tightness of our analysis implies in particular, the information theoretical necessity of combining the different sources of information. 
Our analysis holds for networks of large degrees as well as for a Gaussian version of the model. ","Yash Deshpande*,Massachusetts Institute of Technology;Subhabrata Sen,Microsoft Research;Andrea Montanari,Stanford University;Elchanan Mossel,MIT",yash@mit.edu;ssen90@mit.edu;montanari@stanford.edu;elmos@mit.edu,Theory/Information Theory*; Probabilistic Methods/Belief Propagation; Probabilistic Methods/Graphical Models; Theory/Fr,Spotlight
5189,A Likelihood-Free Inference Framework for Population Genetic Data using Exchangeable Neural Networks,"An explosion of high-throughput DNA sequencing in the past decade has led to a surge of interest in population-scale inference with whole-genome data. Recent work in population genetics has centered on designing inference methods for relatively simple model classes, but few scalable general-purpose inference techniques exist for more realistic, complex models. To achieve this, two inferential challenges need to be addressed: (1) population data are exchangeable, calling for methods that efficiently exploit the symmetries of the data, and (2) computing likelihoods is intractable as it requires integrating over a set of correlated, extremely high-dimensional latent variables. These challenges are traditionally tackled by likelihood-free methods that use scientific simulators to generate datasets and reduce them to hand-designed, permutation-invariant summary statistics, often leading to inaccurate inference. In this work, we develop an exchangeable neural network that performs summary statistic-free, likelihood-free inference. Our framework can be applied in a black-box fashion  across a variety of simulation-based tasks, both within and outside biology. We demonstrate the power of our approach on the recombination hotspot testing problem, outperforming the state-of-the-art.","Jeffrey Chan*,UC Berkeley;Valerio Perrone,University of Warwick;Jeffrey Spence,UC Berkeley;Paul Jenkins,University of Warwick;Sara Mathieson,Swarthmore College;Yun Song,UC Berkeley",chanjed@berkeley.edu;v.perrone@warwick.ac.uk;spence.jeffrey@berkeley.edu;p.jenkins@warwick.ac.uk;smathie1@swarthmore.edu;yss@berkeley.edu,Applications/Computational Biology and Bioinformatics,Spotlight
5214,"Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects","We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for image sequences.
It can reliably discover and track objects through the sequence; it can also conditionally generate future frames, thereby simulating expected motion of objects. 
This is achieved by explicitly encoding object numbers, locations and appearances in the latent variables of the model.
SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al. 2016), including unsupervised learning, made possible by inductive biases present in the model structure.
We use a moving multi-\textsc{mnist} dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how \textsc{sqair} overcomes them by leveraging temporal consistency of objects.
Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.","Adam Kosiorek*,Oxford;Hyunjik Kim,;Yee Whye Teh,University of Oxford;Ingmar Posner,Oxford",adamk@robots.ox.ac.uk;hkim@stats.ox.ac.uk;y.w.teh@stats.ox.ac.uk;ingmar@robots.ox.ac.uk,Deep Learning/Generative Models*; Algorithms/Representation Learning; Algorithms/Unsupervised Learning,Spotlight
5217,Randomized Prior Functions for Deep Reinforcement Learning,"Dealing with uncertainty is essential for efficient reinforcement learning.
There is a growing literature on uncertainty estimation for deep learning from fixed datasets, but many of the most popular approaches are poorly-suited to sequential decision problems.
Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data.
We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable `prior' network to each ensemble member.
We prove that this approach is efficient with linear representations, provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.","Ian Osband*,Google Deepmind;John Aslanides,DeepMind;Albin Cassirer,DeepMind",iosband@google.com;jaslanides@google.com;cassirer@google.com,Reinforcement Learning and Planning/Exploration*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Spotlight
5220,Compact Representation of Uncertainty In Clustering,"For many classic structured prediction problems, probability distributions over the dependent variables can be efficiently computed using widely-known algorithms and data structures (such as forward-backward, and its corresponding trellis for exact probability distributions in Markov models). However, we know of no previous work studying efficient representations of exact distributions over clusterings.  This paper presents definitions and proofs for a dynamic-programming inference procedure that computes the partition function, the marginal probability of a cluster, and the MAP clustering---all exactly.  Rather than the $N^{th}$ Bell number, these exact solutions take time and space proportional to the substantially smaller powerset of $N$.  Indeed, we reduce the best known time complexity for this problem by a factor of $N$.  While still large, this previously unknown result is intellectually interesting in its own right, makes feasible exact inference for important real-world small data applications (such as medicine), and provides a natural stepping stone towards sparse-trellis approximations that enable further scalability (which we also explore). In experiments, we demonstrate the superiority of our approach over approximate methods in analyzing real-world gene expression data used in cancer treatment.","Craig Greenberg*,University of Massachusetts Amherst;Nicholas Monath,University of Massachusetts Amherst;Ari Kobren,University of Massachusetts Amherst;Patrick  Flaherty,UMASS;Andrew McGregor,University of Massachusetts Amherst;Andrew McCallum,Univ of Massachusetts Amherst",csgreenberg@cs.umass.edu;nmonath@cs.umass.edu;akobren@cs.umass.edu;flaherty@math.umass.edu;mcgregor@cs.umass.edu;mccallum@cs.umass.edu,Algorithms/Clustering,Poster
5221,Minimax Rates in Contextual Partial Monitoring," We generalize the finite partial monitoring problem to the contextual setting. Partial monitoring allows learning even when the loss of the chosen action is not observed. In the non-contextual problem, the minimax regret is known to be O(T^{2/3}) if a global observability condition is satisfied and improves to O(\sqrt{T}) under a stronger local observability condition. Perhaps surprisingly, we show that the same characterization does not hold in the contextual case and a stronger notion of pairwise observability is necessary for O(\sqrt{T}) minimax regret. In particular, we provide a lower bound of O(T^{2/3}) for any non-pairwise observable game, which applies to locally observable games. We also propose two algorithms in the adversarial setting. The first requires a finite policy class but allows for arbitrary contexts and can be tuned to obtain the optimal O(\sqrt{T}) rate in pairwise observable settings or the optimal O(T^{2/3}) rate otherwise. The second allows for arbitrary policy classes with an empirical risk minimization oracle but requires i.i.d. contexts; we also show an optimal O(T^{2/3}) upper bound and an efficient implementation using only a constant number of oracle calls per round.","Alan Malek,MIT;Dylan Foster*,Cornell University;Ali Jadbabaie,;Alexander Rakhlin,MIT",alan.malek@gmail.com;djf244@cornell.edu;jadbabai@mit.edu;rakhlin@mit.edu,Algorithms/Online Learning*; Theory/Learning Theory,Poster
5227,Learning without Phase: Regularized PhaseMax Achieves Optimal Sample Complexity,"The problem of estimating an unknown signal, $\x_0\in \RR^n$, from a vector $\y\in \RR^m$ consisting of $m$ magnitude-only measurements of the form $y_i=|\mathbf a_i\x_0|$, where  $\mathbf a_i$'s are the rows of a known measurement matrix $\mathbf A$ is a classical problem known as phase retrieval. This problem arises when measuring the phase is costly or altogether infeasible. In many applications in machine learning, signal processing, statistics, etc., the underlying signal has certain structure (sparse, low-rank, finite alphabet, etc.), opening of up the possibility of recovering $\x_0$ from a number of measurements smaller than the ambient dimension, i.e., $m<n$. Ideally, one would like to recover the signal from a number of phaseless measurements that is on the order of the ""degrees of freedom"" of the structured $\x_0$. To this end, inspired by the PhaseMax algorithm, we formulate a convex optimization problem, where the objective function relies on an initial estimate of the true signal and also includes an additive regularization term to encourage structure. The new formulation is referred to as {\textbf{regularized PhaseMax}}. We analyze the performance of regularized PhaseMax to find the minimum number of phaseless measurements required for perfect signal recovery. The results are asymptotic and are in terms of the geometrical properties (such as the Gaussian width) of certain convex cones. When the measurement matrix has i.i.d. Gaussian entries, we show that our proposed method is indeed order-wise optimal, allowing perfect recovery from a number of phaseless measurements that is only a constant factor away from the degrees of freedom. We explicitly compute this constant factor, in terms of the quality of the initial estimate, by deriving the exact phase transition. The theory well matches empirical results from numerical simulations.","Fariborz Salehi*,California Institute of Technology;Ehsan Abbasi,California Institute of Technology;Babak Hassibi,Caltech",fsalehi@caltech.edu;eabbasi@caltech.edu;hassibi@caltech.edu,Algorithms/Sparsity and Compressed Sensing*; Algorithms/Structured Prediction; Applications/Information Retrieval; Applications/Signal P,Poster
5230,Multilingual Anchoring: Interactive Topic Modeling and Alignment Across Languages,"Multilingual topic models can reveal patterns in cross-lingual
document collections.  However, existing models lack speed and
interactivity, which prevents adoption in quick moving situations
(e.g. natural disasters, political instability).  We extend fast,
interactive topic modeling algorithms for monolingual text to
multilingual corpora.  First, we propose a multilingual anchoring
algorithm that builds an anchor-based topic model for comparable
corpora.  Then, we incorporate interactivity to develop MTAnchor (Multilingual Topic Anchors), a system that allows users to refine the topic model.  We
test our system on labeled English and Chinese documents to evaluate classification accuracy and topic coherence.  Through user study experiments, we show that users can find topics of interest and correct errors.","Michelle Yuan*,University of Maryland, College Park;Benjamin Van Durme ,Johns Hopkins University;Jordan Boyd-Graber,University of Maryland",myuan@cs.umd.edu;vandurme@jhu.edu;jbg@umiacs.umd.edu,"Probabilistic Methods/Topic Models*; Algorithms/Spectral Methods; Applications/Fairness, Accountability, and Transparency; Applicat",Poster
5238,Estimators for Multivariate Information Measures in General Probability Spaces,"Information theoretic quantities play an important role in various settings in machine learning, including causality testing, structure inference in graphical models, time-series problems, feature selection as well as in providing privacy guarantees. A key quantity of interest is the mutual information and generalizations thereof, including conditional mutual information, multivariate mutual information, total correlation and directed information. While the aforementioned information quantities are well defined in arbitrary probability spaces, existing estimators employ a $\Sigma H$ method, which can only work in purely discrete space or purely continuous case since entropy (or differential entropy) is well defined only in that regime.
In this paper, we define a general graph divergence measure ($\mathbb{GDM}$), generalizing the aforementioned information measures and we construct a novel estimator via a coupling trick that directly estimates these multivariate information measures using the Radon-Nikodym derivative. These estimators are proven to be consistent in a general setting which includes several cases where the existing estimators fail, thus providing the only known estimators for the following settings: (1) the data has some discrete and some continuous valued components (2) some (or all) of the components themselves are discrete-continuous \textit{mixtures} (3) the data is real-valued but does not have a joint density on the entire space, rather is supported on a low-dimensional manifold. We show that our proposed estimators significantly outperform known estimators on synthetic and real datasets. ","Arman Rahimzamani*,University of Washington;Himanshu Asnani,University of Washington, Seattle;Pramod Viswanath,UIUC;Sreeram Kannan,University of Washington",armanrz@uw.edu;asnani@uw.edu;pramodv@illinois.edu;ksreeram@uw.edu,Theory/Information Theory*; Probabilistic Methods/Graphical Models,Poster
5240,DeepPINK: reproducible feature selection in deep neural networks,"Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.","Yang Lu*,University of Washington;Yingying Fan,USC;Jinchi Lv,University of Southern California;William Noble,University of Washington",ylu465@uw.edu;fanyingy@marshall.usc.edu;jinchilv@marshall.usc.edu;wnoble@uw.edu,Deep Learning/Biologically Plausible Deep Networks*; Algorithms/Classification; Algorithms/Regression; Algorithms/Sparsity and Compressed Sensin,Poster
5246,Synthesis of Differentiable Functional Programs for Lifelong Learning,"We present a neurosymbolic approach to the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a combination of gradient-based learning and symbolic program synthesis can be a more effective response to these challenges than purely neural methods. Our approach, called Houdini, represents neural networks as strongly typed, end-to-end differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. Our experiments show that Houdini transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks.","Lazar Valkov,University of Edinburgh;Dipak Chaudhari,Rice University;Akash Srivastava,University of Edinburgh;Charles Sutton,University of Edinburgh;Swarat Chaudhuri*,Rice University	",L.Valkov@sms.ed.ac.uk;dipakc@gmail.com;Akash.Srivastava@ed.ac.uk;csutton@inf.ed.ac.uk;swarat@rice.edu,Deep Learning/Program Induction*; Algorithms/Multitask and Transfer Learning,Poster
5247,Searching for Efficient Multi-Scale Architectures for Dense Image Prediction,"The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7% on Cityscapes (street scene parsing), 71.3% on PASCAL-Person-Part (person-part segmentation), and 87.9% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.","Liang-Chieh Chen*,Google Inc.;Maxwell Collins,Google Inc.;Yukun Zhu,Google Inc.;George Papandreou,Google;Barret Zoph,Google;Florian Schroff,Google Inc.;Hartwig Adam,Google;Jon Shlens,Google",lcchen@cs.ucla.edu;maxwellcollins@google.com;yukun@google.com;gpapan@google.com;barretzoph@google.com;fschroff@google.com;hadam@google.com;shlens@google.com,Applications/Image Segmentation*; Algorithms/AutoML; Applications/Computer Vision; Deep Learning/CNN Architectures,Poster
5263,Orthogonally Decoupled Variational Gaussian Processes,"Gaussian processes provide a powerful non-parametric framework for reasoning over functions. Despite appealing theories, its superlinear computational and memory complexities have presented a long-standing challenge. The state-of-the-art methods of sparse variational inference trade modeling accuracy with complexity. However, their complexities still scale superlinearly in the number of basis functions, so they can learn from large datasets only when a small model is used. Recently, a decoupled approach was proposed to remove the unnecessary coupling between the complexities of modeling the mean and the covariance functions. It achieves a linear complexity in the number of mean parameters, so an expressive posterior mean function can be modeled. While promising, this approach suffers from optimization difficulties due to ill-conditioning and non-convexity. In this work, we propose an alternative decoupled parametrization. It adopts an orthogonal basis in the mean function to model the residues that cannot be learned by the standard coupled approach. Therefore, our method extends, rather than replaces, the coupled approach to achieve strictly better performance. This construction admits a straightforward natural gradient update rule, so the structure of the information manifold that is lost during decoupling can be leveraged to speed up learning. Empirically, our algorithm demonstrates significantly faster convergence in multiple experiments.","Hugh Salimbeni*,Imperial College;Ching-An Cheng,Georgia Institute of Technology;Byron Boots,Georgia Institute of Technology;Marc Deisenroth,Imperial College London",hrs13@ic.ac.uk;cacheng@gatech.edu;bboots@cc.gatech.edu;m.deisenroth@imperial.ac.uk,Probabilistic Methods/Gaussian Processes*; Probabilistic Methods/Variational Inference,Poster
5269,Dendritic cortical microcircuits approximate the backpropagation algorithm,"Deep learning has seen remarkable developments over the last years.  Many of these developments have been inspired by neuroscience. However, the learning algorithm behind these developments -- backprop -- appears to be at odds with neurobiology. Here, we introduce a multi-layer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous models our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.","Joao Sacramento*,University of Bern;Rui Ponte Costa,Univeristy of Bern;Yoshua Bengio,Universite de Montreal;Walter Senn,University of Bern",sacramento@pyl.unibe.ch;costa@pyl.unibe.ch;yoshua.umontreal@gmail.com;senn@pyl.unibe.ch,Neuroscience and Cognitive Science/Plasticity and Adaptation*; Deep Learning/Biologically Plausible Deep Networks; Neuroscience and Cognitive Science/Neurosc,Oral
5271,Learning Plannable Representations with Causal InfoGAN,"In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans -- a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.","Thanard Kurutach,UC Berkeley;Aviv Tamar*,UC Berkeley;Ge Yang,University of Chicago;Stuart Russell,UC Berkeley;Pieter Abbeel,UC Berkeley",thanard.kurutach@berkeley.edu;avivt@berkeley.edu;yangge1987@gmail.com;russell@berkeley.edu;pabbeel@cs.berkeley.edu,Reinforcement Learning and Planning*; Algorithms/Representation Learning; Deep Learning/Embedding Approaches; Deep Learning/Gener,Poster
5274,Uniform Convergence of Gradients for Non-Convex Learning and Optimization,"We introduce vector-valued Rademacher complexities as a user-friendly tool to bound the rate at which refined properties of the empirical risk such as gradients and Hessians converge to their population counterparts in non-convex settings. Our tools are simple, composable, and allow one derive dimension-free uniform convergence bounds for gradients and hessians in a diverse range of non-convex optimization under a robust set of assumptions. As an application of our techniques, we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression models, showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity both in high (possibly infinite)- and low-dimensional regimes. This analysis applies under weaker distributional assumptions than in past works and applies even when multiple passes over the dataset are allowed.

Moving beyond smooth models we show----in contrast to the smooth case---even for simple models such as a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side, we show that it is still possible to obtain dimension-independent rates for this and other non-smooth models under a new type of distributional assumption.","Dylan Foster*,Cornell University;Ayush Sekhari,CORNELL UNIVERSITY;Karthik Sridharan,Cornell University",djf244@cornell.edu;ayush.sekhari@gmail.com;sridharan@cs.cornell.edu,Theory/Learning Theory*; Optimization/Non-Convex Optimization,Poster
5277,Automatic differentiation in ML: Where we are and where we should be going,"We review the current state of automatic differentiation (AD) for array programming in machine learning (ML), including the different approaches such as operator overloading (OO) and source transformation (ST) used for AD, graph-based intermediate representations for programs, and source languages. Based on these insights, we introduce a new graph-based intermediate representation (IR) closely related to A-normal form (ANF) which is specifically aimed at supporting fully-general AD for array programming efficiently. Unlike existing dataflow programming representations in ML frameworks, our intermediate representation (IR) naturally supports function calls, higher-order functions, recursion, etc. making ML models easier to implement. The ability to represent closures allows us to perform AD using ST without a tape, making the resulting derivative (adjoint) program amenable to ahead-of-time optimization using tools from functional language compilers, and enabling higher-order derivatives. Lastly, we introduce a proof of concept compiler toolchain called Myia which uses a subset of Python as a front end.","Bart van Merrienboer*,MILA, Google;Olivier Breuleux,MILA;Arnaud Bergeron,MILA;Pascal Lamblin,MILA, Universit√© de Montr√©al",bart.vanmerrienboer@gmail.com;breuleux@gmail.com;abergeron@gmail.com;lamblinp@iro.umontreal.ca,"Data, Competitions, Implementations, and Software/Software Toolkits*; Applications/Hardware and Systems",Spotlight
5284,A Bayesian Nonparametric View on Count-Min Sketch,"The count-min sketch is a time- and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream.  The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n-grams.  We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation.  In particular, we take a nonparametric approach and consider tokens generated from a from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens.  Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees.  Using simulated data with known ground truth, we investigate the properties of these estimators.  Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process.  This variant shares many of the convenient properties of the Dirichlet process construction, but also allows for power law scaling behavior in the number of unique tokens.","Diana Cai*,Princeton University;Michael Mitzenmacher,Harvard;Ryan Adams,Princeton University",dcai@cs.princeton.edu;michaelm@eecs.harvard.edu;rpa@princeton.edu,Probabilistic Methods/Bayesian Nonparametrics,Poster
5289,Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels,"Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and large-scale datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels. ","ZHILU ZHANG*,Cornell University;Mert Sabuncu,Cornell",zz452@cornell.edu;msabuncu@cornell.edu,Algorithms/Classification*; Deep Learning/Optimization for Deep Networks,Spotlight
5291,"Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs","The loss functions of deep neural networks are complex and their geometric properties are not well understood.  We show that the optima of these complex loss functions are in fact connected by simple curves, over which training and test accuracy are nearly constant.  We introduce a training procedure to discover these high-accuracy pathways between modes.  Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model.  We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on  CIFAR-10, CIFAR-100, and ImageNet.","Timur Garipov*,Lomonosov Moscow State University;Pavel Izmailov,Cornell University;Dmitrii Podoprikhin,Lomonosov MSU;Dmitry Vetrov,Higher School of Economics;Andrew Gordon Wilson,Cornell University",timgaripov@gmail.com;izmailovpavel@gmail.com;podoprikhindmitry@gmail.com;vetrovd@yandex.ru;andrew@cornell.edu,Algorithms/Boosting and Ensemble Methods*; Applications/Computer Vision; Deep Learning/Supervised Deep Networks; Deep Learning/Visuali,Spotlight
5296,A flexible neural representation for physics prediction,"Humans have a remarkable capacity to understand the physical dynamics of objects in their environment, flexibly capturing complex structures and interactions at multiple levels of detail.  
Inspired by this ability, we propose a hierarchical particle-based object representation that covers a wide variety of types of three-dimensional objects, including both arbitrary rigid geometrical shapes and deformable materials.  
We then describe the Hierarchical Relation Network (HRN), an end-to-end differentiable neural network based on hierarchical graph convolution, that learns to predict physical dynamics in this representation. 
Compared to other neural network baselines, the HRN accurately handles complex collisions and nonrigid deformations, generating plausible dynamics predictions at long time scales in novel settings, and scaling to large scene configurations.
These results demonstrate an architecture with the potential to form the basis of next-generation physics predictors for use in computer vision, robotics, and quantitative cognitive science.","Damian Mrowca*,Stanford University;Chengxu Zhuang,Stanford University;Elias Wang,Stanford University;Nick Haber,Stanford University;Li Fei-Fei,Stanford University;Joshua Tenenbaum,MIT;Daniel Yamins,Stanford University",mrowca@stanford.edu;chengxuz@stanford.edu;eliwang@stanford.edu;nhaber@stanford.edu;feifeili@cs.stanford.edu;jbt@mit.edu;yamins@stanford.edu,Deep Learning/Interaction-Based Deep Networks*; Deep Learning/Predictive Models; Neuroscience and Cognitive Science/Cognitive Science; Neurosc,Poster
5299,Legendre Decomposition for Tensors,"We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry, the reconstructed tensor is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than nonnegative CP and Tucker decomposition.","Mahito Sugiyama*,National Institute of Informatics;Hiroyuki Nakahara,(organization);Koji Tsuda,The University of Tokyo",mahito@nii.ac.jp;hiro@brain.riken.jp;tsuda@k.u-tokyo.ac.jp,Applications/Matrix and Tensor Factorization*; Theory/Information Theory,Spotlight
5309,Reinforcement Learning of Theorem Proving,"We introduce a theorem proving algorithm that uses practically no domain heuristics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40% more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.","Cezary Kaliszyk,Innsbruck University;Josef Urban*,Czech Technical University in Prague;Henryk Michalewski,University of Warsaw;Mirek Ol≈°√°k,Charles University in Prague",cezary.kaliszyk@uibk.ac.at;Josef.Urban@gmail.com;H.Michalewski@mimuw.edu.pl;mirek@olsak.net,Reinforcement Learning and Planning*; Neuroscience and Cognitive Science/Problem Solving; Neuroscience and Cognitive Science/Reasoni,Poster
5310,Data Amplification: A Unified and Competitive Approach to Property Estimation,"Estimating properties of discrete distributions is a fundamental problem in statistical learning. We design the first unified, linear-time, competitive, property estimator that for a wide class of properties and for all underlying distributions uses just $2n$ samples to achieve the performance attained by the empirical estimator with $n\sqrt{\log n}$ samples. This provides off-the-shelf, distribution-independent,  ``amplification'' of the amount of data available relative to common-practice estimators. 

We illustrate the estimator's practical advantages by comparing it to existing estimators for a wide variety of properties and distributions. In most cases, its performance with $n$ samples is even as good as that of the empirical estimator with $n\log n$ samples, and for essentially all properties, its performance is comparable to that of the best existing estimator designed specifically for that property.","Yi HAO*,University of California, San Diego;Alon Orlitsky,UC San Diego;Ananda Suresh,Google;Yihong Wu,Yale University",yih179@eng.ucsd.edu;alon@ucsd.edu;theertha@google.com;yihong.wu@yale.edu,Theory*; Theory/Information Theory,Poster
5313,Group Equivariant Capsule Networks,"We present group equivariant capsule networks, a framework to introduce guaranteed equivariance and invariance properties to the capsule network idea. We restrict pose vectors and learned transformations to be elements of a group, which allows us to prove equivariance of pose vectors and invariance of activations under application of the group law. Requirements are a modified spatial aggregation method for capsules and a generic routing by agreement algorithm with abstract rules, which we both present in this work. Further, we connect our equivariant capsule networks with work from the field of group convolutional networks, which consist of convolutions that are equivariant under applications of the group law. Through this connection, we are able to provide intuitions of how both methods relate and are able to combine both approaches in one deep neural network architecture, combining the strengths from both fields. The resulting framework allows sparse evaluation of feature maps defined over groups, provides control over specific equivariance and invariance properties and uses routing by agreement instead of pooling operations. It provides interpretable and equivariant representation vectors as output capsules, which disentangle evidence of object existence from its pose.","Jan Lenssen*,TU Dortmund;Matthias Fey,TU Dortmund;Pascal Libuschewski,TU Dortmund",janeric.lenssen@udo.edu;matthias.fey@udo.edu;pascal.libuschewski@udo.edu,Algorithms/Representation Learning*; Applications/Computer Vision; Deep Learning/Supervised Deep Networks,Poster
5317,Stein Variational Gradient Descent as Moment Matching,"Stein variational gradient descent (SVGD) is a non-parametric inference algorithm that evolves a set of particles to fit a given distribution of interest. We analyze the non-asymptotic properties of SVGD, showing that there exists a set of functions, which we call the Stein matching set,  whose expectations are exactly estimated by any set of particles that satisfies the fixed point equation of SVGD. This set is the image of Stein operator applied on the feature maps of the positive definite kernel used in SVGD. Our results provide a theoretical framework for analyzing properties of SVGD with different kernels, shedding insights on optimal kernel choice. In particular, we show that SVGD with linear kernels yield exact estimation of means and variances on Gaussian distributions,  while random Fourier features enable probabilistic bounds for distributional approximation. Our results offer a refreshing view of the classical inference problem as fitting Stein's identity or solving Stein equation, which may motivate more efficient algorithms.","Qiang Liu*,UT Austin;Dilin Wang,UT Austin",lqiang@cs.utexas.edu;dilin@cs.utexas.edu,Probabilistic Methods/Graphical Models,Poster
5326,Differential Privacy for Growing Databases,"The large majority of differentially private algorithms focus on the static setting, where queries are made on an unchanging database. This is unsuitable for the myriad applications involving databases that grow over time. To address this gap in the literature, we consider the dynamic setting, in which new data arrive over time. Previous results in this setting have been limited to answering a single non-adaptive query repeatedly as the database grows. In contrast, we provide tools for richer and more adaptive analysis of growing databases. Our first contribution is a novel modification of the private multiplicative weights algorithm, which provides accurate analysis of exponentially many adaptive linear queries (an expressive query class including all counting queries) for a static database. Our modification maintains the accuracy guarantee of the static setting even as the database grows without bound. Our second contribution is a set of general results which show that many other private and accurate algorithms can be immediately extended to the dynamic setting by rerunning them at appropriate points of data growth with minimal loss of accuracy, even when data growth is unbounded.","Rachel Cummings*,Georgia Tech;Sara Krehiel,University of Richmond;Kevin Lai,Georgia Tech;Uthaipon Tantipongpipat,Georgia Tech",rachelc@gatech.edu;krehbiel@richmond.edu;kevinlai@gatech.edu;tao@gatech.edu,"Applications/Privacy, Anonymity, and Security*; Algorithms/Adaptive Data Analysis",Poster
5327,Exploration in Structured Reinforcement Learning,"We address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration of suboptimal (state, action) pairs. For any arbitrary structure, we derive problem-specific regret lower bounds satisfied by any learning algorithm. These lower bounds are made explicit for unstructured MDPs and for those whose transition probabilities and average reward function are Lipschitz continuous w.r.t. the state and action. For Lipschitz MDPs, the bounds are shown not to scale with the sizes S and A of the state and action spaces, i.e., they are smaller than c log T where T is the time horizon and the constant c only depends on the Lipschitz structure, the span of the bias function, and the minimal action sub-optimality gap. This contrasts with unstructured MDPs where the regret lower bound typically scales as SA logT. We deviseDEL (DirectedExploration Learning), an algorithm that matches our regret lower bounds. We further simplify the algorithm for Lipschitz MDPs, and show that the simplified version is still able to efficiently exploit the structure.","Jungseul Ok*,UIUC;Damianos Tranos,KTH;Alexandre Proutiere,KTH Royal Institute of Technology",ockjs1@gmail.com;tranos@kth.se;alepro@kth.se,Reinforcement Learning and Planning*; Algorithms/Bandit Algorithms; Algorithms/Online Learning; Reinforcement Learning and Planning,Oral
5332,Statistical Recurrent Models on Manifold valued Data,"In a number of disciplines, the data (e.g., graphs, manifolds) to be analyzed are non-Euclidean in nature.
  Geometric deep learning corresponds to techniques that generalize deep neural network models to such non-Euclidean spaces. Several
  recent papers have shown how convolutional neural networks (CNNs) can be extended to learn with graph-based data.
  In this work, we study the setting where the data (or measurements) are ordered, longitudinal or temporal in nature and live on a Riemannian manifold -- this setting is common in a variety of problems in statistical machine learning, vision, and medical imaging. We show how recurrent statistical recurrent network models can be defined in such spaces. We give an efficient algorithm and conduct a rigorous analysis of its statistical properties. We perform extensive numerical experiments showing competitive performance with state of the art methods but with far fewer parameters. We also show applications to a statistical analysis task in brain imaging, a regime where deep neural network models have only been utilized in limited ways.","Rudrasis Chakraborty*,University of Florida;Chun-Hao Yang,University of Florida;Xingjian Zhen,University of Wisconsin-Madison;Monami Banerjee,University of Florida;Derek Archer,University of Florida;David Vaillancourt,University of Florida;Vikas Singh,University of Wisconsin-Madison USA;Baba Vemuri,University of Florida, USA",rudrasischa@gmail.com;chunhaoyang@ufl.edu;xzhen3@wisc.edu;monamie.b@gmail.com;archerdb@ad.ufl.edu;vcourt@ufl.edu;vsingh@biostat.wisc.edu;vemuri@cise.ufl.edu,Applications/Computer Vision*; Applications/Activity and Event Recognition; Deep Learning/Recurrent Networks; Neuroscience an,Poster
5335,Balanced Policy Evaluation and Learning,"We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success. ","Nathan Kallus*,Cornell Tech",kallus@cornell.edu,Probabilistic Methods/Causal Inference*; Algorithms/Kernel Methods; Algorithms/Similarity and Distance Learning; Theory/Learning The,Poster
5337,Distributed Multitask Reinforcement Learning with Quadratic Convergence,"Multitask reinforcement learning (MTRL) suffers from scalability issues when the number of tasks or trajectories grows large. The main reason behind this drawback is the reliance on centeralised solutions. Recent methods exploited the connection between MTRL and general consensus to propose scalable solutions. These methods, however, suffer from two drawbacks. First, they rely on predefined objectives, and, second, exhibit linear convergence guarantees. In this paper, we improve over state-of-the-art by deriving multitask reinforcement learning from a variational inference perspective. We then propose a novel distributed solver for MTRL with quadratic convergence guarantees.","Rasul Tutunov*,PROWLER.io;Dongho Kim,Prowler.io;Haitham Bou Anmar,PROWLER.io",rasul@prowler.io;dongho@prowler.io;haitham@prowler.io,Reinforcement Learning and Planning/Reinforcement Learning*; Algorithms/Multitask and Transfer Learning,Poster
5345,Improving Neural Program Synthesis with Inferred Execution Traces,"The task of program synthesis, or automatically generating programs that are consistent with a provided specification, remains a challenging task in artificial intelligence. As in other fields of AI, deep learning-based end-to-end approaches have made great advances in program synthesis. However, more so than other fields such as computer vision, program synthesis provides greater opportunities to explicitly exploit structured information such as execution traces, which contain a superset of the information input/output pairs. While they are highly useful for program synthesis, as execution traces are more difficult to obtain than input/output pairs, we use the insight that we can split the process into two parts: infer the trace from the input/output example, then infer the program from the trace. This simple modification leads to state-of-the-art results in program synthesis in the Karel domain, improving accuracy to 81.3% from the 77.12% of prior work.","Richard Shin*,UC Berkeley;Illia Polosukhin,;Dawn Song,UC Berkeley",ricshin@berkeley.edu;illia@near.ai;dawnsong@gmail.com,Deep Learning,Spotlight
5353,Adaptive Path-Integral Autoencoders: Representation Learning and Planning for Dynamical Systems,"We present a representation learning algorithm that learns a low-dimensional latent dynamical system from high-dimensional sequential raw data, e.g., video. The framework builds upon recent advances in amortized inference methods that use both an inference network and a refinement procedure to output samples from a variational distribution given an observation sequence, and takes advantage of the duality between control and inference to approximately solve the intractable inference problem using the path integral control approach. The learned dynamical model can be used to predict and plan the future states; we also present the efficient planning method that exploits the learned low-dimensional latent dynamics. Numerical experiments show that the proposed path-integral control based variational inference method leads to tighter lower bounds in statistical model learning of sequential data.","Jung-Su Ha*,KAIST;Young-Jin Park,KAIST;Hyeok-Joo Chae,KAIST;Soon-Seo Park,KAIST;Han-Lim Choi,Korea Advanced Institute of Science and Technology",jsha@lics.kaist.ac.kr;yjpark@lics.kaist.ac.kr;hjchae@lics.kaist.ac.kr;sspark@lics.kaist.ac.kr;hanlimc@kaist.ac.kr,Probabilistic Methods/Variational Inference*; Reinforcement Learning and Planning/Planning,Poster
5356,Policy-Conditioned Uncertainty Sets for Robust Markov Decision Processes,"What policy should be employed in a Markov decision process with uncertain parameters? Robust optimization answer to this question is to use rectangular uncertainty sets, which independently reflect available knowledge about each state, and then obtains a decision policy that maximizes expected reward for the worst-case decision process parameters from these uncertainty sets. While this rectangularity is convenient computationally and leads to tractable solutions, it often produces policies that are too conservative in practice, and does not facilitate knowledge transfer between portions of the state space or across related decision processes. In this work, we propose non-rectangular uncertainty sets that bound marginal moments of state-action features defined over entire trajectories through a decision process. This enables generalization to different portions of the state space while retaining appropriate uncertainty of the decision process. We develop algorithms for solving the resulting robust decision problems, which reduce to finding an optimal policy for a mixture of decision processes, and demonstrate the benefits of our approach experimentally.","Andrea Tirinzoni*,Politecnico di Milano;Marek Petrik,University of New Hampshire;Xiangli Chen,University of Illinois at Chicago;Brian Ziebart,UIC",andrea.tirinzoni@polimi.it;mpetrik@cs.unh.edu;xchen40@uic.edu;bziebart@uic.edu,Reinforcement Learning and Planning/Decision and Control,Spotlight
5363,Unsupervisedly Learned Latent Graphs as Transferable Representations,"Modern deep transfer learning approaches have mainly focused on learning \emph{generic} feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different types of input embeddings or features.","Zhilin Yang*,CMU;Junbo Zhao,New York University;Bhuwan Dhingra,CMU;Kaiming He,Facebook AI Research;William Cohen,Carnegie Mellon University;Ruslan Salakhutdinov,Carnegie Mellon University;Yann LeCun,New York University",zhiliny@cs.cmu.edu;j.zhao@nyu.edu;bdhingra@andrew.cmu.edu;kaiminghe@fb.com;wcohen@cs.cmu.edu;rsalakhu@cs.cmu.edu;yann@cs.nyu.edu,Deep Learning*; Algorithms/Unsupervised Learning,Poster
5367,Online Learning of Quantum States,"Suppose we have many copies of an unknown n-qubit state $\rho$. We measure some copies of $\rho$ using a known two-outcome measurement E_1, then other copies using a measurement E_2, and so on. At each stage t, we generate a current hypothesis $\omega_t$ about the state $\rho$, using the outcomes of the previous measurements. We show that it is possible to do this in a way that guarantees that $|\trace(E_i \omega_t)  - \trace(E_i\rho)|$, the error in our prediction for the next measurement, is at least $eps$ at most $O(n / eps^2)  $\ times. Even in the non-realizable setting---where there could be arbitrary noise in the measurement outcomes---we show how to output hypothesis states that incur at most  $O(\sqrt {Tn})  $ excess loss over the best possible state on the first $T$ measurements. These results generalize a 2007 theorem by Aaronson on the PAC-learnability of quantum states, to the online and regret-minimization settings. We give three different ways to prove our results---using convex optimization, quantum postselection, and sequential fat-shattering dimension---which have different advantages in terms of parameters and portability.","Scott Aaronson,UT Austin;Xinyi Chen*,Google;Elad Hazan,Princeton University and Google Brain;Satyen Kale,Google;Ashwin Nayak,University of Waterloo",aaronson@cs.utexas.edu;xinyic@google.com;ehazan@cs.princeton.edu;satyen.kale@gmail.com;anayak@uwaterloo.ca,Algorithms/Online Learning*; Theory/Information Theory,Poster
5375,Wavelet regression and additive models for irregularly spaced data,"We present a novel approach for nonparametric regression using wavelet basis functions. Our proposal, \name, can be applied to non-equispaced data with sample size not necessarily a power of 2. We develop an efficient proximal gradient descent algorithm for computing the estimator and establish adaptive minimax convergence rates. The main appeal of our approach is that it naturally extends to additive and sparse additive models for a potentially large number of covariates. We prove minimax optimal convergence rates under a weak compatibility condition for sparse additive models. The compatibility condition holds when we have a small number of covariates; in addition, we establish convergence rates for when the condition is not met. We complement our theoretical results with empirical studies comparing \name\ to existing methods.","Asad Haris*,University of Washington;Ali Shojaie,;Noah Simon,University of Washington",aharis@uw.edu;ashojaie@uw.edu;nrsimon@uw.edu,Algorithms*; Algorithms/Sparsity and Compressed Sensing,Poster
5377,Inferring Latent Velocities from Weather Radar Data using Gaussian Processes,"Archived data from the US network of weather radars hold detailed information about bird migration over the last 25 years, including very high-resolution partial measurements of velocity. Historically, most of this spatial resolution is discarded and velocities are summarized at a very small number of locations due to modeling and algorithmic limitations. This paper presents a Gaussian process (GP) model to reconstruct high-resolution full velocity fields across the entire US. The GP faithfully models all aspects of the problem in a single joint framework, including spatially random velocities, partial velocity measurements, station-specific geometries, measurement noise, and an ambiguity known as aliasing. We develop fast inference algorithms based on the FFT; to do so, we employ a creative use of Laplace's method to sidestep the fact that the kernel of the joint process is non-stationary.","Rico Angell*,University of Massachusetts;Daniel Sheldon,University of Massachusetts, Amherst",rangell@cs.umass.edu;sheldon@cs.umass.edu,Applications/Sustainability*; Algorithms/Kernel Methods; Probabilistic Methods/Gaussian Processes,Poster
5380,A Structured Prediction Approach for Label Ranking,"We propose a novel label ranking method based on the framework of structured prediction, more specifically output kernel regression, which usually solves a supervised learning problem in two steps: the regression step in a well-chosen feature space and the pre-image step. We use specific feature maps/embeddings for ranking data, which convert any ranking/permutation into a vector representation. These embeddings are all well-tailored for our approach, either by resulting in consistent estimators, or by solving trivially the pre-image problem which is often the bottleneck in structured prediction. We also propose their natural extension the case of partial rankings and prove their efficiency on real-world datasets.","Anna Korba*,TELECOM PARISTECH;Alexandre Garcia,Telecom ParisTech;Florence d‚ÄôAlche-Buc,T√©l√©com ParisTech",akorba0@gmail.com;algarcia@enst.fr;florence.dalche@telecom-paristech.fr,Algorithms/Ranking and Preference Learning*; Algorithms/Structured Prediction,Poster
5390,Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features,"We develop an efficient and provably no-regret Bayesian optimization (BO) algorithm for optimization of black-box functions in high dimensions. We assume a generalized additive model with possibly overlapping variable groups. When the groups do not overlap, we are able to provide the first provably no-regret \emph{polynomial time} (in the number of evaluations of the acquisition function) algorithm for solving high dimensional BO. To make the optimization efficient and feasible, we introduce a novel deterministic Fourier Features approximation based on numerical integration with detailed analysis for the squared exponential kernel. The error of this approximation decreases \emph{exponentially} with the number of features, and allows for a precise approximation of both posterior mean and variance. In addition, the kernel matrix inversion improves in its complexity from cubic to essentially linear in the number of data points measured in basic arithmetic operations.","Mojmir Mutny*,ETH Zurich;Andreas Krause,ETH Z√ºrich",mmutny@inf.ethz.ch;krausea@ethz.ch,Probabilistic Methods/Gaussian Processes*; Algorithms/Bandit Algorithms,Spotlight
5394,"FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network","This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of increased prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix's singular values but have also increased the model size as they required a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by developing a leaky integrator unit inspired peephole connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the peephole to a gated architecture by reusing the RNN matrices in the gate to match state-of-the-art accuracies but with a 2-4x smaller model as compared to other gated architectures and with almost no overheads over a standard RNN. Further compression could be achieved by allowing FastGRNN's matrices to be low-rank, sparse and quantized without a significant loss in accuracy. Experiments on multiple benchmark datasets revealed that FastGRNN could make more accurate predictions with up to a 35x smaller model as compared to leading unitary and gated RNN techniques. FastGRNN's code can be publicly downloaded from~\citep{anonymous}.","Aditya Kusupati*,Microsoft Research;Manish Singh,Indian Institute of Technology Delhi;Kush Bhatia,UC Berkeley;Ashish Kumar,UC Berkeley;Prateek Jain,Microsoft Research;Manik Varma,Microsoft Research",t-vekusu@microsoft.com;singhmanishiitd@gmail.com;kushbhatia@berkeley.edu;ashish_kumar@berkeley.edu;prajain@microsoft.com;manik@microsoft.com,Deep Learning/Recurrent Networks,Poster
5410,Reversible Recurrent Neural Networks,"Recurrent neural networks (RNNs) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of RNN models which can be trained. Reversible RNNs---RNNs for which the hidden-to-hidden transition can be reversed---offer a path to reduce the memory requirements of training, as hidden states need not be stored and instead can be recomputed during backpropagation. We begin by arguing that perfectly reversible RNNs, which require no storage of the hidden activations, are fundamentally limited because they cannot forget information from their hidden state. We find that even if forgetting is allowed, the hidden-to-hidden transition can still be reversed by storing the forgotten information. Through efficient storage of this information, memory requirements of training RNNs are greatly reduced. Our method achieves roughly equivalent performance to traditional models while saving 5--15 times memory on language modeling and neural machine translation tasks.","Matthew MacKay*,University of Toronto;Paul Vicol,University of Toronto;Jimmy Ba,;Roger Grosse,-",mmackay@cs.toronto.edu;pvicol@cs.toronto.edu;jba@cs.toronto.edu;rgrosse@cs.toronto.edu,Deep Learning/Recurrent Networks*; Applications/Natural Language Processing; Deep Learning/Attention Models; Deep Learning/Eff,Poster
5412,SING: Symbol-to-Instrument Neural Generator,"Recent progress in deep learning for audio synthesis opens
the way to models that directly produce the waveform, shifting away
from the traditional paradigm of relying on vocoders or MIDI synthesizers for speech or music generation. Despite
their successes, current state-of-the-art neural audio synthesizers such
as WaveNet and SampleRNN suffer from prohibitive training and inference times because they are based on
autoregressive models that generate audio samples one at a time at a rate of 16kHz. In
this work, we study the more computationally efficient alternative of generating the waveform frame-by-frame with large strides.
We present a lightweight neural audio synthesizer for the original task of generating musical notes given desired instrument, pitch and velocity. Our model is trained end-to-end to generate notes from nearly 1000 instruments with a single decoder, thanks to a new loss function that minimizes the distances between the log spectrograms of the generated and target waveforms.
On the generalization task of synthesizing notes for pairs of pitch and instrument not seen during training,
the model is competitive in terms of perceptual quality to a
state-of-the-art autoencoder based on WaveNet, 
and is 85 times faster for training and 2,500 times faster for inference.","Alexandre Defossez*,Facebook;Neil Zeghidour,facebook;Nicolas Usunier,Facebook AI Research;Leon Bottou,Facebook;Francis Bach,INRIA - Ecole Normale Sup√©rieure",defossez@fb.com;neilz@fb.com;usunier@fb.com;leon@bottou.org;francis.bach@inria.fr,Applications/Audio and Speech Processing,Poster
5425,Learning Compressed Transforms with Low Displacement Rank,"The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a rich class of LDR matrices with much more general displacement operators, and explicitly learn over both the operator and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes consistently exceed the accuracy of existing compression approaches, and on some tasks even outperform fully-connected layers while using more than 20X fewer parameters. ","Anna Thomas*,Stanford;Albert Gu,Stanford University;Tri Dao,Stanford University;Atri Rudra,University at Buffalo, SUNY;Christopher Re,Stanford University",thomasat@stanford.edu;albertgu@stanford.edu;trid@stanford.edu;atri@buffalo.edu;chrismre@cs.stanford.edu,Deep Learning/Efficient Training Methods*; Algorithms/Representation Learning,Poster
5436,Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds,"In recent years, unfolding iterative algorithms as neural networks has been shown an empirical success in solving sparse recovery problems. However, its theoretical understanding is still immature, which prevents us from fully utilizing the power of neural networks. In this work, we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure, unfolded ISTA can attain a linear convergence, which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore, we propose to incorporate thresholding in the network to perform support selection, which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations, including sparse vector recovery and a compressive sensing experiment on real image data, corroborate our theoretical results and demonstrate their practical usefulness.","Xiaohan Chen,Texas A&M University;Jialin Liu,University of California, Los Angeles (UCLA);Zhangyang Wang*,TAMU;Wotao Yin,University of California, Los Angeles",chernxh@tamu.edu;liujl11@math.ucla.edu;atlaswang@tamu.edu;wotaoyin@math.ucla.edu,Algorithms/Sparsity and Compressed Sensing*; Deep Learning/CNN Architectures,Spotlight
5440,Iterative Value-Aware Model Learning,"This paper introduces a model-based reinforcement learning (MBRL) framework that incorporates the underlying decision problem in learning the transition model of the environment.
This is in contrast with conventional approaches to MBRL that learn the model of the environment, for example by finding the maximum likelihood estimate, without taking into account the decision problem.
Value-Aware Model Learning (VAML) framework argues that this might not be a good idea, especially if the true model of the environment does not belong to the model class from which we are estimating the model.

The original VAML framework, however, may result in an optimization problem that is difficult to solve. This paper introduces a new MBRL class of algorithms, called Iterative VAML, that benefit from the structure of how the planning is performed (i.e., through approximate value iteration) to devise a simpler optimization problem.
The paper theoretically analyze Iterative VAML and provide finite sample guarantee for it.","Amir-massoud Farahmand*,Vector Institute",farahmand@vectorinstitute.ai,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Model-Based RL; Theory/Learning Theory,Poster
5443,Evading the Adversary in Invariant Representation,"Representations of data that are invariant to changes in specified nuisance factors are useful for a wide range of problems: removing potential bias in prediction problems, controlling the effects of known confounders, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive by casting invariant representation learning for various tasks as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations.","Daniel Moyer*,University of Southern California;Shuyang Gao,ISI USC;Rob Brekelmans,USC / ISI;Aram Galstyan,USC Information Sciences Institute;Greg Ver Steeg,USC Information Sciences Institute",moyerd@usc.edu;sgao@isi.edu;brekelma@usc.edu;galstyan@isi.edu;gregv@isi.edu,"Algorithms/Representation Learning*; Applications/Fairness, Accountability, and Transparency; Deep Learning/Adversarial Networks; D",Poster
5452,Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias,"Data-driven approaches to solving robotic tasks have gained a lot of traction in recent years. However, most existing policies are trained on large-scale datasets collected in curated lab settings. If we aim to deploy these models in unstructured visual environments like people's homes, they will be unable to cope with the mismatch in data distribution. In such light, we present the first systematic effort in collecting a large dataset for robotic grasping in homes. First, to scale and parallelize data collection, we built a low cost mobile manipulator assembled for under 3K USD. Second, data collected using low cost robots suffer from noisy labels due to imperfect execution and calibration errors. To handle this, we develop a framework which factors out the noise as a latent variable. Our model is trained on 28K grasps collected in several houses under an array of different environmental conditions. We evaluate our models by physically executing grasps on a collection of novel objects in multiple unseen homes. The models trained with our home dataset showed a marked improvement of 43.7% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10% better than one that did not factor out the noise. We hope this effort inspires the robotics community to look outside the lab and embrace learning based approaches to handle inaccurate cheap robots.","Abhinav Gupta,CMU;Adithyavairavan Murali*,Carnegie Mellon University Robotics Institute;Dhiraj Gandhi,Carnegie Mellon University Robotics Institute;Lerrel Pinto,Carnegie Mellon University",abhinavg@cs.cmu.edu;amurali@andrew.cmu.edu;dgandhi@andrew.cmu.edu;lerrelp@andrew.cmu.edu,Applications/Robotics,Poster
5456,Learning safe policies with expert guidance,"We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.","Jessie Huang*,McGill University;Fa Wu,;Doina Precup,McGill University;Yang Cai,McGill University",jessieht@gmail.com;fa.wu2@mail.mcgill.ca;dprecup@cs.mcgill.ca;cai@cs.mcgill.ca,Reinforcement Learning and Planning*; Algorithms/Online Learning; Reinforcement Learning and Planning/Reinforcement Learning; Theory,Poster
5458,Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data,"Precision medicine aims for personalized prognosis and therapeutics by utilizing recent genome-scale high-throughput profiling techniques, including next-generation sequencing (NGS). However, translating NGS data faces several challenges. First, NGS count data are often overdispersed, requiring appropriate modeling. Second, compared to the number of involved molecules and system complexity, the number of available samples for studying complex disease, such as cancer, is often limited, especially considering disease heterogeneity. The key question is whether we may integrate available data from all different sources or domains to achieve reproducible disease prognosis based on NGS count data. In this paper, we develop a Bayesian Multi-Domain Learning (BMDL) model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small. Experimental results from both our simulated and NGS datasets from The Cancer Genome Atlas (TCGA) demonstrate the promising potential of BMDL for effective multi-domain learning without ``negative transfer'' effects often seen in existing multi-task learning and transfer learning methods. ","Ehsan Hajiramezanali*,Texas A&M University;Siamak  Zamani Dadaneh,Texas A&M University;Alireza Karbalayghareh,Texas A&M University;Mingyuan Zhou,University of Texas at Austin;Xiaoning  Qian,Texas A&M University",ehsanr@tamu.edu;siamak@tamu.edu;alireza.kg@tamu.edu;Mingyuan.Zhou@mccombs.utexas.edu;xqian@ece.tamu.edu,Applications/Computational Biology and Bioinformatics,Poster
5484,Learning SMaLL Predictors,"We introduce a new framework for learning in severely resource-constrained settings. Our technique delicately amalgamates the representational richness of multiple linear predictors with the sparsity of Boolean relaxations, and thereby yields classifiers that are compact, interpretable, and accurate. We provide a rigorous formalism of the learning problem, and establish fast convergence of the ensuing algorithm via relaxation to a minimax saddle point objective. We corroborate the theoretical foundations of our work with an extensive empirical evaluation. Our method,  Sparse Multiprototype Linear Learner (SMaLL), achieves state-of-the-art performance on several OpenML datasets.","Vikas Garg*,MIT;Ofer Dekel,Microsoft Research;Lin Xiao,Microsoft Research",vgarg@csail.mit.edu;oferd@microsoft.com;lin.xiao@microsoft.com,Algorithms/Classification*; Optimization/Convex Optimization,Poster
5491,Phase Retrieval Under a Generative Prior,"We introduce a novel deep-learning inspired formulation of the \textit{phase retrieval problem}, which asks to recover a signal $y_0 \in \R^n$ from $m$ quadratic observations, under structural assumptions on the underlying signal. As is common in many imaging problems, previous methodologies have considered natural signals as being sparse with respect to a known basis, resulting in the decision to enforce a generic sparsity prior. However, these methods for phase retrieval have encountered possibly fundamental limitations, as no computationally efficient algorithm for sparse phase retrieval has been proven to succeed with fewer than $O(k^2\log n)$ generic measurements, which is larger than the theoretical optimum of $O(k \log n)$. In this paper, we sidestep this issue by considering a prior that a natural signal is in the range of a generative neural network $G : \R^k \rightarrow \R^n$.  We introduce an empirical risk formulation that has favorable global geometry for gradient methods, as soon as $m = O(k)$, under the model of a multilayer fully-connected neural network with random weights.  Specifically, we show that there exists a descent direction outside of a small neighborhood around the true $k$-dimensional latent code and a negative multiple thereof.  This formulation for structured phase retrieval thus benefits from two effects: generative priors can more tightly represent natural signals than sparsity priors, and this empirical risk formulation can exploit those generative priors at an information theoretically optimal sample complexity, unlike for a sparsity prior. We corroborate these results with experiments showing that exploiting generative models in phase retrieval tasks outperforms both sparse and general phase retrieval methods.","Paul Hand,Northeastern University;Oscar Leong*,Rice University;Vladislav Voroninski,Helm.ai",p.hand@northeastern.edu;ofl2@rice.edu;vlad@helm.ai,Algorithms/Sparsity and Compressed Sensing*; Applications/Signal Processing; Optimization/Non-Convex Optimization,Oral
5500,Quadrature-based features for kernel approximation,"We consider the problem of improving kernel approximation via randomized feature maps. These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets. Based on an efficient numerical integration technique, we propose a unifying approach that reinterprets the previous random features methods and extends to better estimates of the kernel approximation. We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.","Marina Munkhoeva*,Skoltech;Yermek Kapushev,Skoltech, Institute of Information Transmission Problems;Evgeny Burnaev,Skoltech;Ivan Oseledets,Skolkovo Institute of Science and Technology",marina.munkhoeva@skolkovotech.ru;kapushev@gmail.com;e.burnaev@skoltech.ru;I.Oseledets@skoltech.ru,Algorithms/Kernel Methods*; Theory/Computational Complexity,Spotlight
5501,Reducing Network Agnostophobia,"Agnostophobia, the fear of the unknown, can be experienced by deep learning engineers while applying their networks to real-world applications. Unfortunately, network behavior is not well defined for inputs far from its training. In an uncontrolled environment, networks face many instances which are not of interest to them and have to be rejected in order to avoid a false positive. This problem has previously been tackled by researchers by either a) thresholding softmax, which by construction must return one of the known classes, or b) using an additional background or garbage class. In this paper, we show that both of these approaches help, but are generally insufficient when previously unseen classes are encountered. We introduce a new evaluation metric which focuses on comparing the performance of multiple approaches in scenarios where unknowns are encountered. Our major contributions are our simple yet effective Entropic Openset, and Objectosphere losses, which similar to the current approaches train with negative samples. However, these novel losses are designed to maximize entropy for unknown inputs while also increasing separation in deep feature magnitude between known and unknown classes. Experiments on MNIST and CIFAR-10 show that our novel loss is significantly better at dealing with unknown inputs from datasets such as letters, Not MNIST, Devanagari, and SVHN.","Akshay Dhamija*,University of Colorado Colorado Springs	;Manuel G√ºnther,Vision and Security Technology Lab (VaST);Terrance Boult,University of Colorado Colorado Springs",adhamija@uccs.edu;mgunther@vast.uccs.edu;tboult@VAST.UCCS.EDU,Algorithms/Representation Learning*; Algorithms/Classification; Applications/Object Recognition,Spotlight
5504,A Stein variational Newton method,"Stein variational gradient descent (SVGD) was recently proposed as a general purpose nonparametric variational inference algorithm: it minimizes the Kullback‚ÄìLeibler divergence between the target distribution and its approximation by implementing a form of functional gradient descent on a reproducing kernel Hilbert space [Liu & Wang, NIPS 2016]. In this paper, we accelerate and generalize the SVGD algorithm by including second-order information, thereby approximating a Newton-like iteration in function space. We also show how second-order information can lead to more effective choices of kernel. We observe significant computational gains over the original SVGD algorithm in multiple test cases.","Gianluca Detommaso*,University of Bath;Tiangang Cui,Monash University;Youssef Marzouk,MIT;Alessio Spantini,MIT;Robert Scheichl,Heidelberg University",detommaso.gianluca@gmail.com;Tiangang.Cui@monash.edu;ymarz@mit.edu;spantini@mit.edu;r.scheichl@uni-heidelberg.de,Probabilistic Methods/Variational Inference*; Deep Learning/Efficient Inference Methods,Poster
5521,Watch Your Step: Learning Node Embeddings via Graph Attention,"Graph embedding methods represent nodes in a continuous vector space,
preserving different types of relational information from the graph.
There are many hyper-parameters to these methods (e.g. the length of a random walk) which have to be manually tuned for every graph.
In this paper, we replace previously fixed hyper-parameters with trainable ones that we automatically learn via backpropagation. 
In particular, we learn a novel attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective.
Unlike previous approaches to attention models, the method that we propose utilizes attention parameters exclusively on the data itself (e.g. on the random walk), and are not used by the model for inference.
We experiment on link prediction tasks, as we aim to produce embeddings that best-preserve the graph structure, generalizing to unseen information. We improve state-of-the-art on a comprehensive suite of real world datasets including social, collaboration, and biological networks. Adding attention to random walks can reduce the error by 20\% to 45\% on datasets we attempted.
We show that our learned attention parameters can vary significantly for different graphs, and correspond to the optimal choice of hyper-parameter if we manually tune existing methods.","Sami Abu-El-Haija*,Google Research;Bryan Perozzi,Google Research;Rami Al-Rfou,Google Research;Alex Alemi,Google",sami@haija.org;hubris@google.com;rmyeid@google.com;alemi@google.com,Algorithms/Representation Learning*; Applications/Network Analysis,Poster
5535,Visual Goal-Conditioned Reinforcement Learning by Representation Learning,"For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals in a real-world physical system, and substantially outperforms prior techniques.","Ashvin Nair*,UC Berkeley;Vitchyr Pong,UC Berkeley;Murtaza Dalal,UC Berkeley;Shikhar Bahl,UC Berkeley;Steven Lin,UC Berkeley;Sergey Levine,UC Berkeley",anair17@berkeley.edu;vitchyr@eecs.berkeley.edu;mdalal@berkeley.edu;shikharbahl@berkeley.edu;stevenlin598@berkeley.edu;svlevine@eecs.berkeley.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Deep Learning,Spotlight
5541,Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition,"Inspired by ""predictive coding"" - a theory in neuroscience, we develop a bi-directional and dynamical neural network with local recurrent processing, namely predictive coding network (PCN). Unlike any feedforward-only convolutional neural network, PCN includes both feedback connections, which carry top-down predictions, and feedforward connections, which carry bottom-up errors of prediction. Feedback and feedforward connections enable adjacent layers to interact locally and recurrently to refine representations towards minimization of layer-wise prediction errors. When unfolded over time, the recurrent processing gives rise to an increasingly deeper hierarchy of non-linear transformation, allowing a shallow network to dynamically extend itself into an arbitrarily deep network. We train and test PCN for image classification with SVHN, CIFAR and ImageNet datasets. Despite notably fewer layers and parameters, PCN achieves competitive performance compared to classical and state-of-the-art models. Further analysis shows that the internal representations in PCN converge over time and yield increasingly better accuracy in object recognition. Errors of top-down prediction also map visual saliency or bottom-up attention. This work takes us one step closer to bridging human and machine intelligence in vision.","Kuan Han,Purdue University;Haiguang Wen,Purdue University;Yizhen Zhang,;Di Fu,Purdue University;Eugenio Culurciello,Nil;Zhongming Liu*,Purdue University",han424@purdue.edu;wen37@purdue.edu;zhan2251@purdue.edu;fu230@purdue.edu;euge@purdue.edu;zmliu@purdue.edu,Deep Learning/Biologically Plausible Deep Networks*; Algorithms/Classification,Poster
5551,PAC-Bayes bounds for stable algorithms with instance-dependent priors,"PAC-Bayes bounds have been proposed to get risk estimates based on a training sample. In this paper the PAC-Bayes approach is combined with stability of the hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting is used with a Gaussian prior centered at the expected output. Thus a novelty of our paper is using priors defined in terms of the data-generating distribution. Our main result estimates the risk of the randomized algorithm in terms of the hypothesis stability coefficients. We also provide a new bound for the SVM classifier, which is compared to other known bounds experimentally. Ours appears to be the first stability-based bound that evaluates to non-trivial values.","Omar Rivasplata*,University College London;Csaba Szepesvari,DeepMind/University of Alberta;John  Shawe-Taylor,University College London;Emilio Parrado-Hern√°ndez,Universidad Carlos III de Madrid;Shiliang Sun,East China Normal University",o.rivasplata@cs.ucl.ac.uk;szepesva@cs.ualberta.ca;j.shawe-taylor@ucl.ac.uk;emipar@tsc.uc3m.es;shiliangsun@gmail.com,Theory*; Algorithms/Classification; Algorithms/Kernel Methods; Algorithms/Model Selection and Struct,Poster
5558,Beyond Grids: Learning Graph Representations for Visual Recognition,"We propose learning graph representations from 2D feature maps for visual recognition. Our method draws inspiration from region based recognition, and learns to transform a 2D image into a graph structure. The vertices of the graph define clusters of pixels (""regions""), and the edges measure the similarity between these clusters in a feature space. Our method further learns to propagate information across all vertices on the graph, and is able to project the learned graph representation back into 2D grids. Our graph representation facilitates reasoning beyond regular grids and can capture long range dependencies among regions. We demonstrate that our model can be trained from end-to-end, and is easily integrated into existing networks. Finally, we evaluate our method on three challenging recognition tasks: semantic segmentation, object detection and object instance segmentation. For all tasks, our method outperforms state-of-the-art methods.","Yin Li*,University of Wisconsin-Madison;Abhinav Gupta,CMU",yin.li@wisc.edu;abhinavg@cs.cmu.edu,Applications/Computer Vision*; Deep Learning/CNN Architectures,Poster
5568,The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization,"Motivated by applications in Optimization, Game Theory, and the training of Generative Adversarial Networks, the convergence properties of first order methods in min-max problems have received extensive study. It has been recognized that they may cycle, and there is no good understanding of their limit points when they do not. When they converge, do they converge to local min-max solutions? We characterize the limit points of two basic first order methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent Ascent (OGDA).  We show that both dynamics avoid unstable critical points for almost all initializations. Moreover, for small step sizes and under mild assumptions, the set of  \{OGDA\}-stable critical points is a superset of \{GDA\}-stable critical points, which is a superset of local min-max solutions (strict in some cases). The connecting thread is that the behavior of these dynamics can be studied from a dynamical systems perspective.","Constantinos Daskalakis,MIT;Ioannis Panageas*,MIT",costis@csail.mit.edu;panageasj@gmail.com,Optimization/Non-Convex Optimization*; Algorithms/Dynamical Systems; Algorithms/Online Learning,Poster
5579,Coordinate Descent with Bandit Sampling,"Coordinate descent methods minimize a cost function by updating a single decision variable (corresponding to one coordinate) at a time. Ideally, we would update the decision variable that yields the largest marginal decrease in the cost function. However, finding this coordinate would require checking all of them, which is not computationally practical. Therefore, we propose a new adaptive method for coordinate descent. First, we define a lower bound on the decrease of the cost function when a coordinate is updated and, instead of calculating this lower bound for all coordinates, we use a multi-armed bandit algorithm to learn which coordinates result in the largest marginal decrease and simultaneously perform coordinate descent. We show that our approach improves the convergence of the coordinate methods both theoretically and experimentally.","Farnood Salehi*,EPFL;Patrick Thiran,;Elisa Celis,EPFL",farnood.salehi@epfl.ch;patrick.thiran@epfl.ch;elisa.celis@epfl.ch,Optimization/Convex Optimization,Poster
5603,Deep Dynamical Modeling and Control of Unsteady Fluid Flows,"The design of flow control systems remains a challenge due to the nonlinear nature of the equations that govern fluid flow. However, recent advances in computational fluid dynamics (CFD) have enabled the simulation of complex fluid flows with high accuracy, opening the possibility of using learning-based approaches to facilitate controller design. We present a method for learning the forced and unforced dynamics of airflow over a cylinder directly from CFD data. The proposed approach, grounded in Koopman theory, is shown to produce stable dynamical models that can predict the time evolution of the cylinder system over extended time horizons. Finally, by performing model predictive control with the learned dynamical models, we are able to find a straightforward, interpretable control law for suppressing vortex shedding in the wake of the cylinder.","Jeremy Morton*,Stanford University;Antony Jameson,Texas A&M University;Mykel Kochenderfer,Stanford University;Freddie Witherden,Imperial College London",jmorton2@stanford.edu;jameson@baboon.stanford.edu;mykel@stanford.edu;fdw@stanford.edu,Reinforcement Learning and Planning/Decision and Control*; Deep Learning/Deep Autoencoders; Deep Learning/Efficient Training Methods,Poster
5614,Confounding-Robust Policy Improvement,"We study the problem of learning personalized decision policies from observational data while accounting for possible unobserved confounding in the data-generating process. Unlike previous approaches which assume unconfoundedness, i.e. no unobserved confounders affected treatment assignment as well as outcome, we calibrate policy learning for realistic violations of this unverifiable assumption with uncertainty sets motivated by sensitivity analysis in causal inference. Our framework for confounding-robust policy improvement optimizes the minimax regret of a candidate policy against a baseline or reference ""status quo"" policy, over a uncertainty set around nominal propensity weights. We prove that if the uncertainty set is well-specified, robust policy learning can do no worse than the baseline, and only improve if the data supports it. We characterize the adversarial subproblem and use efficient algorithmic solutions to optimize over parametrized spaces of decision policies such as logistic treatment assignment. We assess our methods on synthetic data and a large clinical trial, demonstrating that confounded selection can hinder policy learning and lead to unwarranted harm, while our robust approach guarantees safety and focuses on well-evidenced improvement.","Nathan Kallus,Cornell Tech;Angela Zhou*,Cornell University",kallus@cornell.edu;az434@cornell.edu,Probabilistic Methods/Causal Inference*; Optimization/Convex Optimization; Theory/Learning Theory,Poster
5640,The Importance of Sampling inMeta-Reinforcement Learning,We interpret meta-reinforcement learning as the problem of learning how to quickly find a good sampling distribution in a new environment. This interpretation leads to the development of two new meta-reinforcement learning algorithms: E-MAML and E-$\text{RL}^2$. Results are presented on a new environment we call `Krazy World': a difficult high-dimensional gridworld which is designed to highlight the importance of correctly differentiating through sampling distributions in  meta-reinforcement learning. Further results are presented on a set of maze environments. We show E-MAML and E-$\text{RL}^2$ deliver better performance than baseline algorithms on both tasks.,"Bradly Stadie*,University of California Berkeley;Ge Yang,University of Chicago;Rein Houthooft,OpenAI;Xi Chen,UC Berkeley;Yan Duan,University of California, Berkeley;Yuhuai Wu,University of Toronto;Pieter Abbeel,UC Berkeley;Ilya Sutskever,OpenAI",stadiebradly@gmail.com;yangge1987@gmail.com;rein.houthooft@gmail.com;adslcx@gmail.com;dementrock@gmail.com;ywu@cs.toronto.edu;pabbeel@cs.berkeley.edu;ilyasu@openai.com,Deep Learning/Meta-Learning*; Algorithms/Representation Learning; Deep Learning/Optimization for Deep Networks; Reinforcemen,Poster
5669,Representer Point Selection for Explaining Deep Neural Networks,"We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given black-box test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions. ","Chih-Kuan  Yeh,Carnegie Mellon University;Joon Sik Kim*,Carnegie Mellon University;Ian En-Hsu Yen,Carnegie Mellon University;Pradeep Ravikumar,Carnegie Mellon University",cjyeh@cs.cmu.edu;joonsikk@cs.cmu.edu;eyan@cs.cmu.edu;pradeepr@cs.cmu.edu,Deep Learning/Visualization or Exposition Techniques for Deep Networks*; Applications/Visual Scene Analysis and Interpretation; Deep Learning/Optimization for Deep Net,Poster
5672,The Effect of Network Width on the Performance of  Large-batch Training,"Distributed implementations of mini-batch stochastic gradient descent (SGD)  suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however it besets the convergence of the algorithm and the generalization performance.

In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that--for a fixed number of parameters--wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants.","Lingjiao Chen*,University of Wisconsin-Madison;Hongyi Wang,University of Wisconsin-Madison;Jinman Zhao,University of Wisconsin-Madison	;Dimitris Papailiopoulos,University of Wisconsin-Madison;Paraschos Koutris,University of Wisconsin-Madison",lchen362@wisc.edu;hongyiwang@cs.wisc.edu;jz@cs.wisc.edu;dimitris@papail.io;paris@cs.wisc.edu,Optimization*; Deep Learning/Optimization for Deep Networks; Optimization/Non-Convex Optimization,Poster
5679,SNIPER: Efficient Multi-Scale Training,"We present SNIPER, an algorithm for performing scale invariant training in instance level visual recognition tasks. Instead of processing every pixel in an image pyramid, SNIPER only processes context regions around ground-truth instances (referred to as chips) at the appropriate scale. For background sampling, these context-regions are generated using proposals extracted from a region proposal network trained with a short learning schedule. Hence, the number of chips generated per image during training adaptively changes based on the scene complexity. SNIPER only processes 30% more pixels compared to the commonly used single scale training at 800x1333 pixels on the COCO dataset. But, it also observes samples from extreme resolutions of the image pyramid, like 1400x2000 pixels. As SNIPER operates on low resolution chips (512x512 pixels), it can have a batch size as large as 20 on a single GPU even with a ResNet-101 backbone. Therefore it can benefit from batch-normalization during training without the need for synchronizing batch-normalization statistics across GPUs. SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct. Our implementation based on Faster-RCNN with a ResNet-101 backbone obtains an mAP of 47.6% on the COCO dataset for bounding box detection and can process 5 images per second with a single GPU. ","Bharat Singh*,Amazon;Mahyar Najibi,University of Maryland;Larry Davis,University of Maryland at College Park",bharat@cs.umd.edu;najibi@cs.umd.edu;lsd@cs.umd.edu,Applications/Object Detection*; Applications/Computer Vision; Applications/Object Recognition,Poster
5689,Sample Complexity of Nonparametric Semi-Supervised Learning,"We study the sample complexity of semi-supervised learning (SSL) and introduce new assumptions based on the mismatch between a mixture model learned from unlabeled data and the true mixture model induced by the (unknown) class conditional distributions. Under these assumptions, we establish an $\Omega(K\log K)$ labeled sample complexity bound without imposing parametric assumptions, where $K$ is the number of classes. Our results suggest that even in nonparametric settings it is possible to learn a near-optimal classifier using only a few labeled samples. Unlike previous theoretical work which focuses on binary classification, we consider general multiclass classification ($K>2$), which requires solving a difficult permutation learning problem. This permutation defines a classifier whose classification error is controlled by the Wasserstein distance between mixing measures, and we provide finite-sample results characterizing the behaviour of the excess risk of this classifier. Finally, we describe three algorithms for computing these estimators based on a connection to bipartite graph matching, and perform experiments to illustrate the superiority of the MLE over the majority vote estimator.","Chen Dan*,Carnegie Mellon University;Liu Leqi,Carnegie Mellon University;Bryon Aragam,Carnegie Mellon University;Pradeep Ravikumar,Carnegie Mellon University;Eric Xing,Petuum Inc. and CMU",cdan@andrew.cmu.edu;leqil@andrew.cmu.edu;naragam@cs.cmu.edu;pradeepr@cs.cmu.edu;epxing@cs.cmu.edu,Algorithms/Semi-Supervised Learning,Poster
5696,Hardware Conditioned Policies for Multi-Robot Transfer Learning,"Deep reinforcement learning could be used to learn dexterous robotic policies but it is extremely challenging to transfer them to new robots with vastly different hardware properties. It is also prohibitively expensive to learn a new policy from scratch for each robot hardware due to the high sample complexity of modern state-of-the-art algorithms. We propose a novel approach called \textit{Hardware Conditioned Policies} where we train a universal policy conditioned on a vector representation of robot hardware. We considered robots in simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. First, we use the kinematic structure directly as the hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. For robots with lower zero-shot success rate, we also demonstrate that fine-tuning the policy network is significantly more sample-efficient than training a model from scratch. In tasks where knowing the agent dynamics is crucial for success, we learn an embedding for robot hardware and show that policies conditioned on the encoding of hardware tend to generalize and transfer well. ","Tao Chen*,Carnegie Mellon University;Adithyavairavan Murali,Carnegie Mellon University Robotics Institute;Abhinav Gupta,CMU",taoc1@andrew.cmu.edu;amurali@andrew.cmu.edu;abhinavg@cs.cmu.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Decision and Control,Poster
5706,Co-regularized Alignment for Unsupervised Domain Adaptation,"Deep neural networks, trained with large amount of labeled data, can fail to
generalize well when tested with examples from a target domain whose distribution differs from the training data distribution, referred as the source domain. It can be expensive or even infeasible to obtain required amount of labeled data in all possible domains. Unsupervised domain adaptation sets out to address this problem, aiming to learn a good predictive model for the target domain using labeled examples from the source domain but only unlabeled examples from the target domain. 
Domain alignment approaches this problem by matching the source and target feature distributions, and has been used as a key component in many state-of-the-art domain adaptation methods. However, matching the marginal feature distributions does not guarantee that the corresponding class conditional distributions will be aligned across the two domains. We propose co-regularized domain alignment for unsupervised domain adaptation, which constructs multiple diverse feature  spaces and aligns source and target distributions in each of them individually, while encouraging that alignments agree with each other with regard to the class predictions on the unlabeled target examples.
The proposed method is generic and can be used to improve any domain adaptation method which uses domain alignment. We instantiate it in the context of a recent state-of-the-art method and 
observe that it provides significant performance improvements on several domain adaptation benchmarks.","Abhishek Kumar*,IBM Research;Prasanna Sattigeri,IBM Research;kahini wadhawan,IBM Research;Leonid Karlinsky,IBM-Research;Rogerio Feris,IBM Research;Bill Freeman,MIT;Gregory Wornell,MIT",abhishk@us.ibm.com;psattig@us.ibm.com;kahini.wadhawan@ibm.com;leonidka@il.ibm.com;rsferis@us.ibm.com;billf@mit.edu;gww@mit.edu,Algorithms/Multitask and Transfer Learning*; Algorithms/Unsupervised Learning,Poster
5707,Statistical and Computational Trade-Offs in Kernel K-Means,"We investigate the efficiency of k-means  in terms of both statistical and computational requirements.
More precisely,  we study  a Nystr\""om approach to kernel k-means. We analyze the statistical properties of the proposed method and show that it achieves  the same accuracy of exact kernel k-means with only a fraction of computations.
Indeed, we prove under basic assumptions  that sampling  $\sqrt{n}$ Nystr\""om  landmarks allows to greatly reduce computations without incurring in any loss of accuracy. To the best of our knowledge this is the first result showing in this kind for unsupervised learning. ","Daniele Calandriello*,LCSL IIT/MIT;Lorenzo Rosasco,",daniele.calandriello@iit.it;lrosasco@mit.edu,Algorithms/Kernel Methods*; Algorithms/Clustering; Applications/Matrix and Tensor Factorization; Theory/Learning Theory,Spotlight
5709,Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures,"The backpropagation of error algorithm (BP) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might implement or approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present the first results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on MNIST, CIFAR-10, and ImageNet and explore variants of the difference target-propagation (DTP) algorithm. We focus on DTP and introduce weight-transport-free variants modified to remove backpropagation from the penultimate layer, in both fully- and locally-connected architectures. These algorithms perform well for MNIST, but for CIFAR and ImageNet we find that DTP and variants perform significantly worse than BP, especially for network composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.","Sergey Bartunov*,DeepMind;Adam Santoro,DeepMind;Blake Richards,University of Toronto;Luke Marris,DeepMind;Geoffrey Hinton,Google;Timothy Lillicrap,DeepMind",bartunov@google.com;adamsantoro@google.com;blake.richards@utoronto.ca;marris@google.com;geoffhinton@google.com;countzero@google.com,Neuroscience and Cognitive Science*; Deep Learning; Neuroscience and Cognitive Science/Neuroscience; Neuroscience and Cognitive Scienc,Poster
5715,Learning Attractor Dynamics for Generative Memory,"A central challenge faced by memory systems is the robust retrieval of a stored pattern in the presence of interference due to other stored patterns and noise. A theoretically well-founded solution to robust retrieval is given by attractor dynamics, which iteratively cleans up patterns during recall. However, incorporating attractor dynamics into modern deep learning systems poses difficulties: attractor basins are characterised by vanishing gradients, which are known to make training neural networks difficult.  In this work, we exploit recent advances in variational inference and avoid the vanishing gradient problem by training a generative distributed memory with a variational lower-bound-based Lyapunov function. The model is minimalistic with surprisingly few parameters. Experiments shows it converges to correct patterns upon iterative retrieval and achieves competitive performance as both a memory model and a generative model.","Yan Wu*,DeepMind;Greg Wayne,DeepMind;Karol Gregor,DeepMind;Timothy Lillicrap,DeepMind",yanwu@google.com;gregwayne@google.com;karolg@google.com;countzero@google.com,Deep Learning/Memory-Augmented Neural Networks*; Algorithms/Dynamical Systems; Deep Learning/Generative Models,Poster
5723,The emergence of multiple retinal cell types through efficient coding of natural movies,"One of the most striking aspects of early visual processing in the retina is the immediate parcellation of visual information into multiple parallel pathways, formed by different retinal ganglion cell types each tiling the entire visual field. Existing theories of efficient coding have been unable to account for the functional advantages of such cell-type diversity in encoding natural scenes. Here we go beyond previous theories to analyze how a simple linear retinal encoding model with different convolutional cell types efficiently encodes naturalistic spatiotemporal movies with a fixed firing rate budget. We find that optimizing the receptive fields and cell densities of two cell types makes them match the properties of two main cell types in the primate retina, midget and parasol cells, in terms of spatial and temporal sensitivity, cell spacing, and their relative ratio. Moreover, our theory gives a precise account of how the ratio of midget to parasol cells decreases with retinal eccentricity.  Also, we train a nonlinear encoding model with a rectifying nonlinearity to efficiently encode naturalistic movies, and again find emergent receptive fields resembling those of midget and parasol cells that are now further subdivided into ON and OFF types. Thus our work provides a theoretical justification, based on the efficient coding of naturalistic movies, for the existence of the four most dominant cell types in the primate retina that together comprise 90\% of all ganglion cells.          ","Samuel Ocko*,Stanford University;Jack Lindsey,Stanford University;Surya Ganguli,;Stephane Deny,Stanford",socko@stanford.edu;jacklindsey@stanford.edu;sganguli@stanford.edu;sdeny@stanford.edu,Neuroscience and Cognitive Science/Neural Coding*; Applications/Signal Processing; Neuroscience and Cognitive Science/Neuroscience; Neuroscience,Poster
5727,Gather-Excite: Exploiting feature context in ConvNets,"The powerful image representations learned by deep convolutional neural networks (ConvNets) have propelled this family of models to a state of dominance in image classification.  But by constructing features in a strictly bottom-up manner with local operators, ConvNets may be unable to efficiently exploit contextual information that resides in the relationships between features. The focus of this work is to propose a simple, lightweight solution to the issue of limited context propagation in ConvNets.  Our approach, which we formulate as a gather-scatter operator pair, propagates context across a group of neurons by aggregating responses over their extent and redistributing the aggregates back through the group.  The simplicity of our approach brings several benefits: the operators add few parameters, minimal computational overhead and, importantly, can be directly integrated into existing architectures to improve performance without careful hyperparameter tuning.  We present evidence that integration of gather-scatter operators into a ConvNet produces qualitatively different intermediate feature representations.  Moreover, we show with experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets that improving context diffusion can be just as important as increasing the depth of a network, at a fraction of the cost.  In fact, we find that by supplementing a ResNet-50 model with gather-scatter operators, it is able to outperform its 101-layer counterpart on ImageNet with no additional learnable parameters.","Jie Hu,Momenta;Li Shen*,University of Oxford;Samuel Albanie,University of Oxford;Gang Sun,Momenta;Andrea Vedaldi,Oxford University",hujie@momenta.ai;lishen@robots.ox.ac.uk;albanie@robots.ox.ac.uk;sungang@momenta.ai;vedaldi@robots.ox.ac.uk,Deep Learning/CNN Architectures*; Algorithms/Classification,Poster
5728,Quantifying Linguistic Shifts: The Global Anchor Method and Its Applications,"Language is dynamic, constantly evolving and adapting with respect to time, domain or topic. The adaptability of language is an active research area, where researchers discover social, cultural and domain-specific changes in language using distributional tools such as word embeddings. In this paper, we introduce the global anchor method for detecting corpus-level language shifts. We show both theoretically and empirically that the global anchor method is equivalent to the alignment method, a widely-used method for comparing word embeddings, in terms of detecting corpus-level language shifts. Despite their equivalence in terms of detection abilities, we demonstrate that the global anchor method is superior in terms of applicability as it can compare embeddings of different dimensionalities. Furthermore, the global anchor method has implementation and parallelization advantages. We show that the global anchor method reveals fine structures in the evolution of language and domain adaptation. When combined with the graph Laplacian technique, the global anchor method recovers the evolution trajectory and domain clustering of disparate text corpora.","Zi Yin*,n.a.;Vin Sachidananda,Stanford University;Balaji Prabhakar,Stanford Univeristy",s0960974@gmail.com;Vsachi@stanford.edu;balaji@stanford.edu,Deep Learning/Embedding Approaches*; Applications/Computational Social Science; Applications/Natural Language Processing,Poster
5729,Identification and Estimation of Causal Effects from Dependent Data,"The assumption that data samples are independent and identically distributed (iid) is standard in many areas of statistics and machine learning. Nevertheless, in some settings, such as social networks, infectious disease modeling, and reasoning with spatial and temporal data, this assumption is false. An extensive literature exists on making causal inferences under the iid assumption [12, 8, 21, 16], but, as pointed out in [14], causal inference in non-iid contexts is challenging due to the combination of unobserved confounding bias and data dependence. In this paper we develop a general theory describing when causal inferences are possible in such scenarios. We use segregated graphs [15], a generalization of latent projection mixed graphs [23], to represent causal models of this type and provide a complete algorithm for non-parametric identification in these models. We then demonstrate how statistical inferences may be performed on causal parameters identified by this algorithm, even in cases where parts of the model exhibit full interference, meaning only a single sample is available for parts of the model [19]. We apply these techniques to a synthetic data set which considers the adoption of fake news articles given the social network structure, articles read by each person, and baseline demographics and socioeconomic covariates.","Eli Sherman*,Johns Hopkins University;Ilya Shpitser,Johns Hopkins University",esherma9@cs.jhu.edu;ilyas@cs.jhu.edu,Probabilistic Methods/Causal Inference,Spotlight
5743,Deepcode: Feedback Codes via Deep Learning,"The design of codes for communicating reliably over a statistically well defined channel is an important endeavor involving deep mathematical research and wide- ranging practical applications. In this work, we present the first family of codes obtained via deep learning, which significantly beats state-of-the-art codes designed over several decades of research. The communication channel under consideration is the Gaussian noise channel with feedback, whose study was initiated by Shannon; feedback is known theoretically to improve reliability of communication, but no practical codes that do so have ever been successfully constructed.

We break this logjam by integrating information theoretic insights harmoniously with recurrent-neural-network based encoders and decoders to create novel codes that outperform known codes by 3 orders of magnitude in reliability. We also demonstrate several desirable properties in the codes: (a) generalization to larger block lengths; (b) composability with known codes; (c) adaptation to practical constraints. This result also presents broader ramifications to coding theory: even when the channel has a clear mathematical model, deep learning methodologies, when combined with channel specific information-theoretic insights, can potentially beat state-of-the-art codes, constructed over decades of mathematical research.","Hyeji Kim*,Samsung AI Centre Cambridge;Yihan Jiang,University of Washington;Sreeram Kannan,University of Washington;Sewoong Oh,UIUC;Pramod Viswanath,UIUC",hkim1505@gmail.com;yihanrogerjiang@gmail.com;ksreeram@uw.edu;sewoong79@gmail.com;pramodv@illinois.edu,Theory/Information Theory*; Deep Learning,Poster
5744,Learning and Testing Causal Models with Interventions,"We consider testing and learning problems on causal Bayesian networks as defined by Pearl (Pearl, 2009). Given a causal Bayesian network M on a graph with n discrete variables and bounded in-degree and bounded ``confounded components'', we show that O(\log n) interventions on an unknown causal Bayesian network X on the same graph, and O~(n/epsilon^2) samples per intervention, suffice to efficiently distinguish whether X=M or whether there exists some intervention under which X and M are farther than epsilon in total variation distance.  We also obtain sample/time/intervention efficient algorithms for: (i) testing the identity of two unknown causal Bayesian networks on the same graph; and (ii) learning a causal Bayesian network on a given graph.  Although our algorithms are non-adaptive, we show that adaptivity does not help in general: Omega(log n) interventions are necessary for testing the identity of two unknown causal Bayesian networks on the same graph, even adaptively.  Our algorithms are enabled by a new subadditivity inequality for the squared Hellinger distance between two causal Bayesian networks.","Jayadev Acharya,Cornell University;Arnab Bhattacharyya*,Indian Institute of Science;Constantinos  Daskalakis,MIT;Saravanan Kandasamy,Tata Institute of Fundamental Research",acharya@cornell.edu;arnabb@iisc.ac.in;costis@mit.edu;saravan.tuty@gmail.com,Probabilistic Methods/Causal Inference*; Algorithms/Model Selection and Structure Learning; Probabilistic Methods/Graphical Models,Poster
5749,Implicit Bias of Gradient Descent on Linear Convolutional Networks,"We show that gradient descent on full-width linear convolutional networks of depth $L$ converges to a linear predictor related to the $\ell_{2/L}$ bridge penalty in the frequency domain. This is in contrast to linearly fully connected networks, where gradient descent converges to the hard margin linear SVM solution, regardless of depth. ","Suriya Gunasekar*,TTI Chicago;Jason Lee,USC;Daniel Soudry,Technion;Nathan Srebro,Toyota Technical Institute of Chicago",suriya@ttic.edu;jasonlee@marshall.usc.edu;daniel.soudry@gmail.com;nati@ttic.edu,Optimization/Non-Convex Optimization*; Deep Learning/CNN Architectures; Deep Learning/Optimization for Deep Networks,Poster
5750,DAGs with NO TEARS: Continuous Optimization for Structure Learning,"Estimating the structure of directed acyclic graphs (DAGs, also known as {Bayesian networks}) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely \emph{continuous} optimization problem over real matrices that avoids this combinatorial constraint entirely. 
This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.","Xun Zheng,Carnegie Mellon University;Bryon Aragam*,Carnegie Mellon University;Pradeep Ravikumar,Carnegie Mellon University;Eric Xing,Petuum Inc. and CMU",xunzheng90@gmail.com;naragam@cs.cmu.edu;pradeepr@cs.cmu.edu;epxing@cs.cmu.edu,Probabilistic Methods/Graphical Models,Spotlight
5763,PAC-Bayes Tree: weighted subtrees with guarantees,"We present a weighted-majority classification approach over subtrees of a fixed tree, which provably achieves excess-risk of the same order as the best tree-pruning. Furthermore, the computational efficiency of pruning is maintained at both training and testing time despite having to aggregate over an exponential number of subtrees. We believe this is the first subtree aggregation approach with such guarantees. ","Tin Nguyen*,Princeton University;Samory Kpotufe,",tindanh.nguyen96@gmail.com;samory@princeton.edu,Theory/Learning Theory*; Algorithms/Classification,Poster
5773,Multi-objective Maximization of Monotone Submodular Functions with Cardinality Constraint,"	We consider the problem of multi-objective maximization of monotone submodular functions subject to cardinality constraint, often formulated as $\max_{|A|=k}\min_{i\in\{1,\dots,m\}}f_i(A)$. While it is well known that greedy methods work well for a single objective, the problem becomes much harder with multiple objectives. In fact, Krause et al.\ (2008) showed that when the number of objectives $m$ grows as the cardinality $k$ i.e., $m=\Omega(k)$, the problem is inapproximable (unless $P=NP$). On the other hand when $m$ is constant, there is a randomized $(1-1/e)-\epsilon$ approximation with runtime (number of queries to function oracle) $n^{m/\epsilon^3}$ due to Chekuri et al.\ (2010). %In fact, the result of Chekuri et al.\ (2010) is for the far more general case of matroid constant. 
	
	In this paper, we focus on finding a fast and practical algorithm that has (asymptotic) approximation guarantees even when $m$ is super constant. We first modify the algorithm of Chekuri et al.\ (2010) to achieve an $(1-1/e)$ approximation for $m=o(\frac{k}{\log^3 k})$, demonstrating a steep transition from constant factor approximability to inapproximability around $\Omega(k)$. More importantly, using Multiplicative-Weight-Updates (MWU) we find a much faster $\tilde{O}(n/\delta^3)$ time, asymptotic $(1-1/e)^2-\delta$ approximation. While the above results are all randomized, we also give a simple deterministic $(1-1/e)-\epsilon$ approximation with runtime $kn^{m/\epsilon^4}$. Finally, we run synthetic experiments on Kronecker graphs and find that our MWU inspired heuristic outperforms existing heuristics.","RAJAN UDWANI*,MIT",rudwani@mit.edu,Optimization*; Algorithms; Optimization/Combinatorial Optimization; Optimization/Non-Convex Optimization; Opt,Poster
5780,Sanity Checks for Saliency Maps,"Saliency methods have emerged as a popular tool to highlight features in an input
deemed relevant for the prediction of a learned model. Several saliency methods
have been proposed, often guided by visual appeal on image data. In this work, we
propose an actionable methodology to evaluate what kinds of explanations a given
method can and cannot provide. We find that reliance, solely, on visual assessment
can be misleading. Through extensive experiments we show that some existing
saliency methods are independent both of the model and of the data generating
process. Consequently, methods that fail the proposed tests are inadequate for
tasks that are sensitive to either data or model, such as, finding outliers in the data,
explaining the relationship between inputs and outputs that the model learned,
and debugging the model. We interpret our findings through an analogy with
edge detection in images, a technique that requires neither training data nor model.
Theory in the case of a linear model and a single-layer convolutional neural network
supports our experimental findings.","Julius Adebayo*,Google ;Justin Gilmer,Google Brain;Michael Muelly,Google;Ian Goodfellow,Google Brain;Moritz Hardt,Google Brain;Been Kim,Google",juliusadebayonipsreview@gmail.com;gilmer@google.com;mmuelly@google.com;goodfellow@google.com;mrtz@google.com;beenkim@google.com,Deep Learning/Visualization or Exposition Techniques for Deep Networks,Spotlight
5781,Probabilistic Model-Agnostic Meta-Learning,"Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems.","Chelsea Finn*,UC Berkeley;Kelvin Xu,University of California, Berkeley;Sergey Levine,UC Berkeley",cbfinn@eecs.berkeley.edu;kelvinxu@berkeley.edu;svlevine@eecs.berkeley.edu,Deep Learning/Meta-Learning*; Probabilistic Methods/Hierarchical Models,Poster
5798,Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach,"Potential based reward shaping is a powerful technique for accelerating convergence of reinforcement learning algorithms. Typically, such information includes an estimate of the optimal value function and is often provided by a human expert or other sources of domain knowledge. However, this information is often biased or inaccurate and can mislead many reinforcement learning algorithms. In this paper, we apply Bayesian Model Combination with multiple experts in a way which learns to trust the best combination of experts as training progresses. This approach is both computationally efficient and general, and is shown numerically to improve convergence of various reinforcement learning algorithms across many domains.","Mike Gimelfarb*,University of Toronto;Scott Sanner,University of Toronto;Chi-Guhn Lee,University of Toronto",mike.gimelfarb@mail.utoronto.ca;ssanner@mie.utoronto.ca;cglee@mie.utoronto.ca,Reinforcement Learning and Planning/Reinforcement Learning*; Probabilistic Methods/Bayesian Theory; Reinforcement Learning and Planning/Exploration,Poster
5810,e-SNLI: Natural Language Inference with Natural Language Explanations,"In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from natural language explanations. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated free-form explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show that our corpus of explanations can be used for various goals, such as obtaining full sentence justifications of a model's decisions and providing consistent improvements on a range of tasks compared to universal sentence representations learned without explanations. Our dataset opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust.","Oana-Maria Camburu*,University of Oxford;Tim Rocktaschel,Oxford;Thomas Lukasiewicz,University of Oxford;Phil Blunsom,",oana-maria.camburu@cs.ox.ac.uk;tim.rocktaeschel@gmail.com;thomas.lukasiewicz@cs.ox.ac.uk;phil.blunsom@cs.ox.ac.uk,Applications/Natural Language Processing*; Deep Learning,Poster
5828,Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis,"Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covariance matrix they are based on however becomes gigantic, rendering them inapplicable in their original form. This has motivated research into both simple diagonal approximations and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is both provably better than KFAC and that is amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures, both in number of iterations and in wall-clock-time.","Thomas George*,MILA, Universit√© de Montr√©al;C√©sar Laurent,MILA;Xavier Bouthillier,MILA Universit√© de Montr√©al;Nicolas Ballas,Facebook FAIR;Pascal Vincent,Facebook FAIR & MILA Universit√© de Montr√©al",thomas.george@umontreal.ca;cesar.laurent@umontreal.ca;xavier.bouthillier@umontreal.ca;ballasn@fb.com;vincentp@iro.umontreal.ca,Deep Learning/Optimization for Deep Networks*; Deep Learning/Efficient Training Methods,Poster
5839,Learning convex bounds for linear quadratic control policy synthesis,"Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a numbers of fields, from artificial intelligence and robotics, to medicine and finance.
This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function.
We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data.
The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees.
Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.","Jack Umenberger*,Uppsala University;Thomas Sch√∂n,Uppsala University",jack.umenberger@it.uu.se;thomas.schon@it.uu.se,Reinforcement Learning and Planning/Decision and Control*; Optimization/Convex Optimization,Spotlight
5840,Neural Proximal Gradient Descent for Compressive Imaging,"Recovering high-resolution images from limited sensory data typically leads to a serious ill-posed inverse problem, demanding inversion algorithms that effectively capture the prior information. Learning a good inverse mapping from training data faces severe challenges, including: (i) scarcity of training data; (ii) need for  plausible reconstructions that are physically feasible; (iii) need for fast reconstruction, especially in real-time applications. We develop a successful system solving all these challenges, using as basic architecture the repetitive application of alternating proximal and data fidelity constraints. We learn a proximal map that works well with real images based on residual networks with recurrent blocks. Extensive experiments are carried out under different settings: (a) reconstructing abdominal MRI of pediatric patients from highly undersampled k-space data and (b) super-resolving natural face images. Our key findings include: 1. a recurrent ResNet with a single residual block (10-fold repetition) yields an effective proximal which accurately reveals MR image details. 2. Our architecture significantly outperforms conventional non-recurrent deep ResNets by 2dB SNR; it is also trained much more rapidly. 3. It outperforms state-of-the-art compressed-sensing Wavelet-based methods by 4dB SNR, with 100x speedups in reconstruction time.","Morteza Mardani*,Stanford University;Qingyun Sun,Stanford university;David Donoho,Stanford University;Vardan Papyan,Stanford University;Hatef Monajemi,Stanford University;Shreyas Vasanawala,Stanford University;John Pauly,Stanford University",morteza@stanford.edu;qysun@stanford.edu;donoho@stanford.edu;papyan@stanford.edu;monajemi@stanford.edu;vasanawala@stanford.edu;pauly@stanford.edu,Deep Learning*; Algorithms/Sparsity and Compressed Sensing,Poster
5841,To What Extent Do Different Neural Networks Learn the Same Representation: A Neuron Activation Subspace Match Approach,"Studying the learned representations is important for understanding deep neural networks.  In this paper, we investigate the similarity of representations learned by two networks with identical architecture but trained from different initializations.  Instead of resorting to heuristic methods, we develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches.  Finally, experimental study using our algorithms suggests that, somewhat surprisingly, representations learned by the same convolutional layers of two networks are not as similar as prevalently expected.","Liwei Wang*,Peking University;Lunjia Hu,Stanford University;Jiayuan Gu,Peking University;Zhiqiang Hu,Peking University;Yue Wu,Peking University;Kun He,Hua Zhong University of Science and Technology;John  Hopcroft,Cornell University",wanglw@cis.pku.edu.cn;lunjia@stanford.edu;gujiayuan@pku.edu.cn;huzq_pku@126.com;1500012766@pku.edu.cn;brooklet60@gmail.com;jeh@cs.cornell.edu,Deep Learning*; Theory,Spotlight
5855,Optimal Algorithms for Continuous   Non-monotone Submodular and DR-Submodular Maximization,"In this paper we study the fundamental problems of maximizing a continuous non monotone submodular function over a hypercube, with and without coordinate-wise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first 1/2 approximation algorithm for continuous submodular function maximization; this approximation factor of is the best possible for algorithms that use only polynomially many queries.  For the special case of DR-submodular maximization, we provide a different 1/2-approximation algorithm that runs in quasi-linear time. Both of these results improve upon prior work [Bian et al., 2017].
Our first algorithm uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications. ","Rad Niazadeh*,Stanford University, California;Tim Roughgarden,Stanford University;Joshua Wang,Stanford University",rad@cs.stanford.edu;tim@cs.stanford.edu;jrwang@stanford.edu,Optimization/Submodular Optimization*; Optimization/Combinatorial Optimization; Optimization/Non-Convex Optimization; Theory/Game,Oral
5859,An intriguing failing of convolutional neural networks and the CoordConv solution,"Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and coordinates in pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either perfect translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem 150 times faster, with 10-100 times fewer parameters, and with perfect generalization.

This stark contrast leads to a final question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will likely require much follow up work, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. We show that using CoordConv in GANs results in less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. We show small but statistically significant improvements by simply adding a CoordConv layer to ResNet-50, and we show significant improvements in the RL domain by giving agents playing Atari games access to CoordConv layers.","Rosanne Liu*,Uber AI Labs;Joel Lehman,;Piero Molino,Uber AI Labs;Felipe Petroski Such,Uber AI Labs;Eric Frank,Uber AI Labs;Alex Sergeev,Uber Technologies Inc,;Jason Yosinski,Uber AI Labs",rosanne@uber.com;joel.lehman@uber.com;piero@uber.com;felipe.such@uber.com;mysterefrank@uber.com;asergeev@uber.com;jason@yosinski.com,Deep Learning/CNN Architectures*; Algorithms/Classification; Algorithms/Regression; Deep Learning/Attention Models; Deep Lear,Poster
5877,Trading robust representations for sample complexity through self-supervised visual experience,"Learning in small sample regimes is among the most remarkable features of the human perceptual system. This ability is related to robustness to transformations, which is acquired through visual experience in the form of weak- or self-supervision during development. We explore the idea of allowing artificial systems to learn representations of visual stimuli through weak supervision prior to downstream supervised tasks. We introduce a novel loss function for representation learning using unlabeled image sets and video sequences, and experimentally demonstrate that these representations support one-shot learning and reduce the sample complexity of multiple recognition tasks. We establish the existence of a trade-off between the sizes of weakly supervised, automatically obtained from video sequences, and fully supervised data sets. Our results suggest that equivalence sets other than class labels, which are abundant in unlabeled visual experience, can be used for self-supervised learning of semantically relevant image embeddings.","Andrea Tacchetti*,DeepMind;Stephen Voinea,MIT;Georgios Evangelopoulos,X, Alphabet",atacchet@google.com;voinea@mit.edu;gevang@google.com,Deep Learning/Few-Shot Learning Approaches*; Algorithms/Representation Learning; Deep Learning/Deep Autoencoders; Deep Learning/Embeddin,Poster
5878,Invertibility of Convolutional Generative Networks from Partial Measurements,"In this work, we present new theoretical results on convolutional generative neural networks, in particular their invertibility (i.e., the recovery of input latent code given the network output). This inversion problem is highly non-convex, which is in general computationally challenging and has no performance guarantee. However, we rigorously prove that, even when the network output is only partially observed (e.g., with missing pixels), the input of a two-layer convolutional generative network can always be computed from the network output, using simple gradient descent. This new theoretical finding implies that the mapping from the low-dimensional latent space to the high-dimensional image space is bijective (i.e., one-to-one). Our theorem holds for 2-layer convolutional generative network with relu as the activation function, but we demonstrate that the same conclusion empirically extends to multi-layer networks and networks with other activation functions (including the leaky relu, sigmoid and tanh). Our proof is built on our newly proposed permutation technique, which can potentially be generalized to networks with multiple layers and in other theoretical studies on convolutional neural networks, and thus is a merit on its own.","Fangchang Ma*,Massachusetts Institute of Technology;Ulas Ayaz,Massachusetts Institute of Technology;Sertac Karaman,Massachusetts Institute of Technology",fcma@mit.edu;uayaz@mit.edu;sertac@mit.edu,Deep Learning/Generative Models*; Optimization/Non-Convex Optimization,Poster
5940,Ex ante coordination and collusion in zero-sum multi-player extensive-form games,"Recent milestones in equilibrium computation, such as the success of Libratus, show that it is possible to compute strong solutions to two-player zero-sum games in theory and practice. This is not the case for games with multiple players, which remains one of the main open challenges in computational game theory. This paper focuses on zero-sum games where a team of players faces an opponent, as is the case, for example, in Bridge, collusion in poker, and many non-recreational applications such as war, where the colluders do not have time or means of communicating during battle, collusion in bidding, where communication during the auction is illegal, and coordinated swindling in public. The possibility for the team members to communicate before game play‚Äîthat is, coordinate their strategies ex ante‚Äîmakes the use of behavioral strategies unsatisfactory. The reasons for this are closely related to the fact that the team can be represented as a single player with imperfect recall.
We propose a new game representation, the realization form, that generalizes the sequence form but can also be applied to imperfect-recall games. Then, we use it to derive an auxiliary game that is equivalent to the original one. It provides a sound way to map the problem of finding an optimal ex-ante-correlated strategy for the team to the well-understood Nash equilibrium-finding problem in a (larger) two-player zero-sum perfect-recall game. By reasoning over the auxiliary game, we devise an anytime algorithm, Fictitious Team-Play, that is guaranteed to converge to an optimal coordinated strategy for the team against an optimal opponent, and that is dramatically faster than the prior state-of-the-art algorithm for this problem.","Gabriele Farina*,Carnegie Mellon University;Andrea Celli,Politecnico di Milano;Nicola Gatti,Politecnico di Milano;Tuomas Sandholm,Carnegie Mellon University",gfarina@cs.cmu.edu;andrea.celli@polimi.it;nicola.gatti@polimi.it;sandholm@cs.cmu.edu,Theory/Game Theory and Computational Economics,Poster
6015,Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization,"Despite the success of single-agent reinforcement learning, multi-agent reinforcement learning (MARL) remains challenging due to complex interactions between agents. Motivated by decentralized applications such as sensor networks, swarm robotics, and power grids, we study policy evaluation in MARL, where agents with jointly observed state-action pairs and private local rewards collaborate to learn the value of a given policy.  
In this paper, we propose a double averaging scheme, where each agent iteratively performs averaging over both space and time to incorporate neighboring gradient information and local reward information, respectively. We prove that the proposed algorithm converges to the optimal solution at a global geometric rate. In particular, such an algorithm is built upon a primal-dual reformulation of the mean squared Bellman error minimization problem, which gives rise to a decentralized convex-concave saddle-point problem. To the best of our knowledge, the proposed double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems.","Hoi-To Wai*,Arizona State University;Zhuoran Yang,;Zhaoran Wang,Northwestern U;Mingyi Hong,University of Minnesota",htwai@asu.edu;zy6@princeton.edu;zhaoranwang@gmail.com;mhong@umn.edu,Reinforcement Learning and Planning/Multi-Agent RL*; Optimization/Convex Optimization,Poster
6046,Improving Online Algorithms via ML Predictions,"In this work we study the problem of using machine learned predictions to improve performance of online algorithms.  We consider two classical problems, ski rental and non-clairvoyant job scheduling, and obtain new online algorithms that use predictions to make their decisions.  These algorithms are oblivious to the performance of the predictor, improve with better predictions, but do not degrade much if the predictions are poor.","Manish Purohit*,Google;Zoya Svitkina,Google;Ravi Kumar,Google",mpurohit@google.com;zoya@cs.cornell.edu;ravi.k53@gmail.com,Theory/Competitive Analysis*; Algorithms; Optimization/Combinatorial Optimization,Poster
6072,Non-convex Optimization with Discretized Diffusions,"  An Euler discretization of the Langevin diffusion is known to converge
  to the global minimizers of certain convex and non-convex optimization problems.  
  We show that this property holds for any suitably smooth diffusion and
  that different diffusions are suitable for optimizing different classes of convex
  and non-convex functions.
  This allows us to design diffusions suitable for globally optimizing non-convex functions
  not covered by the existing Langevin theory.
  Our non-asymptotic analysis establishes explicit,
  finite-time convergence rates to global minima,
  and is based on a multidimensional version of Stein's method
  with new explicit bounds on the solutions of Poisson equations.","Murat Erdogdu*,University of Toronto;Lester Mackey,Microsoft Research;Ohad Shamir,Weizmann Institute of Science",erdogdu.ma@gmail.com;lmackey@microsoft.com;ohad.shamir@weizmann.ac.il,Optimization/Non-Convex Optimization*; Probabilistic Methods/MCMC,Poster
6129,Theoretical guarantees for EM under misspecified Gaussian mixture models,"Recent years have witnessed substantial progress in understanding
  the behavior of EM for mixture models that are correctly specified.
  Given that model misspecification is common in practice, it is
  important to understand EM in this more general setting.  We provide
  non-asymptotic guarantees for population and sample-based EM for
  parameter estimation under a few specific univariate settings of
  misspecified Gaussian mixture models.  Due to misspecification, the
  EM iterates no longer converge to the true model and instead
  converge to the projection of the true model over the set of models
  being searched over.  We provide two classes of theoretical
  guarantees: first, we characterize the bias introduced due to the
  misspecification; and second, we prove that population EM converges
  at a geometric rate to the model projection under a suitable
  initialization condition.  This geometric convergence rate for
  population EM imply a statistical complexity of order $1/\sqrt{n}$
  when running EM with $n$ samples. We validate our theoretical
  findings in different cases via several numerical examples.","Raaz Dwivedi,UC Berkeley;Nhat Ho,University of California, Berkeley;Koulik Khamaru,University of California Berkeley;Martin Wainwright,UC Berkeley;Michael Jordan*,UC Berkeley",dwivediraaz@gmail.com;minhnhat@berkeley.edu;koulik@berkeley.edu;wainwrig@berkeley.edu;jordan@cs.berkeley.edu,Probabilistic Methods/Hierarchical Models*; Algorithms/Missing Data; Probabilistic Methods/Latent Variable Models; Theory/Computational,Poster
6162,Coupled Variational Bayes via Optimization Embedding,"Variational inference plays a vital role in learning graphical models, especially on large-scale datasets. Much of its success depends on a proper choice of auxiliary distribution class for posterior approximation. However, how to pursue an auxiliary distribution class that achieves both good approximation ability and computation efficiency remains a core challenge. In this paper, we construct such a distribution class, termed optimization embedding, since it takes root in an optimization procedure. This flexible function class couples the variational distribution with the original parameters in the graphical models, allowing end-to-end learning of the graphical models by back-propagation through the variational distribution. Theoretically,  we establish an interesting connection to gradient flow and demonstrate the extremely flexibility of this implicit distribution family in the limit sense.  Practically,  the proposed technique allows to significantly accelerate the learning procedure, i.e., the proposed coupled variational Bayes, by reducing the searching space to a large extent. We further demonstrate the significant superiority of the proposed method on multiple graphical models with either continuous or discrete latent variables comparing to state-of-the-art methods.","Bo Dai*,Google Brain;Hanjun Dai,Georgia Tech;Niao He,University of Illinois at Urbana-Champaign;Weiyang Liu,Georgia Tech;Zhen Liu,Georgia Tech;Jianshu Chen,Microsoft Research;Lin Xiao,Microsoft Research;Le Song,Ant Financial & Georgia Institute of Technology ",bohr.dai@gmail.com;hanjundai@gatech.edu;niaohe@illinois.edu;wyliu@gatech.edu;liuzhen1994@gatech.edu;chenjianshu@gmail.com;lin.xiao@microsoft.com;lsong@cc.gatech.edu,Probabilistic Methods/Variational Inference*; Probabilistic Methods/Graphical Models; Probabilistic Methods/Latent Variable Models,Poster
6174,Improving Explorability in Variational Inference with Annealed Variational Objectives,"Despite the advances in the representational capacity of approximate distributions for variational inference, the optimization process can still limit the density that is ultimately learned.
We demonstrate the drawbacks of biasing the true posterior to be unimodal, and introduce Annealed Variational Objectives (AVO) into the training of hierarchical variational methods.
Inspired by Annealed Importance Sampling, the proposed method facilitates learning by incorporating energy tempering into the optimization objective.
In our experiments, we demonstrate our method's robustness to deterministic warm up, and the benefits of encouraging exploration in the latent space.","Chin-Wei Huang*,MILA;Shawn Tan,University of Montreal;Alexandre Lacoste,;Aaron Courville,MILA, Universit√© de Montr√©al",chin-wei.huang@umontreal.ca;shawn@wtf.sg;allac@elementai.com;aaron.courville@gmail.com,Deep Learning/Generative Models*; Algorithms/Density Estimation; Probabilistic Methods/Variational Inference,Poster
6253,Latent Alignment and Variational Attention,"Attention, a method for learning a soft alignment function embedded in a neural network, is central for many state-of-the-art models in natural language processing and related domains. Attention networks are easy to train and interpret; however, the standard (soft) attention approach is a fully feed-forward approach and does not marginalize over the latent alignment in a traditional sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A popular latent variable approach, hard attention, fixes these issues, but is unfortunately generally less accurate and harder to train. In this work, we explore the space of modern variational inference approaches as alternatives for learning latent variable alignment models. Variational attention generalizes hard attention and can provide a tighter approximation bound. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, while hard attention performs worse than soft attention, exact latent variable models can outperform both. Furthermore variational attention retains almost all of this performance gain with training speed comparable to soft attention.","Yuntian Deng*,Harvard University;Yoon Kim,Harvard University;Justin Chiu,Harvard;Demi Guo,Harvard;Alexander Rush,Harvard",dengyuntian@seas.harvard.edu;yoonkim@seas.harvard.edu;Justinchiu@g.harvard.edu;dguo@college.harvard.edu;srush@seas.harvard.edu,Deep Learning/Attention Models*; Applications/Natural Language Processing,Poster
6383,Towards Deep Conversational Recommendations,"There has been growing interest in using neural networks and deep learning techniques to create dialogue systems. Conversational recommendation is an interesting setting for the scientific exploration of dialogue with natural language as the associated discourse involves goal-driven dialogue that often transforms naturally into more free-form chat. This paper provides two contributions. First, until now there has been no publicly available large-scale data set consisting of real-world dialogues centered around recommendations.
To address this issue and to facilitate our exploration here, we have collected a data set consisting of over 10,000 conversations centered around the theme of providing movie recommendations. We intend to make this data available to the community for further research. Second, we use this dataset to explore multiple facets of  conversational recommendations. In particular we explore new neural architectures, mechanisms and methods suitable for composing conversational recommendation systems. Our dataset allows us to systematically probe model sub-components addressing different parts of the overall problem domain ranging from: sentiment analysis and cold-start recommendation generation to detailed aspects of how natural language is used in this setting in the real world. We combine such sub-components into a full-blown dialogue system and examine its behavior.","Li Raymond*,Polytechnique Montr√©al;Samira Ebrahimi Kahou,Microsoft;Hannes Schulz,Microsoft;Vincent Michalski,University of Montreal;Laurent Charlin,University of Toronto;Christopher Pal,√âcole Polytechnique de Montr√©al ",rlraymondli@gmail.com;samira.ebrahimi.kahou@gmail.com;Hannes.Schulz@microsoft.com;michalskivince@gmail.com;lcharlin@gmail.com;christopher.pal@polymtl.ca,Applications/Dialog- or Communication-Based Learning*; Applications/Recommender Systems; Deep Learning/Generative Models,Poster
6384,"Unsupervised Depth Estimation, 3D Face Rotation and Replacement","We present an unsupervised approach for learning to estimate three dimensional (3D) facial structure from a single image while also predicting 3D viewpoint transformations that match a desired pose and facial geometry.

We achieve this by inferring the depth of facial keypoints of an input image in an unsupervised way. We show how it is possible to use these depths as intermediate computations within a new backproppable loss to predict the parameters of a 3D affine transformation matrix that maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on a desired target facial geometry or pose.

Our resulting approach can therefore be used to infer plausible 3D transformations from one face pose to another, allowing faces to be frontalized, transformed into 3D models or even warped to another pose and facial geometry.

Lastly, we identify certain shortcomings with our formulation, and explore adversarial image translation techniques as a post-processing step to re-synthesize complete head shots for faces re-targeted to different poses or identities.","Joel Ruben Antony Moniz*,Carnegie Mellon University;Christopher Beckham,Ecole Polytechnique de Montreal;Simon Rajotte,Polytechnique Montr√©al;Sina Honari,MILA-University of Montreal;Christopher Pal,√âcole Polytechnique de Montr√©al ",jrmoniz@andrew.cmu.edu;christopher.j.beckham@gmail.com;srajotte@gmail.com;honaris@iro.umontreal.ca;christopher.pal@polymtl.ca,"Applications/Computer Vision*; Algorithms/Unsupervised Learning; Applications/Body Pose, Face, and Gesture Analysis",Poster
6396,Generalization Bounds for Uniformly Stable Algorithms,"  Uniform stability of a learning algorithm is a classical notion of algorithmic stability introduced to derive high-probability bounds on the generalization error (Bousquet and Elisseeff, 2002).  Specifically, for a loss function with range bounded in $[0,1]$, the generalization error of $\gamma$-uniformly stable learning algorithm on $n$ samples is known to be at most $O((\gamma +1/n) \sqrt{n \log(1/\delta)})$ with probability at least $1-\delta$. Unfortunately, this bound does not lead to meaningful generalization bounds in many common settings where $\gamma \geq 1/\sqrt{n}$. At the same time the bound is known to be tight only when $\gamma = O(1/n)$.
  Here we prove substantially stronger generalization bounds for uniformly stable algorithms without any additional assumptions. First, we show that the generalization error in this setting is at most $O(\sqrt{(\gamma + 1/n) \log(1/\delta)})$ with probability at least $1-\delta$. In addition, we prove a tight bound of $O(\gamma^2 + 1/n)$ on the second moment of the generalization error. The best previous bound on the second moment of the generalization error is $O(\gamma + 1/n)$. Our proofs are based on new analysis techniques and our results imply substantially stronger generalization guarantees for several well-studied algorithms.","Vitaly Feldman*,Google;Jan Vondrak,Stanford University",vitaly.edu@gmail.com;jvondrak@stanford.edu,Theory/Learning Theory,Spotlight
6397,Deep Anomaly Detection Using Geometric Transformations,"We consider the problem of anomaly detection in images, and 
present a new detection technique. Given a sample
of images, all known to belong to a ``normal'' class (e.g., dogs), 
we show how to train a deep neural model that can detect 
out-of-distribution images (i.e., non-dog objects). The main 
idea behind our scheme is to train a multi-class model to discriminate between
dozens of geometric transformations applied on all the given images. The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images.
We present extensive experiments using the proposed detector, which indicate that our algorithm improves state-of-the-art methods by a wide margin.","Izhak Golan*,Technion;Ran El-Yaniv,Technion",izikgo89@gmail.com;rani@cs.technion.ac.il,Deep Learning*; Algorithms/Classification; Algorithms/Representation Learning; Algorithms/Unsupervised Lear,Poster
6405,Large Scale computation of Means and Clusters for Persistence Diagrams using Optimal Transport,"Persistence diagrams (PDs) are now routinely used to summarize the underlying topology of sophisticated data encountered in challenging learning problems. Despite several appealing properties, integrating PDs in learning pipelines can be challenging because their natural geometry is not Hilbertian. In particular, algorithms to average a family of PDs have only been considered recently and are known to be computationally prohibitive. We propose in this article a tractable framework to carry out fundamental tasks on PDs, namely evaluating distances, computing barycenters and carrying out clustering. This framework builds upon a formulation of PD metrics as optimal transport (OT) problems, for which recent computational advances, in particular entropic regularization and its convolutional formulation on regular grids, can all be leveraged to provide efficient and (GPU) scalable computations. We demonstrate the efficiency of our approach by carrying out clustering on PDs at scales never seen before in the literature.","Theo Lacombe*,Inria Saclay;marco Cuturi,ENSAE;Steve OUDOT,inria",theo.lacombe@inria.fr;marco.cuturi.cameto@gmail.com;steve.oudot@inria.fr,"Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning*; Algorithms/Clustering; Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA); Algorithms -",Poster
6406,Entropy Rate Estimation for Markov Chains with Large State Space,"Entropy estimation is one of the prototypical problems in distribution property testing. To consistently estimate the Shannon entropy of a distribution on $S$ elements with independent samples, the optimal sample complexity scales sublinearly with $S$ as $\Theta(\frac{S}{\log S})$ as shown by Valiant and Valiant \cite{Valiant--Valiant2011}. Extending the theory and algorithms for entropy estimation to dependent data, this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with $S$ states from a sample path of $n$ observations. We show that
\begin{itemize}
	\item Provided the Markov chain mixes not too slowly, \textit{i.e.}, the relaxation time is at most $O(\frac{S}{\ln^3 S})$, consistent estimation is achievable when $n \gg \frac{S^2}{\log S}$.
	\item Provided the Markov chain has some slight dependency, \textit{i.e.}, the relaxation time is at least $1+\Omega(\frac{\ln^2 S}{\sqrt{S}})$, consistent estimation is impossible when $n \lesssim \frac{S^2}{\log S}$.
\end{itemize}
Under both assumptions, the optimal estimation accuracy is shown to be $\Theta(\frac{S^2}{n \log S})$. In comparison, the empirical entropy rate requires at least $\Omega(S^2)$ samples to be consistent, even when the Markov chain is memoryless. In addition to synthetic experiments, we also apply the estimators that achieve the optimal sample complexity to estimate the entropy rate of the English language in the Penn Treebank and the Google One Billion Words corpora, which provides a natural benchmark for language modeling and relates it directly to the widely used perplexity measure.","Yanjun Han,Stanford University;Jiantao Jiao*,University of California, Berkeley;Chuan-Zheng Lee,Stanford University;Tsachy Weissman,Stanford University;Yihong Wu,Yale University;Tiancheng Yu,Tsinghua University",yjhan@stanford.edu;jiantao@berkeley.edu;czlee@stanford.edu;tsachy@stanford.edu;yihong.wu@yale.edu;thueeyutc14@foxmail.com,Theory/Information Theory*; Theory/Frequentist Statistics,Spotlight
6407,Adaptive Methods for Nonconvex Optimization,"Adaptive gradient methods that rely on scaling gradients down by the square root of exponential moving averages of past squared gradients, such RMSProp, Adam, Adadelta have found wide application in optimizing the nonconvex problems that arise in deep learning. However, it has been recently demonstrated that such methods can fail to converge even in simple convex optimization settings. In this work, we provide a new analysis of such methods applied to nonconvex stochastic optimization problems, characterizing the effect of increasing minibatch size. Our analysis shows that under this scenario such methods do converge to stationarity up to the statistical limit of variance in the stochastic gradients (scaled by a constant factor). In particular, our result implies that increasing minibatch sizes enables convergence,  thus providing a way to circumvent the non-convergence issues. Furthermore, we provide a new adaptive optimization algorithm, Yogi, which controls the increase in effective learning rate,  leading to even better performance with similar theoretical guarantees on convergence. Extensive experiments show that Yogi with very little hyperparameter tuning outperforms methods such as Adam in several challenging machine learning tasks.","Manzil Zaheer,Carnegie Mellon University;Sashank Reddi*,Google;Devendra Sachan,Petuum;Satyen Kale,Google;Sanjiv Kumar,Google Research",manzil@cmu.edu;sashank@google.com;sachan.devendra@gmail.com;satyenkale@google.com;sanjivk@google.com,Optimization/Non-Convex Optimization*; Optimization/Convex Optimization,Poster
6410,Object-Oriented Dynamics Predictor,"Generalization has been one of the major challenges for learning dynamics models in model-based reinforcement learning. However, previous work on action-conditioned dynamics prediction focuses on learning the pixel-level motion and thus does not generalize well to novel environments with different object layouts. In this paper, we present a novel object-oriented framework, called object-oriented dynamics predictor (OODP), which decomposes the environment into objects and predicts the dynamics of objects conditioned on both actions and object-to-object relations. It is an end-to-end neural network and can be trained in an unsupervised manner. To enable the generalization ability of dynamics learning, we design a novel CNN-based relation mechanism that is class-specific (rather than object-specific) and exploits the locality principle. Empirical results show that OODP significantly outperforms previous methods in terms of generalization over novel environments with various object layouts. OODP is able to learn from very few environments and accurately predict dynamics in a large number of unseen environments. In addition, OODP learns semantically and visually interpretable dynamics models.","Guangxiang Zhu*,Tsinghua university;Zhiao Huang,Tsinghua University;Chongjie Zhang,Tsinghua University",guangxiangzhu@outlook.com;hza14@mails.tsinghua.edu.cn;chongjie@tsinghua.edu.cn,Reinforcement Learning and Planning/Model-Based RL,Poster
6418,Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models,"We introduce a method which enables a recurrent dynamics model to be temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI), is based on the observation that in many sequential prediction tasks, the exact time at which events occur is irrelevant to the underlying objective. Moreover, in many situations, there exist prediction intervals which result in particularly easy-to-predict transitions. We show that there are prediction tasks for which we gain both computational efficiency and prediction accuracy by allowing the model to make predictions at a sampling rate which it can choose itself.","Alexander Neitz*,Max Planck Institute for Intelligent Systems;Giambattista Parascandolo,MPI for Intelligent Systems;Stefan Bauer,MPI IS;Bernhard Sch√∂lkopf,Max Planck Institute for Intelligent Systems",alexander.neitz@tuebingen.mpg.de;gparascandolo@tue.mpg.de;stefan.bauer@tuebingen.mpg.de;bernhard.schoelkopf@tuebingen.mpg.de,Reinforcement Learning and Planning/Model-Based RL*; Deep Learning/Predictive Models,Poster
6426,Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation,"While recent developments in autonomous vehicle (AV) technology highlight substantial progress, we lack the tools for rigorous, scalable testing that are so important for these safety-critical systems. Real-world testing, the \textit{de facto} evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles to validate empirically. We implement a simulation framework, \emph{pseudoreality}, which can put an entire autonomous driving system under test -- this includes deep-learning based perception and control algorithms, but also the underlying dynamics models and near-photo-realistic rendering engine required to complete the loop.  While complete systems of this complexity are currently intractable for formal verification, we demonstrate full-scale testing using a \emph{risk-based framework} where our goal is to evaluate the probability of an accident under a base distribution governing standard traffic behavior. Further, we address fundamental challenges in the sample complexity of risk evaluation through the use of adaptive importance-sampling methods. We demonstrate our framework on a highway scenario, showing that it is possible to accelerate system evaluation by $10$-$50 \mathsf{P}$ times that of real-world testing ($\mathsf{P}$ is the number of processors), and $1.5$-$5$ times that of
naive Monte Carlo sampling methods.","Matthew O'Kelly*,University of Pennsylvania;Aman Sinha,Stanford University;Hongseok Namkoong,Stanford University;Russ Tedrake,MIT;John Duchi,Stanford University",mokelly@seas.upenn.edu;amans@stanford.edu;hnamk@stanford.edu;russt@mit.edu;jduchi@stanford.edu,"Data, Competitions, Implementations, and Software*; Data, Competitions, Implementations, and Software/Benchmarks; Data, Competitions, Implementations",Poster
6428,Reinforcement Learning for Solving the Vehicle Routing Problem,"We present an end-to-end framework for solving the Vehicle Routing Problem (VRP) using reinforcement learning. In this approach, we train a single model that finds near-optimal solutions for problem instances sampled from a given distribution, only by observing the reward signals and following feasibility rules. Our model represents a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. On capacitated VRP, our approach outperforms classical heuristics and Google's OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality. Our proposed framework can be applied to other variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems. ","MohammadReza Nazari*,Lehigh University;Afshin Oroojlooy,Lehigh University;Lawrence Snyder,Lehigh University;Martin Takac,Lehigh University",mon314@lehigh.edu;oroojlooy@gmail.com;lvs2@lehigh.edu;martin.taki@gmail.com,Reinforcement Learning and Planning*; Optimization/Combinatorial Optimization; Reinforcement Learning and Planning/Reinforcement Lea,Poster
6429,ATOMO: Communication-efficient Learning via Atomic Sparsification,"Distributed model training suffers from communication overheads due to frequent gradient updates transmitted between compute nodes. To mitigate these overheads, several studies propose the use of sparsified stochastic gradients. We argue that these are facets of a general sparsification method that can operate on any possible atomic decomposition. Notable examples include element-wise, singular value, and Fourier decompositions. We present ATOMO, a general framework for atomic sparsification of stochastic gradients. Given a gradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random unbiased sparsification of the atoms minimizing variance. We show that methods such as QSGD and TernGrad are special cases of ATOMO and show that sparsifiying gradients in their singular value decomposition (SVD), rather than the coordinate-wise one, can lead to significantly faster distributed training. ","Hongyi Wang,University of Wisconsin-Madison;Scott Sievert,University of Wisconsin-Madison;Shengchao Liu,UW-Madison;Zachary Charles*,University of Wisconsin - Madison;Dimitris Papailiopoulos,University of Wisconsin-Madison;Stephen Wright,University of Wisconsin-Madison",hongyiwang@cs.wisc.edu;stsievert@wisc.edu;shengchao@cs.wisc.edu;zcharles@wisc.edu;dimitris@papail.io;swright@cs.wisc.edu,Optimization*; Applications/Communication- or Memory-Bounded Learning; Deep Learning/Optimization for Deep Ne,Poster
6431,Dynamic Network Model from Partial Observations,"Can evolving networks be inferred and modeled without directly observing their nodes and edges? In many applications, the edges of a dynamic network might not be observed, but one can observe the dynamics of stochastic cascading processes (e.g., information diffusion, virus propagation) occurring over the unobserved network. While there have been efforts to infer networks based on such data, providing a generative probabilistic model that is able to identify the underlying time-varying network remains an open question. Here we consider the problem of inferring generative dynamic network models based on network cascade diffusion data. We propose a novel framework for providing a non-parametric dynamic network model---based on a mixture of coupled hierarchical Dirichlet processes---based on data capturing cascade node infection times. Our approach allows us to infer the evolving community structure in networks and to obtain an explicit predictive distribution over the edges of the underlying network---including those that were not involved in transmission of any cascade, or are likely to appear in the future. We show the effectiveness of our approach using extensive experiments on synthetic as well as real-world networks.","Elahe Ghalebi*,TU Wien;Baharan Mirzasoleiman,Stanford University;Radu Grosu,TU Wien;Jure Leskovec,Stanford",elahe.ghalebi@cps.tuwien.ac.at;baharanm@stanford.edu;radu.grosu@tuwien.ac.at;jure@cs.stanford.edu,Probabilistic Methods/Bayesian Nonparametrics*; Applications/Computational Social Science; Applications/Information Retrieval; Applications ->,Spotlight
6432,Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies,"Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.","Alessandro Achille*,UCLA;Tom Eccles,DeepMind;Loic Matthey,DeepMind;Christopher Burgess,DeepMind;Nick Watters,DeepMind;Alexander Lerchner,DeepMind;Irina Higgins,DeepMind",achille@cs.ucla.edu;eccles@google.com;lmatthey@google.com;cpburgess@google.com;nwatters@google.com;lerchner@google.com;irinah@google.com,Algorithms/Unsupervised Learning*; Algorithms/Multitask and Transfer Learning; Algorithms/Representation Learning; Deep Learning,Spotlight
6437,Maximizing acquisition functions for Bayesian optimization,"Bayesian optimization is a sample-efficient approach for global optimization and relies on acquisition functions to guide the search process. Maximizing these functions is inherently complicated, especially in the parallel setting, where acquisition functions are routinely non-convex, high-dimensional and intractable. We present two modern approaches for maximizing acquisition functions and show that 1) sample-path derivatives can be used to optimize acquisition functions and 2)  parallel formulations of many acquisition functions are submodular and can therefore be efficiently maximized in greedy fashion with guaranteed near-optimality.","James Wilson*,Imperial College of London;Frank Hutter,University of Freiburg;Marc Deisenroth,Imperial College London",jwilson1020@gmail.com;fh@cs.uni-freiburg.de;m.deisenroth@imperial.ac.uk,Algorithms/AutoML*; Algorithms/Bandit Algorithms; Probabilistic Methods/Gaussian Processes,Poster
6438,On Markov Chain Gradient Descent,"Stochastic gradient methods are the workhorse (algorithms) of large-scale optimization problems in machine learning, signal processing, and other computational sciences and engineering. This paper studies Markov chain gradient descent, a variant of stochastic gradient descent where the random samples are taken on the trajectory of a Markov chain. Existing results of this method assume convex objectives and a reversible Markov chain and thus have their limitations. We establish new non-ergodic convergence under wider step sizes, for nonconvex problems, and for non-reversible finite-state Markov chains. Nonconvexity makes our method applicable to broader problem classes. Non-reversible finite-state Markov chains, on the other hand, can mix substatially faster. To obtain these results, we introduce a new technique that varies the mixing levels of the Markov chains. The reported numerical results validate our contributions.","Tao Sun,National U. of Defense Tech.;Yuejiao Sun,University of California, Los Angeles;Wotao Yin*,University of California, Los Angeles",nudtsuntao@163.com;sunyj@math.ucla.edu;wotaoyin@math.ucla.edu,Optimization/Non-Convex Optimization*; Algorithms/Online Learning,Poster
6442,Variance-Reduced Stochastic Gradient Descent on Streaming Data,"We present an algorithm STRSAGA for efficiently maintaining a machine learning
model over constantly arriving streaming data that can quickly update the model as
new training data is observed. We present a competitive analysis comparing the sub
optimality of the model maintained by STRSAGA with that of an offline algorithm
that is given the entire data beforehand, and analyze the risk-competitiveness of
STRSAGA under different arrival patterns. Our theoretical and experimental results
show that the risk of STRSAGA is comparable to that of offline algorithms on a
variety of input arrival patterns, and its experimental performance is significantly
better than prior algorithms on streaming data, such as SSVRG.","Ellango Jothimurugesan,CMU;Ashraf Tahmasbi,Iowa State University;Phillip Gibbons,CMU;Srikanta Tirthapura*,Iowa State University",ellango@cmu.edu;tahmasbi@iastate.edu;gibbons@cs.cmu.edu;snt@iastate.edu,Theory/Competitive Analysis*; Algorithms/Online Learning,Poster
6444,Online Robust Policy Learning in the Presence of Unknown Adversaries,"The growing prospect of deep reinforcement learning (DRL) being used in cyber-physical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments.","Aaron Havens,Iowa State University;Zhanhong Jiang,Iowa State University;Soumik Sarkar*,Iowa State University",ajhavens@iastate.edu;zhjiang@iastate.edu;soumiks@iastate.edu,Reinforcement Learning and Planning/Reinforcement Learning*; Deep Learning/Meta-Learning; Reinforcement Learning and Planning/Hierarchical RL,Poster
6456,Uplift Modeling from Separate Labels,"Uplift modeling is aimed at estimating the incremental impact of an action on an individual's behavior, which is useful in various application domains such as targeted marketing (advertisement campaigns) and personalized medicine (medical treatments). Conventional methods of uplift modeling require every instance to be jointly equipped with two types of labels: the taken action and its outcome. However, obtaining two labels for each instance at the same time is difficult or expensive in many real-world problems. In this paper, we propose a novel method of uplift modeling that is applicable to a more practical setting where only one type of labels is available for each instance. We provide a generalization error bound of the proposed method and demonstrate its effectiveness through experiments.","Ikko Yamane*,The University of Tokyo/RIKEN;Florian Yger,Universit√© Paris-Dauphine;Jamal Atif,Universit√© Paris-Dauphine;Masashi Sugiyama,RIKEN/The University of Tokyo",yamane@ms.k.u-tokyo.ac.jp;florian.yger@dauphine.fr;jamal.atif@dauphine.fr;sugi@k.u-tokyo.ac.jp,Probabilistic Methods/Causal Inference*; Algorithms/Semi-Supervised Learning,Poster
6457,Learning Invariances using the Marginal Likelihood,"In many supervised learning tasks, learning what changes do not affect the predic-tion target is as crucial to generalisation as learning what does. Data augmentationis a common way to enforce a model to exhibit an invariance: training data is modi-fied according to an invariance designed by a human and added to the training data.We argue that invariances should be incorporated the model structure, and learnedusing themarginal likelihood, which can correctly reward the reduced complexityof invariant models. We incorporate invariances in a Gaussian process, due to goodmarginal likelihood approximations being available for these models. Our maincontribution is a derivation for a variational inference scheme for invariant Gaussianprocesses where the invariance is described by a probability distribution that canbe sampled from, much like how data augmentation is implemented in practice","Mark van der Wilk*,PROWLER.io;Matthias Bauer,MPI Intelligent Systems/University of Cambridge;ST John,PROWLER.io;James Hensman,PROWLER.io",mark@prowler.io;msb55@cam.ac.uk;st@prowler.io;james@prowler.io,Probabilistic Methods/Gaussian Processes*; Algorithms/Kernel Methods; Probabilistic Methods/Bayesian Theory; Probabilistic Methods/Var,Poster
6460,Non-delusional Q-learning and Value-iteration,"We identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Since standard Q-updates make globally uncoordinated action choices with respect to any policy class, inconsistent or even conflicting Q-value estimates can result, leading to pathological behaviour such as over/under-estimation and even divergence. To solve this problem, we introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets‚Äîsets that record constraints on policies consistent with backed-up Q-values. We prove that model-based and model-free algorithms using this backup fully resolve delusional bias, yielding the first known algorithms that can guarantee optimal results under general conditions. These algorithms only require polynomially many information sets (from a potentially exponential support). Finally, we suggest other heuristics for value-iteration and Q-learning that attempt to reduce this bias.","Tyler Lu*,Google;Dale Schuurmans,Google Inc.;Craig Boutilier,Google Research",tyler.lu@gmail.com;schuurmans@google.com;cboutilier@google.com,Reinforcement Learning and Planning*; Reinforcement Learning and Planning/Decision and Control; Reinforcement Learning and Planning ->,Oral
6461,Using Large Ensembles of Control Variates for Variational Inference,"Variational inference is increasingly being addressed with stochastic optimization. While control variates are commonly used to reduce stochastic gradient variance, they are typically looked at in isolation. This paper clarifies the large number of control variates that are available by giving a systematic view of how they are derived. We give a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the speed and robustness of inference over using any single control variate in isolation.","Tomas Geffner*,UMass Amherst;Justin Domke,UMass Amherst",tomasgeffner@gmail.com;domke@cs.umass.edu,Probabilistic Methods/Variational Inference,Poster
6467,Post: Device Placement with Cross-Entropy Minimization and Proximal Policy Optimization,"Training deep neural networks requires an exorbitant amount of computation resources, including a heterogeneous mix of GPU and CPU devices. It is critical to place operations in a neural network on these devices in an optimal way, so that the training process can complete within the shortest amount of time. The state-of-the-art uses reinforcement learning to learn placement skills by repeatedly performing Monte-Carlo experiments. However, due to its equal treatment of placement samples, we argue that there remains ample room for significant improvements. In this paper, we propose a new joint learning algorithm, called Post, that integrates cross-entropy minimization and proximal policy optimization to achieve theoretically guaranteed optimal efficiency. In order to incorporate the cross-entropy method as a sampling technique, we propose to represent placements using discrete probability distributions, which allows us to estimate an optimal probability mass by maximal likelihood estimation, a powerful tool with the best possible efficiency. We have implemented Post in the Google Cloud platform, and our extensive experiments with several popular neural network benchmarks have demonstrated clear evidence of superior performance: with the same amount of learning time, it leads to placements that have training times up to 63.7% shorter over the state-of-the-art.","Yuanxiang Gao*,University of Toronto and UESTC;Li Chen,University of Toronto;Baochun Li,University of Toronto",yuanxiang@ece.utoronto.ca;lchen@ece.utoronto.ca;bli@ece.toronto.edu,Deep Learning/Optimization for Deep Networks*; Algorithms/Density Estimation; Optimization/Combinatorial Optimization; Reinforcement Learning,Poster
6471,Learning to Reason with Third Order Tensor Products,"We combine Recurrent Neural Networks with Tensor Product Representations to learn ‚Äúnear-symbolic,‚Äù interpretable, combinatorial representations of sequential data. 
The new architecture is trained end-to-end through gradient descent on a variety of natural language reasoning tasks, outperforming current state-of-the-art models in joint and single task settings.  
When training and test data exhibit systematic differences, it generalises more systematically than previous state-of-the-art methods.","Imanol Schlag*,IDSIA;J√ºrgen  Schmidhuber,IDSIA - Lugano",imanol@idsia.ch;juergen@idsia.ch,Deep Learning/Recurrent Networks,Poster
6474,Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing,"This paper presents MAPO: a novel policy optimization formulation that incorporates a memory buffer of promising trajectories to reduce the variance of policy gradient estimates for deterministic environments with discrete actions. The formulation expresses the expected return objective as a weighted sum of two terms: an expectation over a memory of trajectories with high rewards, and a separate expectation over the trajectories outside the memory. We propose 3 techniques to make an efficient training algorithm for MAPO: (1) distributed sampling from inside and outside memory with an actor-learner architecture; (2) a marginal likelihood constraint over the memory to initiate training; (3) systematic exploration to discover new high reward trajectories. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with a sparse reward. We evaluate MAPO on weakly supervised program synthesis from natural language /  semantic parsing. On the WikiTableQuestions benchmark we improve the state-of-the-art by 2.5%, achieving an accuracy of 46.2%, and on the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming the state-of-the-art with full supervision. ","Chen Liang*,Northwestern University;Mohammad Norouzi,Google Brain;Jonathan Berant,Tel Aviv University;Quoc Le,Google Brain;Ni Lao,SayMosaic Inc.",crazydonkey200@gmail.com;mnorouzi@google.com;joberant@cs.tau.ac.il;qvl@google.com;noon99@gmail.com,Reinforcement Learning and Planning/Reinforcement Learning*; Applications/Natural Language Processing; Deep Learning/Program Induction,Spotlight
6481,Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams,"Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes, linked twist maps, and material data. Among them, \textit{persistent homology} is a well-known tool to extract robust topological features, and outputs as \textit{persistence diagrams} (PDs). However, PDs are point multi-sets which can not be used in machine learning algorithms for vector data. To deal with it, an emerged approach is to use kernel methods, and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the \textit{Wasserstein metric}. However, Wasserstein distance is not \textit{negative definite}. Thus, it is limited to build positive definite kernels upon the Wasserstein distance \textit{without approximation}. In this work, we rely upon the alternative \textit{Fisher information geometry} to propose a positive definite kernel for PDs \textit{without approximation}, namely the Persistence Fisher (PF) kernel. Then, we analyze eigensystem of the integral operator induced by the proposed kernel for kernel machines. Based on that, we derive generalization error bounds via covering numbers and Rademacher averages for kernel machines with the PF kernel. Additionally, we show some nice properties such as stability and infinite divisibility for the proposed kernel. Furthermore, we also propose a linear time complexity over the number of points in PDs for an approximation of our proposed kernel with a bounded error. Throughout experiments with many different tasks on various benchmark datasets, we illustrate that the PF kernel compares favorably with other baseline kernels for PDs.","Tam Le*,RIKEN AIP;Makoto Yamada,RIKEN AIP",tam.le@riken.jp;makoto.yamada@riken.jp,Algorithms/Kernel Methods*; Algorithms/Classification; Applications/Object Recognition,Poster
6483,Neural Voice Cloning with a Few Samples,"Voice cloning is a highly desired feature for personalized speech interfaces. We introduce a neural voice cloning system that learns to synthesize a person's voice from only a few audio samples. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model. Speaker encoding is based on training a separate model to directly infer a new speaker embedding, which will be applied to a multi-speaker generative model. In terms of naturalness of the speech and similarity to the original speaker, both approaches can achieve good performance, even with a few cloning audios.  While speaker adaptation can achieve slightly better naturalness and similarity, cloning time and required memory for the speaker encoding approach are significantly less, making it more favorable for low-resource deployment.","Sercan Arik,Baidu Research;Jitong Chen*,Baidu Research;Kainan Peng,;Wei Ping,Baidu Research;Yanqi Zhou,Baidu SVAIL",sercanarik@gmail.com;jitongc@gmail.com;pengkainan@baidu.com;pingwei01@baidu.com;zhouyanqi@baidu.com,Applications/Audio and Speech Processing*; Deep Learning,Spotlight
6487,Blind Deconvolutional Phase Retrieval via Convex Programming,"We consider the task of recovering two real or complex $m$-vectors from phaseless Fourier measurements of their circular convolution.  Our method is a novel convex relaxation that is based on a lifted matrix recovery formulation that allows a nontrivial convex relaxation of the bilinear measurements from convolution.    We prove that if  the two signals belong to known random subspaces of dimensions $k$ and $n$, then they can be recovered up to the inherent scaling ambiguity with $m  >> (k+n) \log^2 m$  phaseless measurements.  Our method provides the first theoretical recovery guarantee for this problem by a computationally efficient algorithm and does not require a solution estimate to be computed for initialization. Our proof is based Rademacher complexity estimates.  Additionally, we provide an ADMM implementation of the method and provide numerical experiments that verify the theory.","Ali Ahmed*,Information Technology University;Alireza Aghasi,Georgia State University;Paul Hand,Northeastern University",ali.ahmed@itu.edu.pk;aaghasi@gsu.edu;p.hand@northeastern.edu,Theory/Information Theory*; Applications/Signal Processing; Optimization/Convex Optimization,Spotlight
6490,Scalable Laplacian K-modes,"We advocate Laplacian K-modes for joint clustering and density mode finding, and propose a concave-convex relaxation of the problem, which yields a parallel algorithm that scales up to large datasets and high dimensions. We optimize a tight bound (auxiliary function) of our relaxation, which, at each iteration, amounts to computing an independent update for each cluster-assignment variable, with guaranteed convergence. Therefore, our bound optimizer can be trivially distributed for large-scale data sets. Furthermore, we show that the density modes can be obtained as byproducts of the assignment variables via simple maximum-value operations whose additional computational cost is linear in the number of data points. Our formulation does not need storing a full affinity matrix and computing its eigenvalue decomposition, neither does it perform expensive projection steps and Lagrangian-dual inner iterates for the simplex constraints of each point. Furthermore, unlike mean-shift, our density-mode estimation does not require inner-loop gradient-ascent iterates. It has a complexity independent of feature-space dimension, yields modes that are valid data points in the input set and is applicable to discrete domains as well as arbitrary kernels. We report comprehensive experiments over various data sets, which show that our algorithm yields very competitive performances in term of optimization quality (i.e., the value of the discrete-variable objective at convergence) and clustering accuracy.","Imtiaz Ziko*,Ecole de technologie superieure;Eric Granger,√âcole de technologie sup√©rieure, Universit√© du Qu√©bec;Ismail Ben Ayed,Canada",ziko.iut@gmail.com;Eric.Granger@etsmtl.ca;ismail.benayed@gmail.com,Algorithms/Clustering*; Optimization,Spotlight
6491,A Retrieve-and-Edit Framework for Predicting Structured Outputs,"Generic sequence-to-sequence models have trouble generating outputs with highly-structured dependencies such as source code.
Motivated by the observation that editing is easier than writing from scratch, we propose a general retrieve-and-edit paradigm that can leverage any base sequence-to-sequence model: given a test input, we first retrieve a training example and then edit the retrieved output into the final predicted output using the base model. The key challenge is to efficiently learn a retriever that is sensitive to the prediction task. We propose first learning a joint variational autoencoder over input-output pairs and then regressing a conditional retriever on the joint embeddings. On the Hearthstone cards benchmark, we show that applying the retrieve-and-edit paradigm to a vanilla sequence-to-sequence model results in BLEU scores approaching those of specialized AST-based code generation models. Additionally, we introduce a new autocomplete task on Python code from GitHub, on which we demonstrate the benefits of retrieve-and-edit.","Tatsunori Hashimoto*,Stanford;Kelvin Guu,Stanford;Yonatan Oren,Stanford;Percy Liang,",tatsu23456@gmail.com;kguu@stanford.edu;yonatano@stanford.edu;pliang@cs.stanford.edu,Applications/Natural Language Processing*; Deep Learning/Generative Models,Oral
6492,Testing for Families of Distributions via the Fourier Transform,"We study the general problem of testing whether an unknown discrete distribution belongs to a specified family of distributions. More specifically, given a distribution family P and sample access to an unknown discrete distribution D , we want to distinguish (with high probability) between the case that D in P and the case that D is Œµ-far, in total variation distance, from every distribution in P . This is the prototypical hypothesis testing problem that has received significant attention in statistics and, more recently, in computer science. The main contribution of this work is a simple and general testing technique that is applicable to all distribution families whose Fourier spectrum satisfies a certain approximate sparsity property. We apply our Fourier-based framework to obtain near sample-optimal and  computationally efficient testers for the following fundamental distribution families: Sums of Independent Integer Random Variables (SIIRVs), Poisson Multinomial Distributions (PMDs), and Discrete Log-Concave Distributions. For the first two, ours are the first non-trivial testers in the literature, vastly generalizing previous work on testing Poisson Binomial Distributions. For the third, our tester improves on prior work in both sample and time complexity.","Alistair Stewart*,University of Southern California;Ilias Diakonikolas,USC;Clement Canonne,Stanford University",stewart.al@gmail.com;iliasdiakonikolas@gmail.com;ccanonne@stanford.edu,Theory/Learning Theory*; Algorithms,Poster
6494,Thwarting Adversarial Examples: An $L_0$-Robust Sparse Fourier Transform,"We give a new algorithm for approximating the Discrete Fourier transform of an
 approximately sparse signal that is robust to worst-case $L_0$ corruptions, namely that some coordinates of the signal can be corrupt arbitrarily. Our techniques generalize to a wide range of linear transformations that are used in data analysis such as the Discrete Cosine and Sine transforms, the Hadamard transform, and their high-dimensional analogs. We use our algorithm to successfully defend
 against worst-case $L_0$ adversaries in the setting of image classification. We give
 experimental results on the Jacobian-based Saliency Map Attack (JSMA) and
 the CW $L_0$ attack on the MNIST and Fashion-MNIST datasets as well as the
 Adversarial Patch on the ImageNet dataset.","Mitali Bafna,Harvard University;Jack Murtagh,Harvard University;Nikhil Vyas*,MIT",mitalibafna@g.harvard.edu;jmurtagh@g.harvard.edu;vyasnikhil96@gmail.com,Deep Learning*; Algorithms/Sparsity and Compressed Sensing,Poster
6497,Blockwise Parallel Decoding for Deep Autoregressive Models,"Deep autoregressive sequence-to-sequence models have demonstrated impressive performance across a wide variety of tasks in recent years. While several common architecture classes including recurrent, convolutional, and self-attention networks make different trade-offs between the amount of computation needed per layer and the length of the critical path at training time, inference for novel inputs still remains an inherently sequential process. We propose a novel blockwise parallel decoding scheme that takes advantage of the fact that some architectures can score sequences in sublinear time. By generating predictions for multiple time steps at once then backing off to the longest prefix validated by the scoring model, we can substantially improve the speed of greedy decoding without compromising performance. When tested on state-of-the-art self-attention models for machine translation and image super-resolution, our approach achieves iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality. Relaxing the acceptance criterion and fine tuning model parameters allows for reductions of up to 7x in exchange for a slight decrease in performance. Our fastest models achieve a 4x speedup in wall-clock time.","Mitchell Stern*,UC Berkeley;Noam Shazeer,Google;Jakob Uszkoreit,",mitchell@berkeley.edu;noam@google.com;uszkoreit@google.com,Deep Learning/Efficient Inference Methods,Poster
6498,Low-rank Tucker decomposition of large tensors using TensorSketch,"We propose two randomized algorithms for low-rank Tucker decompositions of tensors. The algorithms, which incorporate sketching, only require a single pass of the input tensor and can handle tensors whose elements are streamed in any order. To the best of our knowledge, ours are the only algorithms which can do this. We test our algorithms on sparse synthetic data and compare them to multiple other methods. We also apply one of our algorithms to a real dense 38 GB tensor representing a video and use the resulting decomposition to correctly classify frames containing disturbances.","Osman Asif Malik*,University of Colorado Boulder;Stephen Becker,University of Colorado",osman.malik.87@gmail.com;stephen.becker@colorado.edu,Applications/Matrix and Tensor Factorization*; Probabilistic Methods,Poster
6499,A Simple Cache Model for Image Recognition,"Training large-scale image recognition models is computationally expensive. This raises the question of whether there might be simple ways to improve the test performance of an already trained model without having to re-train or even fine-tune it with new data. Here, we show that, surprisingly, this is indeed possible. The key observation we make is that the layers of a deep network close to the output layer contain independent, easily extractable class-relevant information that is not contained in the output layer itself. We propose to extract this extra class-relevant information using a simple key-value cache memory to improve the classification performance of the model at test time. Our cache memory is directly inspired by a similar cache model previously proposed for language modeling (Grave et al., 2017). This cache component does not require any training or fine-tuning; it can be applied to any pre-trained model and, by properly setting only two hyper-parameters, leads to significant improvements in its classification performance. Improvements are observed across several architectures and datasets. In the cache component, using features extracted from layers close to the output (but not from the output layer itself) as keys leads to the largest improvements. Concatenating features from multiple layers to form keys can further improve performance over using single-layer features as keys. The cache component also has a regularizing effect, a simple consequence of which is that it substantially increases the robustness of models against adversarial attacks.","Emin Orhan*,Rice University",aeminorhan@gmail.com,Deep Learning*; Algorithms/Classification; Applications/Object Recognition; Deep Learning/Adversarial Netwo,Poster
6510,Clebsch‚ÄìGordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network,"Recent work by Cohen et al. has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch--Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.","Risi Kondor,U. Chicago;Zhen Lin,The University of Chicago;Shubhendu Trivedi*,Toyota Technological Institute",risi@cs.uchicago.edu;zlin7@uchicago.edu;shubhendu@cs.uchicago.edu,Deep Learning*; Deep Learning/CNN Architectures; Deep Learning/Efficient Training Methods; Deep Learning/Su,Poster
6512,Bayesian Nonparametric Spectral Estimation,"Spectral estimation (SE) aims to identify how the energy of a signal (e.g., time series) is distributed across different frequencies. This is a challenging task when only partial and noisy observations are available, where current methods fail to find expressive representations of the data and handle uncertainty appropriately. In this context, we propose a joint probabilistic model for signals, observations and spectra, where  SE is addressed as an inference problem. Assuming a Gaussian process prior over the signal, we apply Bayes' rule to find the analytic posterior distribution of the spectrum given a set of observations. Besides its expressiveness and natural ability to represent spectral uncertainty, the proposed model provides a functional-form estimate of the power spectral density which can be optimised efficiently. We include a comparison to previous methods for SE and validation on three experiments using synthetic and real-world data. ","Felipe Tobar*,Universidad de Chile",ftobar@dim.uchile.cl,Applications/Time Series Analysis*; Probabilistic Methods/Bayesian Theory; Probabilistic Methods/Gaussian Processes; Probabilistic,Spotlight
6514,A Spectral View of Adversarially Robust Features,"Given the apparent difficulty of learning models that are robust to adversarial perturbations, we propose tackling the simpler problem of developing adversarially robust features.  Specifically, given a dataset and metric of interest, the goal is to return a function (or multiple functions) that 1) is robust to adversarial perturbations, and 2) has significant variation across the datapoints.  We establish strong connections between adversarially robust features, and a natural spectral property of the geometry of the dataset and metric of interest.  This connection can be leveraged both to provide robust features, and to provide a lower bound on the robustness of any function that has significant variance across the dataset.  Finally, we provide empirical evidence that the adversarially robust features yielded via this spectral approach can be be fruitfully leveraged to learn a robust (and accurate) model.","Shivam Garg,Stanford University;Vatsal Sharan,Stanford University;Brian Zhang*,Stanford University;Gregory Valiant,Stanford University",shivamgarg55@gmail.com;vsharan@stanford.edu;zhangbrianh@gmail.com;gregory.valiant@gmail.com,"Applications/Privacy, Anonymity, and Security*; Algorithms; Algorithms/Spectral Methods; Deep Learning; Theory/Learning Theory",Spotlight
6523,Synaptic Strength For Convolutional Neural Network,"Convolutional Neural Networks(CNNs) are both computation and memory intensive which hindered their deployment in many resource efficient devices. 
Inspired by neural science research, we propose the synaptic pruning: a data-driven method to prune connections between convolution layers with a newly proposed class of parameters called Synaptic Strength. Synaptic Strength is designed to capture the importance of a synapse based on the amount of information it transports. Experimental results show the effectiveness of our approach empirically. On CIFAR-10, we can prune various CNN models with up to $96\%$ connections removed, which results in significant size reduction and computation saving. Further evaluation on ImageNet demonstrates that synaptic pruning is able to discover efficient models which are competitive to state-of-the-art compact CNNs such as MobileNet-V$2$ and NasNet-Mobile. Our contribution is summarized as follows: (1) We introduce Synaptic Strength, a new class of parameters for convolution layer to indicate the importance of each connection. (2) Our approach can prune various CNN models with high compression without compromising accuracy. (3) Further investigation shows, the proposed Synaptic Strength is a better indicator for kernel pruning compare with the previous approach both in empirical results and theoretical analysis.
    ","CHEN LIN*,Sensetime;Zhao Zhong,CASIA;Wu Wei,Sensetime;Junjie Yan,Sensetime Group Limited",linchen@sensetime.com;zhao.zhong@nlpr.ia.ac.cn;wuwei@sensetime.com;yanjunjie@sensetime.com,Deep Learning/Biologically Plausible Deep Networks,Poster
6531,Human-in-the-Loop Interpretability Prior,"We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required.  In this work, we optimize for interpretability by directly including humans in the optimization loop.  We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets.  Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.","Isaac Lage*,Harvard;Andrew Ross,Harvard University;Samuel Gershman,Harvard University;Been Kim,Google;Finale Doshi-Velez,Harvard",isaaclage@g.harvard.edu;andrew_ross@g.harvard.edu;gershman@fas.harvard.edu;beenkim@google.com;finale@seas.harvard.edu,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Active Learning",Spotlight
6532,Learning To Learn Around A Common Mean,"The problem of learning-to-learn (LTL) or meta-learning is gaining increasing attention due to recent empirical evidence of its effectiveness in applications. Motivated by recent work on few-shot learning, in this paper we tackle the LTL problem by a novel approach, in which the training datasets received by the meta-algorithm are splitted into two subsets used to train and test the underlying algorithm, respectively. As the underlying algorithm we consider a form of Ridge Regression, in which the regularizer is the square distance to an unknown mean vector. We observe that, in this setting, the LTL problem can be reformulated as a Least Squares (LS) problem and we exploit a stochastic procedure to efficiently solve it. Under specific assumptions, we present a bound for the generalization error of out meta-algorithm. An implication of this bound is that our approach provides a consistent estimate of the transfer risk as the number of tasks grows, even if the sample size is kept constant. Preliminary experiments highlight the advantage offered by our approach.","Giulia Denevi*,IIT;Carlo Ciliberto,;Dimitrios Stamos,University College London;Massimiliano Pontil,",giulia.denevi@iit.it;cciliber@gmail.com;dimitrios.i.stamos@gmail.com;m.pontil@cs.ucl.ac.uk,Algorithms/Multitask and Transfer Learning*; Algorithms/Online Learning; Algorithms/Regression; Deep Learning/Meta-Learning,Poster
6537,Backpropagation with Callbacks: Towards Efficient and Expressive Differentiable Programming,"Deep learning rests in crucial ways on gradient-descent optimization and end-
to-end differentiation. Under the slogan of differentiable programming, there is
an increasing demand for efficient automatic gradient computation for emerging
network architectures that incorporate dynamic control flow.
In this paper we take a fresh look at backpropagation, and propose an implementation using functions with callbacks, where the forward pass is executed as a sequence of function calls and the backward pass when the functions return. A key realization is that this technique of chaining callbacks is well known in the
programming languages community under the name continuation-passing style
(CPS), and any program can be converted to this form using standard techniques.
Our approach achieves the same flexibility as other reverse-mode automatic differentiation (AD) techniques, but it can be implemented without any auxiliary data structures, and it can easily be combined with native code generation techniques, leading to a highly efficient implementation that combines the performance benefits of deep learning frameworks based on explicit reified computation graphs (e.g., TensorFlow) with the expressiveness of pure library approaches (e.g., PyTorch).","Fei Wang,Purdue University;James Decker,Purdue University;Xilun Wu,Purdue University;Gregory Essertel,Purdue University;Tiark Rompf*,Purdue University",wang603@purdue.edu;decker31@purdue.edu;wu636@purdue.edu;gesserte@purdue.edu;tiark.rompf@gmail.com,"Data, Competitions, Implementations, and Software/Software Toolkits",Poster
6538,Learning with SGD and Random Features,"Sketching and stochastic gradient methods are arguably the most common  techniques to derive efficient large scale learning algorithms. In this paper, we investigate their application in the context of nonparametric statistical learning. More precisely, we study the estimator defined by stochastic gradient with mini batches and   random features. The latter can be seen as form of nonlinear sketching and  used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed, our study highlight how different parameters, such as number of features, iterations, step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds, under standard  assumptions. The obtained results are corroborated and illustrated by numerical experiments.","Luigi Carratino*,unige;Alessandro Rudi,INRIA, Ecole Normale Superieure;Lorenzo Rosasco,",luigicarratino@gmail.com;alessandro.rudi@inria.fr;lrosasco@mit.edu,Algorithms/Kernel Methods*; Algorithms/Stochastic Methods; Theory/Learning Theory,Spotlight
6556,Total stochastic gradient algorithms and applications in reinforcement learning,"Back propagation and the chain rule of derivatives have been prominent, however,
the total derivative rule has not enjoyed the same amount of attention. In this work
we show how the total derivative rule leads to an intuitive visual framework for
creating gradient estimators on graphical models. In particular, previous ‚Äùpolicy
gradient theorems‚Äù are easily derived. We derive new gradient estimators based
on density estimation, as well as a likelihood ratio gradient, which ‚Äùjumps‚Äù to an
intermediate node, not directly to the objective function. We evaluate our methods
on model-based policy gradient algorithms, achieve good performance, and present evidence towards demystifying the success of the popular PILCO algorithm.","Paavo Parmas*,Okinawa Inst. of Sci. and Tech",paavoparmas@gmail.com,Reinforcement Learning and Planning/Model-Based RL*; Algorithms/Stochastic Methods; Applications/Motor Control; Applications/Robotics; Deep Lear,Poster
6558,Glow: Generative Flow with Invertible 1x1 Convolutions,"Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood and qualitative sample quality. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient synthesis of large and subjectively realistic-looking images.","Diederik Kingma*,OpenAI;Prafulla Dhariwal,Open AI",dpkingma@gmail.com;prafulla@openai.com,Deep Learning/Generative Models*; Algorithms/Density Estimation; Algorithms/Nonlinear Dimensionality Reduction and Manifold Lear,Poster
6570,Nonparametric Density Estimation under Adversarial Losses,"We study minimax convergence rates of nonparametric density estimation under a large class of loss functions called ``adversarial losses'', which, besides classical $\mathcal{L}^p$ losses, include maximum mean discrepancy (MMD), Wasserstein distance, and total variation distance. These losses are closely related to the losses encoded by discriminator networks in generative adversarial networks (GANs). In a general framework, we study how the choice of loss and the assumed smoothness of the underlying density together determine the minimax rate. We also discuss implications for training GANs based on deep ReLU networks, and more general connections to learning implicit generative models in a minimax statistical sense.","Shashank Singh*,CMU;Ananya Uppal,Carnegie Mellon University;Boyue Li,Carnegie Mellon University;Chun-Liang Li,Carnegie Mellon University;Manzil Zaheer,Carnegie Mellon University;Barnabas Poczos,	Carnegie Mellon University",sss1@andrew.cmu.edu;auppal@andrew.cmu.edu;boyuel@andrew.cmu.edu;chunlial@cs.cmu.edu;manzil@cmu.edu;bapoczos@cs.cmu.edu,Algorithms/Density Estimation*; Deep Learning/Generative Models; Theory/Frequentist Statistics; Theory/Information Theory;,Poster
6577,Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions,"Embedding complex objects as vectors in low dimensional spaces is a longstanding problem in machine learning. We propose in this work an extension of that approach, which consists in embedding objects as elliptical probability distributions, namely distributions whose densities have elliptical level sets. We endow these measures with the 2-Wasserstein metric, with two important benefits: \emph{(i)} For such measures, the squared 2-Wasserstein metric has a closed form, equal to the sum of the squared Euclidean distance between means and the squared Bures metric between covariance matrices. The latter is a Riemannian metric between positive semi-definite matrices, which turns out to be Euclidean on a suitable factor representation of such matrices, which is valid on the entire geodesic between these matrices. \emph{(ii)} The 2-Wasserstein distance boils down to the usual Euclidean metric when comparing Diracs, and therefore provides the natural framework to extend point embeddings. We show that for these reasons Wasserstein elliptical embeddings are more intuitive and yield tools that are better behaved numerically than the alternative choice of Gaussian embeddings with the Kullback-Leibler divergence. In particular, and unlike previous work based on the KL geometry, we learn elliptical distributions that are not necessarily diagonal. We demonstrate the interest of elliptical embeddings by using them for visualization, to compute embeddings of words, and to reflect entanglement or hypernymy. ","Boris Muzellec*,CREST, ENSAE;marco Cuturi,ENSAE",borismuzellec@gmail.com;marco.cuturi.cameto@gmail.com,Deep Learning/Embedding Approaches,Poster
6580,Learning to Share and Hide Intentions using Information Regularization,"Learning to cooperate with friends and compete with foes is a key component of multi-agent reinforcement learning. Typically to do so, one requires access to either a model of or interaction with the other agent(s). Here we show how to learn effective strategies for cooperation and competition in an asymmetric information game with no such model or interaction. Our approach is to encourage an agent to reveal or hide their intentions using an information-theoretic regularizer. We consider both the mutual information between goal and action given state, as well as the mutual information between goal and state. We show how to stochastically optimize these regularizers in a way that is easy to integrate with policy gradient reinforcement learning. Finally, we demonstrate that cooperative (competitive) policies learned with our approach lead to more (less) reward for a second agent in two simple asymmetric information games.","DJ Strouse*,Princeton University;Max Kleiman-Weiner,MIT;Joshua Tenenbaum,MIT;Matthew Botvinick,google;David Schwab,ITS, CUNY Graduate Center",danieljstrouse@gmail.com;maxkw@mit.edu;jbt@mit.edu;botvinick@google.com;dschwab@gc.cuny.edu,Reinforcement Learning and Planning/Multi-Agent RL*; Reinforcement Learning and Planning/Reinforcement Learning; Theory/Information Theory,Poster
6582,Predictive Approximate Bayesian Computation via Saddle Points,"Approximate Bayesian Computation (ABC) has been an important methodology for Bayesian inference when the likelihood function is intractable. Traditional sampling-based ABC algorithms such as ABC rejection and K2-ABC are inefficient performance-wise, while the regression-based algorithms such as K-ABC and DR-ABC are hard to scale. In this paper, we introduce an optimization-based framework for ABC that addresses these deficiencies. Leveraging a generative model for posterior and joint distribution matching, we show that ABC can be framed into  saddle point problems, whose objectives can be accessed directly with samples. We present \emph{the predictive ABC algorithm (P-ABC)}, and provide a PAC bound guaranteeing its learning consistency. Numerical experiment shows that, when compared to K2-ABC and DR-ABC, the proposed P-ABC outperforms both with large margins.","Yingxiang Yang*,University of Illinois at Urbana Champaign;Bo Dai,Google Brain;Negar Kiyavash,University of Illinois at Urbana-Champaign;Niao He,University of Illinois at Urbana-Champaign",yyang172@illinois.edu;bohr.dai@gmail.com;kiyavash@illinois.edu;niaohe@illinois.edu,Probabilistic Methods/Bayesian Theory,Poster
6588,Robustness of conditional GANs to noisy labels,"We study the problem of learning conditional generators from noisy samples, where the labels are corrupted by random noise. Naively training a standard conditional GAN not only produces samples with wrong labels, but also generates poor quality samples. We consider two scenarios, depending on whether the noise model is known or not. When the distribution of the noise is  known, we provide a novel, theoretically sound, and practical Robust Conditional GAN (RCGAN) architecture. We give theoretical justification of our architectural choices. We provide a sharp characterization of how performance depends on the noise statistics, and provide sample complexity of the loss in neural network distances under standard assumptions on the discriminator class. When the distribution of the noise is not known, we provide an extension of our architecture, RCGAN-U. We show experimentally that there is almost no loss in not knowing the noise statistics; RCGAN-U consistently  improves upon baseline approaches, while closely matching that of RCGAN. ","Kiran Thekumparampil,University of Illinois;Ashish Khetan,UIUC;Zinan Lin,CMU;Sewoong Oh*,UIUC",thekump2@illinois.edu;khetan2@illinois.edu;zinanl@andrew.cmu.edu;sewoong79@gmail.com,Deep Learning/Generative Models*; Deep Learning/Adversarial Networks; Theory/Information Theory,Spotlight
6591,Robust Learning of Fixed-Structure Bayesian Networks,"We investigate the problem of learning Bayesian networks in a robust model where an $\epsilon$-fraction of the samples are adversarially corrupted.  In this work, we study the fully observable discrete case where the structure of the network is given.  Even in this basic setting, previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees.  We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees.  Our algorithm has near-optimal sample complexity, runs in polynomial time, and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples.  Finally, we show on both synthetic and semi-synthetic data that our algorithm performs well in practice.","Yu Cheng*,Duke University;Ilias Diakonikolas,USC;Daniel Kane,UCSD;Alistair Stewart,University of Southern California",chycharlie@gmail.com;iliasdiakonikolas@gmail.com;dakane@ucsd.edu;stewart.al@gmail.com,Theory/Learning Theory*; Algorithms/Unsupervised Learning; Probabilistic Methods/Graphical Models,Poster
6595,Improving Simple Models with Confidence Profiles,"In this paper, we propose a new method called ProfWeight for transferring information from a pre-trained deep neural network that has a high test accuracy to a simpler interpretable model or a very shallow network of low complexity and a priori low test accuracy. We are motivated by applications in interpretability and model deployment in severely memory constrained environments (like sensors). Our method uses linear probes to generate confidence scores through flattened intermediate representations. Our transfer method involves a theoretically justified weighting of samples during the training of the simple model using  confidence scores of these intermediate layers. The value of our method is first demonstrated on CIFAR-10, where our weighting method significantly  improves (3-4\%) networks with only a fraction of the number of Resnet blocks of a complex Resnet model. We further demonstrate operationally significant results on a real manufacturing problem, where we dramatically increase the test accuracy of a CART model (the domain standard) by roughly $13\%$. ","Amit Dhurandhar*,IBM Research;Karthikeyan Shanmugam,IBM Research;Ronny Luss,IBM Research;Peder Olsen,IBM Research",adhuran@us.ibm.com;karthikeyan.shanmugam2@ibm.com;rluss@us.ibm.com;pederao@us.ibm.com,"Applications/Fairness, Accountability, and Transparency*; Algorithms/Classification",Poster
6601,PCA of high dimensional stochastic processes,"One technique to visualize the training of neural networks is to perform
PCA on the parameters over the course of training and plot the subspace
spanned by the first few PCA components.  In this paper we compare this
technique to the PCA of a high dimensional random walk.  We prove that in
the limit of infinite dimensions most of the variance is in the first few
PCA components, and that the projection of the trajectory onto any
subspace spanned by PCA components is a Lissajous curve.  We generalize
these results to Ornstein-Uhlenbeck processes (i.e., a random walk in a
quadratic potential) and show that in high dimensions the walk is not mean
reverting, but will instead be trapped at a fixed distance from the
minimum.  We finally compare the distribution of PCA variances and the PCA
projected training trajectories of a linear model trained on CIFAR-10 and
ResNet-50-v2 trained on Imagenet.  We show that early in training the
distribution of PCA variances is steeper than a random walk, implying that
the training is highly directed, whereas late in training the distribution
is flatter than a random walk and closer to a converged Ornstein-Uhlenbeck
process.","Joseph Antognini*,Google;Jascha Sohl-Dickstein,Google Brain",joe.antognini@gmail.com;jaschasd@google.com,"Deep Learning/Visualization or Exposition Techniques for Deep Networks*; Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA); Optimization; Theory/Statistical Ph",Poster
6603,Learning to solve SMT formulas,"We present a new approach for learning to solve SMT formulas. We phrase the challenge of solving SMT formulas as a tree search problem where at each step a transformation is applied to the input formula until the formula is solved. Our approach works in two steps: first, we learn a model (e.g., based on imitation learning) and then synthesize a strategy in the form of a loop-free program with branches. This strategy is an interpretable representation of the model's decisions and can be directly passed and used to guide the SMT solver, without requiring any modification to the solver itself. We show that our technique is practically effective: it solves 20.0% more formulas over a number of benchmarks and achieves up to 1000x runtime improvement over a state-of-the-art SMT solver.","Mislav Balunovic,ETH Zurich;Pavol Bielik*,ETH Zurich;Martin Vechev,ETH Zurich",bmislav@ethz.ch;pavol.bielik@inf.ethz.ch;martin.vechev@inf.ethz.ch,Applications*; Algorithms/AutoML; Reinforcement Learning and Planning,Oral
6609,Lifted Weighted Mini-Bucket,"Many real-world problems, such as Markov Logic Networks (MLNs) with evidence, possess highly symmetric sub-structures but no exact symmetries.   Efficiently exploiting the symmetric substructure of these problems to perform approximate inference is a challenge for which few principled methods exist. In this paper, we present a lifted variant of the Weighted Mini-Bucket elimination algorithm which provides a principled way to 1) exploit the highly symmetric substructure of MLN models, and 2) incorporate high-order inference terms, which are necessary for high quality approximate inference. Our method maintains a concrete connection to the ground problem and has significant control over the accuracy-time trade-off of the approximation. Experimental results demonstrate good anytime performance and the utility of this class of approximations, especially in models with strong repulsive potentials.","Nicholas Gallo*,UC Irvine;Alexander Ihler,UC Irvine",nickgallo631@gmail.com;ihler@ics.uci.edu,Probabilistic Methods/Variational Inference*; Probabilistic Methods/Graphical Models,Poster
6611,Using Quantum Graphical Models to Perform Inference in Hilbert Space,"Quantum Graphical Models (QGMs) generalize classical graphical models by adopting the formalism for reasoning about uncertainty from quantum mechanics. Unlike classical graphical models, QGMs represent uncertainty with density matrices in complex Hilbert spaces. Hilbert space embeddings (HSEs) also generalize Bayesian inference in Hilbert spaces. We investigate the link between QGMs and HSEs and show that the sum rule and Bayes rule for QGMs are equivalent to the kernel sum rule in HSEs and a special case of Nadaraya-Watson kernel regression respectively. We show that these operations can be kernelized, and use these insights to propose a Hilbert Space Embedding of Hidden Quantum Markov Models (HSE-HQMM) to model dynamics. We present experimental results showing that HSE-HQMMs can outperform state-of-the-art models like LSTMs and PSRNNs on several datasets, while also providing a nonparametric method for maintaining a probability distribution over continuous-valued features.","Siddarth Srinivasan*,Georgia Institute of Technology;Carlton Downey,Carnegie Mellon University;Byron Boots,Georgia Institute of Technology",sidsrini@gatech.edu;cmdowney@cs.cmu.edu;bboots@cc.gatech.edu,Probabilistic Methods/Graphical Models*; Algorithms/Dynamical Systems; Algorithms/Kernel Methods; Deep Learning/Recurrent Networks,Poster
6619,Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound,"Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches cannot model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images.","Hadi Kazemi*,WVU;Sobhan Soleymani,West Virginia University;Fariborz Taherkhani,West Virginia University;Seyed mehdi Iranmanesh,West virginia university;Nasser Nasrabadi,West Virginia University",hakazemi@mix.wvu.edu;ssoleyma@mix.wvu.edu;fariborztaherkhani@gmail.com;Seiranmanesh@mix.wvu.edu;nasser.nasrabadi@mail.wvu.edu,Deep Learning/Generative Models*; Algorithms/Unsupervised Learning,Poster
6626,Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution,"We study adversarial perturbations when the instances are uniformly distributed over {0,1}^n. We study both ""inherent"" bounds that apply to any problem and any classifier for such a problem as well as bounds that apply to specific problems and specific hypothesis classes.

As the current literature contains multiple  definitions of adversarial risk and robustness, we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region. We then study some classical algorithms for learning monotone conjunctions and compare their adversarial risk and robustness under different definitions by attacking the hypotheses using instances  drawn from the uniform distribution. We observe that sometimes these definitions lead to significantly different bounds. Thus, this study advocates for the use of the error-region definition, even though other definitions, in other contexts with context-dependent assumptions, may coincide with the error-region definition.

Using the error-region definition of adversarial perturbations, we then study inherent bounds on risk and robustness of any classifier for any classification problem whose instances are uniformly distributed over {0,1}^n. Using the isoperimetric inequality for the Boolean hypercube, we show that for initial error 0.01, there always exists an adversarial perturbation that changes O(‚àön) bits of the instances to increase the risk to 0.5, making classifier's decisions meaningless. Furthermore, by also using the central limit theorem we show that when n‚Üí‚àû, at most c‚àön bits of perturbations, for a universal constant c<1.17, suffice for increasing the risk to 0.5, and the same c‚àön bits of perturbations on average suffice to increase the risk to 1, hence bounding the robustness by c‚àön.","Dimitrios Diochnos,University of Virginia;Saeed Mahloujifar,University of Virginia;Mohammad Mahmoody*,University of Virginia",diochnos@virginia.edu;saeed@virginia.edu;mohammad@virginia.edu,Theory/Learning Theory*; Algorithms/Classification,Poster
6628,Gaussian Process Prior Variational Autoencoders,"Variational autoencoders (VAE) are a powerful and widely-used class of models to learn complex data distributions in an unsupervised fashion. One substantial limitation of VAEs is the prior assumption that latent sample representations are independent. However, for many important applications, such as time-series of images, this assumptions is too strong. Correlations, such as those in time, need to be accounted for to achieve correct model specification, and hence optimal results. Herein, we introduce a new model, the Gaussian Process (GP) Prior Variational Autoencoder (GPVAE) to specifically address this issue. The GPVAE aims to combine the power of VAEs with the ability to model correlations afforded by GP priors. To achieve efficient inference in this new class of models, we leverage structure in the covariance matrix, but also introduce a new stochastic backpropagation strategy that enables full batch gradient descent (GD) in a distributed manner. 
In two image-based applications, we show that our method outperforms  conditional VAEs (CVAEs), and an adaptation of standard VAEs.","Francesco Paolo Casale*,Microsoft Research;Adrian Dalca,MIT;Luca Saglietti,Microsoft Research New England (visitor)  Italian Institute for Genomic Medicine, Torino, Italy;Jennifer Listgarten,UC Berkeley;Nicolo Fusi,MICROSOFT",fncpaolo.casale@gmail.com;adalca@mit.edu;t-lusagl@microsoft.com;jennl@berkeley.edu;fusi@microsoft.com,Probabilistic Methods/Gaussian Processes*; Probabilistic Methods/Latent Variable Models; Probabilistic Methods/Variational Inference,Poster
6629,3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data,"We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R^3 . Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid environment prediction and protein structure classification, both of which have inherent SE(3) symmetry.","Maurice Weiler*,University of Amsterdam;Wouter Boomsma,University of Copenhagen;Mario Geiger,;Max Welling,University of Amsterdam;Taco Cohen,University of Amsterdam",m.weiler@uva.nl;wb@di.ku.dk;geiger.mario@gmail.com;m.welling@uva.nl;taco.cohen@gmail.com,Deep Learning/CNN Architectures*; Algorithms/Classification,Poster
6655,Context-aware Synthesis and Placement of Object Instances,"Learning how to synthesize and place object instances into an image (semantic map) based on the scene context is a challenging and interesting problem in vision and learning. On one hand, solving this problem requires a joint decision of (a) generating an object mask from a certain class at a plausible scale, location, and shape, and (b) inserting the object instance mask into an existing scene so that the synthesized content is semantically realistic. On the other hand, such a model can synthesize realistic outputs to potentially facilitate numerous image editing and scene parsing tasks. In this paper, we propose an end-to-end trainable neural network that can synthesize and insert object instances into an image via a semantic map. The proposed network contains two generative modules that determine where the inserted object should be (i.e., location and scale) and what the object shape (and pose) should look like. The two modules are connected together with a spatial transformation network and jointly trained and optimized in a purely data-driven way. Specifically, we propose a novel network architecture with parallel supervised and unsupervised paths to guarantee diverse results. We show that the proposed network architecture learns the context-aware distribution of the location and shape of object instances to be inserted, and it can generate realistic and statistically meaningful object instances that simultaneously address the where and what sub-problems.","Donghoon Lee*,Seoul National University;Ming-Yu Liu,NVIDIA;Ming-Hsuan Yang,University of California at Merced;Sifei Liu,NVIDIA;Jinwei Gu,Nvidia;Jan Kautz,NVIDIA",dorucia@gmail.com;sean.mingyu.liu@gmail.com;mhyang@ucmerced.edu;sifeil@nvidia.com;jinweig@nvidia.com;jkautz@nvidia.com,Applications/Computer Vision*; Deep Learning/Adversarial Networks,Poster
6666,Convex Elicitation of Continuous Properties,"A property or statistic of a distribution is said to be elicitable if it can be expressed as the minimizer of some loss function in expectation. Recent work shows that continuous real-valued properties are elicitable if and only if they are identifiable, meaning the set of distributions with the same property value can be described by linear constraints. From a practical standpoint, one may ask for which such properties do there exist convex loss functions. In this paper, in a finite-outcome setting, we show that in fact every elicitable real-valued property can be elicited by a convex loss function. Our proof is constructive, and leads to convex loss functions for new properties.","Jessica Finocchiaro*,University of Colorado Boulder;Rafael Frongillo,CU Boulder",jessica.finocchiaro@colorado.edu;raf@colorado.edu,Theory/Game Theory and Computational Economics*; Optimization/Convex Optimization; Theory/Learning Theory,Spotlight
6669,Deep Learning for Supercomputers: Distributed Tensor Layouts Define Distributed Computation,"Data-parallelism is the dominant distributed DNN training strategy, due to its universal applicability across a wide range of model and hardware architectures.   However, memory constraints prevent its application for training very large models, which have been shown in many domains to produce superior results.  Model-parallelism can solve this problem, also reducing step times during training and inference. Unfortunately, model-parallel algorithms tend to be complicated to discover, describe, and to implement, and do not generalize well across model types and hardware types.  We solve this problem by introducing a language for simply specifying distributed tensor computations (model-parallel and/or data-parallel) across an n-dimensional mesh of processors by specifying the distributed storage layouts (split and/or replicated) of the tensors.  The computation is then compiled into processor-local operations, coupled with collective communication primitives such as Allreduce.  Using our new language, we demonstrate very short specifications of a variety of data-parallel and/or model-parallel DNN training algorithms on both a two-layer example model and the Transformer \cite{Vaswani17} sequence-to-sequence model. This allows us to train Transformer models with up to 5 billion parameters on up to 256-node clusters, surpassing SOTA results on WMT14 En-Fr and En-De translation tasks, as well as the one-billion-word language modeling benchmark.","Noam Shazeer,Google;Youlong Cheng,google.com;Niki Parmar,Google;Dustin Tran,Google;Ashish Vaswani,Google Brain;Penporn Koanantakool,Google;Peter Hawkins,google.com;HyoukJoong Lee*,Google;Mingsheng Hong,google.com;Cliff Young,google.com;Ryan Sepassi,Google Brain;Blake Hechtman,Google",noam@google.com;ylc@google.com;nikip@google.com;trandustin@google.com;avaswani@google.com;penporn@google.com;phawkins@google.com;hyouklee@google.com;hongm@google.com;cliffy@google.com;rsepassi@google.com;blakehechtman@google.com,Deep Learning*; Deep Learning/Efficient Inference Methods; Deep Learning/Efficient Training Methods,Poster
6687,Learning Abstract Options,"Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning. One popular approach for addressing this challenge is the options framework (Sutton et al., 2000). However, only recently in (Bacon et al., 2017) was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion. In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time. We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time. We extend results from (Bacon et al., 2017) and derive policy gradient theorems for a deep hierarchy of options. Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals.  Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.","Matthew Riemer*,IBM Research;Miao Liu,IBM;Gerald Tesauro,IBM Research",mdriemer@us.ibm.com;miao.liu1@ibm.com;gtesauro@us.ibm.com,Reinforcement Learning and Planning/Hierarchical RL*; Reinforcement Learning and Planning/Reinforcement Learning,Poster
6693,Bounded-Loss Private Prediction Markets," Prior work has investigated variations of prediction markets that preserve participants' (differential) privacy, which formed the basis of useful mechanisms for purchasing data for machine learning objectives.
  Such markets required potentially unlimited financial subsidy, however, making them impractical.
  In this work, we design an adaptively-growing prediction market with a bounded financial subsidy, while achieving privacy, incentives to produce accurate predictions, and precision in the sense that market prices are
 not heavily impacted by the added privacy-preserving noise.
  We briefly discuss how our mechanism can extend to the data-purchasing setting, and its relationship to traditional learning algorithms.","Rafael Frongillo,CU Boulder;Bo Waggoner*,UPenn",raf@colorado.edu;bwag@seas.upenn.edu,"Theory/Game Theory and Computational Economics*; Applications/Privacy, Anonymity, and Security",Spotlight
6696,Temporal alignment and latent Gaussian process factor inference in population spike trains,"We introduce a novel scalable approach to identifying common latent structure in neural population spike-trains, which allows for variability both in the trajectory and in the rate of progression of the underlying computation. Our approach is based on shared latent Gaussian processes (GPs) which are combined linearly, as in the Gaussian Process Factor Analysis (GPFA) algorithm. We extend GPFA to handle unbinned spike-train data by incorporating a continuous time point-process likelihood model, achieving scalability with a sparse variational approximation. Shared variability is separated into terms that express condition dependence, as well as trial-to-trial variation in trajectories. Finally, we introduce a nested GP formulation to capture variability in the rate of evolution along the trajectory. We show that the new method learns to recover latent trajectories in synthetic data, and can accurately identify the trial-to-trial timing of movement-related parameters from motor cortical data without any supervision.","Lea Duncker*,UCL;Maneesh Sahani,Gatsby Unit, UCL",lea.duncker@gmail.com;maneesh@gatsby.ucl.ac.uk,Neuroscience and Cognitive Science/Neuroscience*; Probabilistic Methods/Gaussian Processes; Probabilistic Methods/Latent Variable Models,Poster
6697,Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise,"The growing importance of massive datasets with the advent of deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling for large datasets, non-expert labeling, and label corruption by data poisoning adversaries. In the latter case, corruptions may be arbitrarily bad, even so bad that a classifier predicts the wrong labels with high confidence. To protect against such sources of noise, we leverage the fact that a small set of clean labels is often easy to procure. We demonstrate that robustness to label noise up to severe strengths can be achieved by using a set of trusted data with clean labels, and propose a loss correction that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.","Dan Hendrycks,University of California, Berkeley;Mantas Mazeika*,University of Chicago;Duncan Wilson,Leap Motion;Kevin Gimpel,",hendrycks@berkeley.edu;mantas@uchicago.edu;duncanw@nevada.unr.edu;kgimpel@ttic.edu,Algorithms*; Algorithms/Classification; Applications/Computer Vision; Applications/Natural Language Proc,Poster
6704,Discretely Relaxing Continuous Variables for tractable Variational Inference,"We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound (ELBO). The proposed ""DIRECT"" approach has several advantages over its predecessors; (i) it can exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient estimates), eliminating the need for high-variance stochastic gradient estimators and enabling the use of quasi-Newton optimization methods; (ii) its training complexity is independent of the number of training points, permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. In addition, our DIRECT models can exactly compute statistical moments of the parameterized predictive posterior without relying on Monte Carlo sampling. Our numerical studies demonstrate accurate inference using latent variables discretized as extremely low-precision 4-bit quantized integers. While the ELBO computations considered require over 10^{2352} log-likelihood evaluations, we train on datasets with over two-million points in just seconds.","Trefor Evans*,University of Toronto;Prasanth Nair,University of Toronto",trefor.evans@mail.utoronto.ca;pbn@utias.utoronto.ca,Probabilistic Methods/Variational Inference,Spotlight
6720,Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior,"Bayesian optimization usually assumes that a Bayesian prior is given. However, the strong theoretical guarantees in Bayesian optimization are often regrettably compromised in practice because of unknown parameters in the prior. In this paper, we adopt a variant of empirical Bayes and show that,  by estimating the Gaussian process prior from offline data sampled from the same prior and constructing unbiased estimators of the posterior, variants of both GP-UCB and \emph{probability of improvement} achieve a near-zero regret bound, which decreases to a constant proportional to the observational noise as the number of offline data and the number of online evaluations increase. Empirically, we have verified our approach on challenging simulated robotic problems featuring task and motion planning.","Zi Wang*,MIT;Beomjoon Kim,MIT;Leslie Kaelbling,MIT",ziw@csail.mit.edu;beomjoon@mit.edu;lpk@csail.mit.edu,Probabilistic Methods/Gaussian Processes*; Algorithms/Bandit Algorithms,Spotlight
6722,Diversity-Driven Exploration Strategy for Deep Reinforcement Learning,"Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards.
To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. We demonstrate the effectiveness of our method in huge 2D gridworlds and a variety of benchmark environments, including Atari 2600 and MuJoCo. Experimental results show that our method outperforms baseline approaches in most tasks in terms of mean scores and exploration efficiency.","Zhang-Wei Hong*,National Tsing Hua University;Tzu-Yun Shann,;Shih-Yang Su,;Yi-Hsiang Chang,;Tsu-Jui Fu,Academia Sinica;Chun-Yi Lee,National Tsing Hua University",williamd4112@gapp.nthu.edu.tw;ariel@shann.net;at7788546@gmail.com;s106062520@m106.nthu.edu.tw;s103062110@m103.nthu.edu.tw;cylee@cs.nthu.edu.tw,Reinforcement Learning and Planning/Reinforcement Learning*; Reinforcement Learning and Planning/Exploration,Poster
6726,Deep Generative Models with Learnable Knowledge Constraints,"The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also 
requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts.
In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.","Zhiting Hu*,Carnegie Mellon University;Zichao Yang ,;Ruslan Salakhutdinov,Carnegie Mellon University;LIANHUI Qin,;Xiaodan Liang,Carnegie Mellon University;Haoye Dong,Sun Yat-sen University;Eric Xing,Petuum Inc. and CMU",zhitingh@cs.cmu.edu;zichaoy@cs.cmu.edu;rsalakhu@cs.cmu.edu;lianhuiqin9@gmail.com;xdliang328@gmail.com;donghy7@mail2.sysu.edu.cn;epxing@cs.cmu.edu,Deep Learning/Generative Models,Poster
6729,The Sparse Manifold Transform,"We present a signal representation framework called the {\em sparse manifold transform} that combines key ideas from sparse coding, manifold learning and slow feature analysis. It turns non-linear transformations in the primary sensory signal space into linear interpolations in a representational embedding space while maintaining approximate invertibility. The sparse manifold transform is an unsupervised and generative framework that explicitly and simultaneously models the sparse discreteness and low-dimensional manifold structure found in natural scenes. When stacked, it also models hierarchical composition. We provide a theoretical description of the transform and demonstrate properties of the learned representation on both synthetic data and natural videos.","Yubei Chen*,UC Berkeley, EECS;Dylan Paiton,UC Berkeley;Bruno Olshausen,UC Berkeley",yubeic@berkeley.edu;dpaiton@berkeley.edu;baolshausen@berkeley.edu,Algorithms/Sparse Coding and Dimensionality Expansion*; Algorithms/Nonlinear Dimensionality Reduction and Manifold Learning; Algorithms/Sparsity and C,Poster
6730,Bayesian Structure Learning by Recursive Bootstrap,"We address the problem of Bayesian structure learning for domains with hundreds of variables by employing non-parametric bootstrap, recursively. We propose a method that covers both model averaging and model selection in the same framework. The proposed method deals with the main weakness of constraint-based learning---sensitivity to errors in the independence tests---by a novel way of combining bootstrap with constraint-based learning. Essentially, we provide an algorithm for learning a tree, in which each node represents a scored CPDAG for a subset of variables and the level of the node corresponds to the maximal order of conditional independencies that are encoded in the graph. As higher order independencies are tested in deeper recursive calls, they benefit from more bootstrap samples, and therefore more resistant to the curse-of-dimensionality. Moreover, the re-use of stable low order independencies allows greater computational efficiency. We also provide an algorithm for sampling CPDAGs efficiently from their posterior given the learned tree. We empirically demonstrate that the proposed algorithm scales well to hundreds of variables, and learns better MAP models and more reliable causal relationships between variables, than other state-of-the-art-methods.","Raanan Rohekar*,Intel Corporation;Yaniv Gurwicz,Intel Corporation;Shami Nisimov,Intel Corporation;Guy Koren,Intel Corporation;Gal Novik,Intel Corporation",raanan.yehezkel@intel.com;yaniv.gurwicz@intel.com;shami.nisimov@intel.com;guy.koren@intel.com;gal.novik@intel.com,Algorithms/Model Selection and Structure Learning*; Algorithms/Boosting and Ensemble Methods; Probabilistic Methods/Causal Inference,Poster
6734,Gated Complex Recurrent Neural Networks,"Complex number have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures.  RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations.  We present a novel complex gate recurrent cell.  When used together with norm-preserving state transition matrices, our complex gated RNN exhibits excellent stability and convergence properties.  We demonstrate competitive performance of our complex gated RNN on the synthetic memory and adding task, as well as on the real-world task of human motion prediction.","Moritz Wolter*,Bonn University;Angela Yao,University of Bonn",wolter@cs.uni-bonn.de;yao@cs.uni-bonn.de,Deep Learning/Recurrent Networks*; Applications/Time Series Analysis,Poster
6739,Learning a Warping Distance from Unlabeled Time Series Using Sequence Autoencoders,"Measuring similarities between unlabeled time series trajectories is an important problem in many domains such as medicine, economics, and vision. It is often unclear what is the appropriate metric to use because of the complex nature of noise in the trajectories (e.g. different sampling rates or outliers). Experts typically hand-craft or manually select a specific metric, such as Dynamic Time Warping (DTW), to apply on their data. In this paper, we propose an end-to-end framework, autowarp, that optimizes and learns a good metric given unlabeled trajectories. We define a flexible and differentiable family of warping metrics, which encompasses common metrics such as DTW, Edit Distance, Euclidean, etc. Autowarp then leverages the representation power of sequence autoencoders to optimize for a member of this warping family. The output is an metric which is easy to interpret and can be robustly learned from relatively few  trajectories. In systematic experiments across different domains, we show that autowarp often outperforms hand-crafted trajectory similarity metrics.  ","Abubakar Abid*,Stanford;James Zou,Stanford University",a12d@stanford.edu;jamesz@stanford.edu,Applications/Time Series Analysis*; Algorithms/Metric Learning; Algorithms/Unsupervised Learning; Deep Learning/Embedding Appro,Poster
6749,Streamlining constraints for random k-SAT,"Several competitive algorithms for random $k$-SAT are based on survey propagation, a variational inference scheme used to obtain approximate marginals. 
These marginals are used to inform branching decisions during search; however, survey propagation marginals are approximate and this can lead to contradictions.
We introduce a more general branching strategy based on streamlining constraints, which sidestep hard assignments to variables. We show that streamlined solvers consistently outperform decimation-only solvers for random $k$-SAT for several problem sizes, shrinking the gap between empirical performance and theoretical limits of satisfiability by $16.3\%$ on average for $k=3,4,5,6$.","Aditya Grover*,Stanford University;Tudor Achim,Stanford University;Stefano  Ermon,Stanford University",adityag@cs.stanford.edu;tachim@cs.stanford.edu;ermon@cs.stanford.edu,Probabilistic Methods/Belief Propagation*; Probabilistic Methods/Graphical Models; Probabilistic Methods/Variational Inference,Poster
6750,Fast deep reinforcement learning using online adjustments from the past,"We propose Ephemeral Value Adjusments (EVA): a means of allowing deep reinforcement learning agents to rapidly adapt to experience in their replay buffer.
EVA shifts the value predicted by a neural network with an estimate of the value function found by prioritised sweeping over experience tuples from the replay buffer near the current state. EVA combines a number of recent ideas around combining episodic memory-like structures into reinforcement learning agents: slot-based storage, content-based retrieval, and memory-based planning.
We show that EVA is performant on a demonstration task and Atari games.","Steven Hansen*,DeepMind;Alexander Pritzel,Deepmind;Pablo Sprechmann,Google DeepMind;Andre Barreto,DeepMind;Charles  Blundell ,DeepMind",stevenhansen@google.com;alexander.pritzel@gmail.com;psprechmann@google.com;andrebarreto@google.com;cblundell@google.com,Deep Learning/Memory-Augmented Neural Networks*; Reinforcement Learning and Planning/Reinforcement Learning,Poster
6751,Improved Network Robustness with Adversary Critic,"  Ideally, what confuses neural network should be confusing to humans. However, recent experiments showed that small, imperceptible image perturbations can change the network prediction. To address this gap in perception, we proposed a novel prior for learning robust classifiers: high-confidence adversarial examples should be indistinguishable from the regular data of the adversarial target. We formulated robust optimization problem in the framework of Generative Adversarial Networks~(GAN) where the adversarial attack on classifier acts as a generator and the critic network learns to distinguish regular and adversarial images. The classifier cost is augmented with the objective to confuse the adversary critic. We also proposed cycle-consistency constraint for the adversarial noise. In the experiments, we demonstrated the effectiveness of the proposed approach. Our method surpassed the robustness of the networks trained with adversarial training. Additionally, we verified with human annotators that adversarial examples are indeed visually confusing.","ALEXANDER MATYASKO*,Nanyang Technological University;Lap-Pui Chau,Nanyang Technological University",ALIAKSAN001@e.ntu.edu.sg;elpchau@ntu.edu.sg,Deep Learning*; Deep Learning/Adversarial Networks; Deep Learning/Generative Models,Poster
6757,Regret Bounds for Online Portfolio Selection with a Cardinality Constraint,"Online portfolio selection is a sequential decision making problem in which a learner repetitively selects a portfolio over a set of assets, aiming to maximize long-term return. In this paper, we study the problem with a cardinality constraint
that the number of assets in a portfolio is restricted to be at most k, and consider
two scenarios: (i) in the full-feedback setting, the learner can observe price relatives (rates of return to cost) for all assets, and (ii) in the bandit-feedback setting, the learner can observe price relatives only for invested assets. We propose efficient algorithms for these scenarios, which achieve sublinear regrets. On the other hand, we also provide regret (statistical) lower bounds for both scenarios, nearly matching the upper bounds when k is a constant. In addition, we give a computational lower bound, which implies that no algorithm maintains both computational efficiency and a small regret upper bound. ","Shinji Ito*,NEC Corporation;Daisuke Hatano,RIKEN AIP;Sumita Hanna, Tokyo Metropolitan University;Akihiro Yabe,;Takuro Fukunaga,RIKEN AIP, JST PRESTO;Naonori Kakimura,Keio University;Ken-ichi Kawarabayashi,National Institute of Informatics",s-ito@me.jp.nec.com;daisuke.hatano@riken.jp;sumita@tmu.ac.jp;a-yabe@cq.jp.nec.com;takuro.fukunaga@riken.jp;kakimura@math.keio.ac.jp;k_keniti@nii.ac.jp,Algorithms/Online Learning*; Algorithms/Bandit Algorithms; Optimization/Convex Optimization; Reinforcement Learning and Pla,Poster
6764,Sketching Method for Large Scale Combinatorial Inference,"We present computationally efficient algorithms to test various combinatorial structures of large-scale graphical models. In order to test the hypotheses on their topological structures, we propose two adjacency matrix sketching frameworks: neighborhood sketching and subgraph sketching. The neighborhood sketching algorithm is proposed to test the connectivity of graphical models. This algorithm randomly subsamples vertices and conducts neighborhood regression and screening. The global sketching algorithm is proposed to test the topological properties requiring exponential computation complexity, especially testing the chromatic number and the maximum clique. This algorithm infers the corresponding property based on the sampled subgraph. Our algorithms are shown to substantially accelerate the computation of existing methods. We validate our theory and method through both synthetic simulations and a real application in neuroscience.","Will Wei Sun*,University of Miami Business School;Junwei Lu,;Han Liu,Northwestern",sunweisurrey8@gmail.com;junweilu@hsph.harvard.edu;hanliu@northwestern.edu,Theory/Frequentist Statistics*; Optimization/Combinatorial Optimization; Theory/Hardness of Learning and Approximations; Theor,Poster
6777,Connecting Optimization and Regularization Paths,"We study the implicit regularization properties of optimization techniques by explicitly connecting their optimization paths to the regularization paths of ``corresponding'' regularized problems. This surprising connection shows that iterates of optimization techniques such as gradient descent and mirror descent are \emph{pointwise} close to solutions of appropriately regularized objectives. While such a tight connection between optimization and regularization is of independent intellectual interest, it also has important implications for machine learning: we can port results from regularized estimators to optimization, and vice versa. We investigate one key consequence, that borrows from the well-studied analysis of regularized estimators, to then obtain tight excess risk bounds of the iterates generated by optimization techniques.","Arun Sai  Suggala*,Carnegie Mellon University;Adarsh Prasad,Carnegie Mellon University;Pradeep Ravikumar,Carnegie Mellon University",asuggala@andrew.cmu.edu;adarshp@andrew.cmu.edu;pradeepr@cs.cmu.edu,Theory/Learning Theory,Poster
6779,Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices,"Real-time automatic speech recognition (ASR) on mobile and embedded devices has been of great interests for many years.  We present real-time speech recognition on smart-phones or embedded systems by employing recurrent neural network (RNN) based acoustic models, RNN based language models, and beam-search decoding. The acoustic model is end-to-end trained with connectionist temporal classification (CTC) loss. The RNN implementation on embedded devices can suffer from excessive DRAM accesses because the parameter size of a neural network usually exceeds that of the cache memory and the parameters are used only once for each time step. To remedy this problem, we employ a multi-time step parallelization approach that computes multiple output samples at a time with the parameters fetched from the DRAM. Since the number of DRAM accesses can be reduced in proportion to the number of parallelization steps, we can achieve a high processing speed. However, conventional RNNs, such as long short term memory (LSTM) or gated recurrent unit (GRU), do not permit multi-time step parallelization. We construct an acoustic model by combining simple recurrent units (SRUs) and simple 1-dimensional convolution layers for multi-time step parallelization. Both the character and word piece models are developed for acoustic modeling, and the corresponding RNN based language models are used for beam search decoding. We achieve a competitive WER for WSJ corpus using the entire model size of around 30MB and achieve real-time speed using only a single core ARM without GPU or special hardware.","Jinhwan Park*,Seoul National University;Yoonho Boo,Seoul National University;Iksoo Choi,Seoul National University;sungho shin,Seoul national university;wonyong sung,Seoul national university",bnoo@snu.ac.kr;dnsgh337@snu.ac.kr;akacis@snu.ac.kr;sungho.develop@gmail.com;wysung@snu.ac.kr,Applications/Speech Recognition*; Deep Learning/Recurrent Networks,Poster
6781,Understanding Regularized Spectral Clustering via Graph Conductance,"This paper uses the relationship between graph conductance and spectral clustering to study (i) the failures of spectral clustering and (ii) the benefits of regularization.  The explanation is simple.  Sparse and stochastic graphs create a lot of small trees that are connected to the core of the graph by only one edge.  Graph conductance is sensitive to these noisy ""dangling sets.""  Spectral clustering inherits this sensitivity.  The second part of the paper starts from a previously proposed form of regularized spectral clustering and shows that it is related to the graph conductance on a ``regularized graph.""  We call the conductance on the regularized graph CoreCut.  Based upon previous arguments that relate graph conductance to spectral clustering (e.g. Cheeger inequality), minimizing CoreCut relaxes to regularized spectral clustering.  Simple inspection of CoreCut reveals why it is less sensitive to small cuts in the graph.  Together, these results show that unbalanced partitions from spectral clustering can be understood as overfitting to noise in the periphery of a sparse and stochastic graph.  Regularization fixes this overfitting.  In addition to this statistical benefit, these results also demonstrate how regularization can improve the computational speed of spectral clustering.  We provide  simulations and data examples to illustrate these results. 
 ","Yilin Zhang,University of Wisconsin-Madison;Karl Rohe*,Wisconsin",yilin.zhang@wisc.edu;karlrohe@stat.wisc.edu,"Algorithms/Clustering*; Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA); Algorithms/Spectral Methods; Applic",Poster
6782,Data-Driven Clustering via Parameterized Lloyd's Families,"Algorithms for clustering points in metric spaces is a long-studied area of research. Clustering has seen a multitude of work both theoretically, in understanding the approximation guarantees possible for many objective functions such as k-median and k-means clustering, and experimentally, in finding the fastest algorithms and seeding procedures for Lloyd's algorithm. The performance of a given clustering algorithm depends on the specific application at hand, and this may not be known up front. For example, a ""typical instance"" may vary depending on the application, and different clustering heuristics perform differently depending on the instance.

In this paper, we define an infinite family of algorithms generalizing Lloyd's algorithm, with one parameter controlling the the initialization procedure, and another parameter controlling the local search procedure. This family of algorithms includes the celebrated k-means++ algorithm, as well as the classic farthest-first traversal algorithm. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal clustering algorithm from the class. We show the best parameters vary significantly across datasets such as MNIST, CIFAR, and mixtures of Gaussians. Our learned algorithms never perform worse than k-means++, and on some datasets we see significant improvements.","Maria-Florina Balcan,Carnegie Mellon University;Travis Dick,Carnegie Mellon University;Colin White*,Carnegie Mellon University",ninamf@cs.cmu.edu;tdick@cs.cmu.edu;crwhite@cs.cmu.edu,Algorithms/Clustering*; Algorithms/AutoML,Spotlight
6783,Learning Beam Search Policies via Imitation Learning,"Beam search is widely used for approximate decoding in
structured prediction problems.
Models often use a beam at test time
but ignore its existence at train time, and therefore do not
explicitly learn how to use the beam.
We develop an unifying meta-algorithm for learning beam search policies
using imitation learning.
In our setting, the beam is part of the model and not
just an artifact of approximate decoding.
Our meta-algorithm captures existing learning algorithms and suggests new ones.
It also lets us show novel no-regret guarantees for learning beam search policies.","Renato Negrinho*,Carnegie Mellon University;Matthew Gormley,Carnegie Mellon University; Geoffrey Gordon,(Carnegie Mellon University)",negrinho@cs.cmu.edu;mgormley@cs.cmu.edu;ggordon@cs.cmu.edu,Algorithms/Structured Prediction*; Algorithms/Online Learning; Theory/Learning Theory,Poster
6785,Benefits of overparameterization with EM,"Expectation Maximization (EM) is among the most popular algorithms for finding the maximizer of the log-likelihood objective. However, due to the non-concavity of log-likelihood, EM is generally only guaranteed to find its stationary points. The goal of this article is to present theoretical and empirical evidence to confirm that over-parameterization can help EM avoid the local minima of the likelihood objective. More specifically, we consider the problem of estimating the mean vectors of a spherical Gaussian mixture model under the following two scenarios: (i) the weights of each mixture is known, and (ii) the weights of the mixture are not known. Our study of the global behavior of EM reveals the better convergence properties of EM for the second model. Hence, it suggests that even if the weights of the mixtures are known, it is better to ignore this piece of information and run a more general EM that learns the weights. For a mixture of two symmetric Gaussians, we will prove that introducing the (statistically redundant) weight parameters enables EM to find the global minimizer under any initialization, despite the existence of local minima. For the mixture of three and four Gaussian, we present strong empirical evidence that confirms a similar behavior. Our results corroborate the value of over-parameterization in solving non-convex optimization problems, a phenomenon that has been observed in other areas of research, such as neural nets. ","Ji Xu*,Columbia University;Daniel Hsu,Columbia University;Arian Maleki,Columbia",jx2193@columbia.edu;djhsu@cs.columbia.edu;arian@stat.columbia.edu,Theory*; Optimization/Non-Convex Optimization; Probabilistic Methods/Latent Variable Models; Theory ->,Poster
6787,Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning,"In this paper, we propose a novel sampling method, the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo, for the purpose of multimodal Bayesian learning. It simulates a noisy dynamical system by incorporating both a continuously-varying tempering variable and the Nos\'e-Hoover thermostats. A significant benefit is that it is not only able to efficiently generate i.i.d. samples when the underlying posterior distributions are multimodal, but also capable of adaptively neutralising the noise arising from the use of mini-batches. While the properties of the approach have been studied using synthetic datasets, our experiments on three real datasets have also shown its performance gains over several strong baselines for Bayesian learning with various types of neural networks plunged in.","Rui Luo*,UCL;Jianhong Wang,UCL;Yaodong Yang,University College London;Jun WANG,UCL;Zhanxing Zhu,Peking University",r.luo@cs.ucl.ac.uk;jianhong.wang.17@ucl.ac.uk;yaodong.yang@outlook.com;junwang@cs.ucl.ac.uk;zhanxing.zhu@pku.edu.cn,Probabilistic Methods/MCMC,Poster
6793,Robust Subspace Approximation in a Stream,"	We study robust subspace estimation in the streaming and distributed settings. Given a set of n data points {a_i}_{i=1}^n in R^d and an integer k, we wish to find a linear subspace S of dimension k for which sum_i M(dist(S, a_i)) is minimized, where dist(S,x) := min_{y in S} |x-y|_2, and M() is some loss function. When M is the identity function, S gives a subspace that is more robust to outliers than that provided by the truncated SVD. Though the problem is NP-hard, it is approximable within a (1+epsilon) factor in polynomial time when k and epsilon are constant.
	We give the first sublinear approximation algorithm for this problem in the turnstile streaming and arbitrary partition distributed models, achieving the same time guarantees as in the offline case. Our algorithm is the first based entirely on oblivious dimensionality reduction, and significantly simplifies prior methods for this problem, which held in neither the streaming nor distributed models.","Roie Levin*,Carnegie Mellon University;Anish Sevekari,Carnegie Mellon University;David Woodruff,Carnegie Mellon University",roiel@cs.cmu.edu;asevekar@andrew.cmu.edu;dwoodruf@cs.cmu.edu,Algorithms*; Algorithms/Regression,Spotlight
6802,Mean Field for the Stochastic Blockmodel: Optimization Landscape and Convergence Issues,"Variational approximation has been widely used in large-scale Bayesian inference recently, the simplest kind of which involves imposing a mean field assumption to approximate complicated latent structures. Despite the computational scalability of mean field, theoretical studies of its loss function surface and the convergence behavior of iterative updates for optimizing the loss are far from complete. In this paper, we focus on the problem of community detection for a simple two-class Stochastic Blockmodel (SBM). Using batch co-ordinate ascent (BCAVI) for updates, we give a complete characterization of all the critical points and show different convergence behaviors with respect to initializations. When the parameters are known, we show a significant proportion of random initializations will converge to ground truth. On the other hand, when the parameters themselves need to be estimated, a random initialization will converge to an uninformative local optimum.","Soumendu Sundar Mukherjee*,University of California, Berkeley;Purnamrita Sarkar,University of Texas at Austin;Y. X. Rachel Wang,University of Sydney;Bowei Yan,University of Texas at Austin",soumendu041@gmail.com;purna.sarkar@austin.utexas.edu;rachel.wang@sydney.edu.au;boweiy@utexas.edu,Probabilistic Methods/Variational Inference*; Optimization/Non-Convex Optimization,Poster
6832,Analysis of Krylov Subspace Solutions of  Regularized Non-Convex Quadratic Problems,"We provide convergence rates for Krylov subspace solutions to the trust-region and cubic-regularized (nonconvex) quadratic problems. Such solutions may be efficiently computed by the Lanczos method and have long been used in practice. We prove error bounds of the form $1/t^2$ and $e^{-4t/\sqrt{\kappa}}$, where $\kappa$ is a condition number for the problem, and $t$ is the Krylov subspace order (number of Lanczos iterations). We also provide lower bounds showing that our analysis is sharp.","Yair Carmon*,Stanford;John Duchi,Stanford University",ycarmon@gmail.com;jduchi@stanford.edu,Optimization/Non-Convex Optimization*; Algorithms; Optimization,Oral
6836,Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language,"Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies.","Matthew Hoffman*,Google;Matthew Johnson,Google Brain;Dustin Tran,Google",mdhoffma@cs.princeton.edu;mattjj@google.com;trandustin@google.com,"Probabilistic Methods/Graphical Models*; Data, Competitions, Implementations, and Software/Software Toolkits",Poster
6839,DropBlock: A regularization method for convolutional networks,"  Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that neurons in a contiguous region in convolutional layers are strongly correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where neurons in a contiguous region of a feature map are dropped together. Extensive experiments show that DropBlock works much better than dropout in regularizing convolutional networks.
  On ImageNet, DropBlock with ResNet-50 architecture achieves $77.65\%$ accuracy, which is more than $1\%$ improvement on the previous result of this architecture.","Golnaz Ghiasi*,Google Brain;Tsung-Yi Lin,Google Brain;Quoc Le,Google Brain",golnazg@google.com;tsungyi@google.com;qvl@google.com,Theory/Regularization*; Deep Learning/CNN Architectures,Poster
6843,Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger,"We formulate the problem of \emph{defogging} as state estimation and future state prediction from previous, partial observations in the context of real-time strategy games.
We propose to employ encoder-decoder neural networks for this task, and introduce proxy tasks and baselines for evaluation to assess their ability of capturing basic game rules and high-level dynamics.
By combining convolutional neural networks and recurrent networks, we exploit spatial and sequential correlations and train well-performing models on a large dataset of human games of StarCraft: Brood War.
Finally, we demonstrate the relevance of our models to downstream tasks by applying them for enemy unit prediction in a state-of-the-art, rule-based StarCraft bot.
We observe improvements in win rates against several strong community bots.","Gabriel Synnaeve*,Facebook;Zeming Lin,Facebook AI Reseach;Jonas Gehring,Facebook AI Research;Dan Gant,Facebook AI Research;Vegard Mella,Facebook AI Research;Vasil Khalidov,Facebook AI Research;Nicolas Carion,Facebook AI Research;Nicolas Usunier,Facebook AI Research",gab@fb.com;zlin@fb.com;jgehring@fb.com;dangant@fb.com;vegardmella@fb.com;vkhalidov@fb.com;alcinos@fb.com;usunier@fb.com,Applications/Game Playing,Poster
6847,"With Friends Like These, Who Needs Adversaries?","The vulnerability of deep networks to adversarial attack is now well known, but less well understood. Via a novel empirical analysis, we illustrate some facts about deep convolutional networks that shed new light on the behaviour of deep nets and the problem of adversaries, with two key results. The first is a straightforward explanation of the existence of universal adversarial perturbations, obtained by analysing the connection between class identity and nets' logit responses as functions of movements along (1D) image-space dimensions. The second is the clear demonstration of the tight coupling between classification performance and vulnerability to adversarial attack within these dimensions. Prior work has noted the importance of low-dimensional subspaces in adversarial vulnerability: we illustrate that this is simply the dual of the nets' notion of saliency. In all, we provide a digestible perspective from which to understand previously reported results which have appeared disjoint or contradictory, with implications for efforts to construct neural nets that are both accurate and robust to adversarial attack.","Saumya Jetley,University of Oxford;Nicholas Lord*,University of Oxford;Philip  Torr,University of Oxford",sjetley@robots.ox.ac.uk;nicklord@robots.ox.ac.uk;phst@robots.ox.ac.uk,"Deep Learning*; Algorithms/Classification; Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA); Algorith",Poster
6859,Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters,"We study the problem of decentralized distributed computation of a discrete approximation for regularized Wasserstein barycenter of a finite set of continuous probability measures distributedly stored over a network. Particularly, we assume that there is a network of agents/machines/computers where each agent holds a private continuous probability measure, and seeks to compute the barycenter of all the measures in the network by getting samples from its local measure and exchanging information with its neighbors. Motivated by this problem, we develop a novel accelerated primal-dual stochastic gradient method for general stochastic convex optimization problems with linear equality constraints. Then, we apply this method to the decentralized distributed optimization setting to generate a new algorithm for the distributed semi-discrete regularized Wasserstein barycenter problem. The proposed algorithm can be executed over arbitrary networks that are undirected, connected and static, using the local information only. Moreover, we show explicit non-asymptotic complexity in terms of the problem parameters. Finally, we show the effectiveness of our method on the distributed computation of the regularized Wasserstein barycenter of univariate Gaussian and von Mises distributions, as well as some applications to image aggregation.","Pavel Dvurechensky,Weierstrass Institute ;Darina Dvinskikh*,Skolkovo Institute of Science and Technology;Alexander Gasnikov,Moscow Institute of Physics and Technology;Cesar Uribe,University of Illinois at Urbana-Champaign;Angelia Nedich,Arizona State University",pavel.dvurechensky@gmail.com;darina.dvinskikh@skolkovotech.ru;gasnikov@yandex.ru;cauribe2@illinois.edu;angelia.nedich@asu.edu,Optimization/Convex Optimization*; Algorithms/Stochastic Methods,Spotlight
6863,Joint Autoregressive and Hierarchical Priors for Learned Image Compression,"Recent models for learned image compression are based on autoencoders,
learning approximately invertible mappings from pixels to a quantized latent
representation. These are combined with an entropy model, a prior on the
latent representation that can be used with standard arithmetic coding
algorithms to yield a compressed bitstream. Recently, hierarchical entropy
models have been introduced as a way to exploit more structure in the latents
than simple fully factorized priors, improving compression performance while
maintaining end-to-end optimization. Inspired by the success of autoregressive
priors in probabilistic generative models, we examine autoregressive,
hierarchical, as well as combined priors as alternatives, weighing their costs
and benefits in the context of image compression. While it is well known that
autoregressive models come with a significant computational penalty, we find
that in terms of compression performance, autoregressive and hierarchical
priors are complementary and, together, exploit the probabilistic structure in
the latents better than all previous learned models.  The combined model
yields state-of-the-art rate--distortion performance, providing a 15.8\%
average reduction in file size over the previous state-of-the-art method based
on deep learning, which corresponds to a 59.8% size reduction over JPEG, more
than 35% reduction compared to WebP and JPEG2000, and bitstreams 8.4%
smaller than BPG, the current state-of-the-art image codec. To the best of our
knowledge, our model is the first learning-based method to outperform BPG on
both PSNR and MS-SSIM distortion metrics.","David Minnen*,Google Inc.;Johannes Ball√©,Google;George Toderici,Google, US",dminnen@google.com;jballe@google.com;gtoderici@google.com,Algorithms/Representation Learning*; Applications/Computer Vision; Deep Learning/Generative Models,Poster
6874,Learning Temporal Point Processes via Reinforcement Learning,"Many real world problems from sustainability, healthcare and Internet generate discrete events in continuous time. The generative processes of these data can be very complex, requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling such event data. However,  sophisticated point process models typically leads to intractable likelihood functions, making model fitting difficult in practice. We address this challenge from the perspective of reinforcement learning (RL), and relate the intensity function of a point process to a stochastic policy in reinforcement learning. We parameterize the policy as a flexible recurrent neural network, and reward models which can mimic the observed event distribution. Since the reward function is unknown in practice, we also uncover an analytic form of the reward function using an inverse reinforcement learning formulation and functions from a reproducing kernel Hilbert space. This new RL framework allows us to derive 
an efficient policy gradient algorithm for learning flexible point process models, and we show that it performs well in both synthetic and real data. ","Shuang Li*,Georgia Tech;Shuai Xiao,Ant Financial Services Group;Shixiang Zhu,Georgia Institute of Technology;Nan Du,Google Brain;Yao Xie,Georgia Tech;Le Song,Ant Financial & Georgia Institute of Technology ",sli370@gatech.edu;benjaminforever@sjtu.edu.cn;shixiang.zhu@gatech.edu;dunan@google.com;yao.xie@isye.gatech.edu;lsong@cc.gatech.edu,Applications/Time Series Analysis,Spotlight
6882,Bias and Generalization in Deep Generative Models: An Empirical Study,"In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images by probing the learning algorithm with carefully designed training datasets. By measuring properties of the learned distribution, we are able to find interesting patterns of generalization. We verify that these patterns are consistent across datasets, common models and architectures.","Shengjia Zhao*,Stanford University;Hongyu Ren,Stanford University;Arianna Yuan,Stanford University;Jiaming Song,Stanford;Noah Goodman,Stanford University;Stefano  Ermon,Stanford University",sjzhao@stanford.edu;hyren@cs.stanford.edu;xfyuan@stanford.edu;jiaming.tsong@gmail.com;ngoodman@stanford.edu;ermon@cs.stanford.edu,Deep Learning/Generative Models*; Deep Learning/Adversarial Networks; Deep Learning/Visualization or Exposition Techniques for D,Spotlight
6892,Fast and Effective Robustness Certification,"We present a new method and system for certifying neural network robustness based on abstract interpretation. Compared to state-of-the-art abstract interpreters for neural networks, our analyzer handles additional types of networks yet is significantly more scalable and precise. These benefits are due to carefully designed abstract transformers tailored to the particular abstraction we use. Our analyzer also achieves precision comparable to approaches based on duality, while being much faster and providing stronger guarantees.","Gagandeep Singh*,ETH Zurich;Timon Gehr,ETH Zurich;Matthew Mirman,ETH Zurich;Markus P√ºschel,ETH Zurich;Martin Vechev,ETH Zurich",gsingh@inf.ethz.ch;timon.gehr@inf.ethz.ch;matthew.mirman@inf.ethz.ch;pueschel@inf.ethz.ch;martin.vechev@inf.ethz.ch,"Deep Learning/Adversarial Networks*; Applications/Privacy, Anonymity, and Security",Poster
6901,Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds,"This paper studies the problem of sparse regression where the goal is to learn a sparse vector that best optimizes a given objective function. Under the assumption that the objective function satisfies restricted strong convexity (RSC), we analyze orthogonal matching pursuit (OMP), a greedy algorithm that is used heavily in applications, and obtain support recovery result as well as a tight generalization error bound for OMP. Furthermore, we obtain lower bounds for OMP, showing that both our results on support recovery and generalization error are tight up to logarithmic factors. To the best of our knowledge, these support recovery and generalization bounds are the first such matching upper and lower bounds (up to logarithmic factors) for {\em any} sparse regression algorithm under the RSC assumption. ","Raghav Somani*,Microsoft Research Lab - India;Chirag Gupta,Microsoft Research, India;Prateek Jain,Microsoft Research;Praneeth Netrapalli,Microsoft Research",raghavsomani1995@gmail.com;chiragpvg@gmail.com;prajain@microsoft.com;praneeth@microsoft.com,Algorithms/Sparsity and Compressed Sensing*; Algorithms/Regression,Spotlight
6905,Differentially Private Change-Point Detection,"The change-point detection problem seeks to identify distributional changes at an unknown change-point k* in a stream of data. This problem appears in many important practical settings involving personal data, including biosurveillance, fault detection, finance, signal detection, and security systems. The field of differential privacy offers data analysis tools that provide powerful worst-case privacy guarantees. We study the statistical problem of change-point problem through the lens of differential privacy. We give private algorithms for both online and offline change-point detection, analyze these algorithms theoretically, and then provide empirical validation of these results. ","Sara Krehiel*,University of Richmond;Rachel Cummings,Georgia Tech;Wanrong Zhang,Georgia Institute of Technology;Yajun Mei,Georgia Institute of Technology;Rui Tuo,Georgia Institute of Technology",krehbiel@richmond.edu;rachelc@gatech.edu;wanrongz@gatech.edu;yajun.mei@isye.gatech.edu;tuorui.temp@gmail.com,"Applications/Privacy, Anonymity, and Security*; Algorithms/Adaptive Data Analysis",Poster
6913,Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations,"We present Multi-value Rule Sets (MRS) for interpretable classification with feature efficient presentations. Compared to rule sets built from single-valued rules, MRS adopts a more generalized form of association rules that allows multiple values in a condition. Rules of this form are more concise than classical single-valued rules in capturing and describing patterns in data. Our formulation also pursues a higher efficiency of feature utilization, which reduces possible cost in data collection and storage. We propose a Bayesian framework for formulating a MRS model and propose an efficient inference method for learning a maximum a posteriori, incorporating theoretically grounded bounds to iteratively reduce the search space and improve the search efficiency. Experiments on synthetic and real-world data demonstrate that MRS models have significantly smaller complexity and fewer features than baseline models while being competitive in predictive accuracy. ","Tong Wang*,University of Iowa",tong-wang@uiowa.edu,"Probabilistic Methods/Hierarchical Models*; Applications/Fairness, Accountability, and Transparency",Poster
6921,Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions,"An important goal common to domain adaptation and causal reasoning is to make accurate predictions when the distributions for the target domain(s) and the source domain(s) differ. We consider the case in which the domains correspond to different contexts in which a system has been measured, for example, a purely observational context and several interventional contexts in which the system has been perturbed by external interventions. We consider a class of such causal domain adaptation problems, where data for multiple source domains are given, and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph, nor of the type of the interventions or the intervention targets. We propose a practical implementation of the approach and evaluate it on simulated and real world data.","Sara Magliacane*,IBM Research;Thijs van Ommen,University of Amsterdam;Tom  Claassen,RU Nijmegen;Stephan Bongers,University of Amsterdam	;Philip Versteeg,University of Amsterdam;Joris M. Mooij,University of Amsterdam",sara.magliacane@gmail.com;thijsvanommen@gmail.com;tomc@cs.ru.nl;srbongers@gmail.com;p.j.j.p.versteeg@uva.nl;j.m.mooij@uva.nl,Probabilistic Methods/Causal Inference*; Deep Learning/Virtual Environments,Poster
6923,Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of Neurons,"We analyze linear independence of rank one tensors produced by tensor powers of randomly perturbed vectors. This enables efficient decomposition of sums of high-order tensors. Our analysis builds upon [BCMV14] but allows for a wider range of perturbation models, including discrete ones. We given an application to recovering assemblies of neurons.
		
		
		Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts related; the phenomenon is called association of assemblies.  This that an animal's memory is a complex web of associations, and poses the problem of recovering this representation from cognitive data.  Motivated by this problem, we study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their $\ell$-wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram.","Nima Anari*,Stanford University;Constantinos Daskalakis,MIT;Wolfgang Maass,Graz University of Technology;Christos Papadimitriou,Columbia University;Amin Saberi,Stanford University;Santosh Vempala,",anari.nima@gmail.com;costis@csail.mit.edu;maass@igi.tugraz.at;christos@cs.columbia.edu;saberi@stanford.edu;vempala@gatech.edu,"Theory*; Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA); Algorithms/Spectral Methods; Neuros",Poster
6961,MixLasso: Generalized Mixed Regression via Convex Atomic-Norm Regularization,"We consider a generalization of mixed regression where the response is an additive combination of several mixture components. Standard mixed regression is a special case where each response is generated from exactly one component. Typical approaches to the mixture regression problem employ local search methods such as Expectation Maximization (EM) that are prone to spurious local optima. On the other hand, a number of recent theoretically-motivated \emph{Tensor-based methods} either have high sample complexity, or require the knowledge of the input distribution, which is not available in most of practical situations. In this work, we study a novel convex estimator \emph{MixLasso} for the estimation of generalized mixed regression, based on an atomic norm specifically constructed to regularize the number of mixture components. Our algorithm gives a risk bound that trades off between prediction accuracy and model sparsity without imposing stringent assumptions on the input/output distribution, and can be easily adapted to the case of non-linear functions. In our numerical experiments on mixtures of linear as well as nonlinear regressions, the proposed method yields high-quality solutions in a wider range of settings than existing approaches.","Ian En-Hsu Yen*,Carnegie Mellon University;Wei-Cheng Lee,National Taiwan University;Kai Zhong,Amazon;Sung-En Chang,Northeastern University;Pradeep Ravikumar,Carnegie Mellon University;Shou-De Lin,NTU",eyan@cs.cmu.edu;b01902065@ntu.edu.tw;kaizhong89@gmail.com;chang.sun@husky.neu.edu;pradeepr@cs.cmu.edu;sdlin@csie.ntu.edu.tw,Optimization,Poster
6968,Semidefinite relaxations for certifying robustness to adversarial examples,"Research on adversarial examples are evolved in arms race between defenders who
attempt to train robust networks and attackers who try to prove them wrong.
This has spurred interest in methods for certifying the robustness of a network.
Methods based on combinatorial optimization compute the true robustness
but do not yet scale.  Methods based on convex relaxations scale better
but can only yield non-vacuous bounds on networks trained with those
relaxations.
In this paper, we propose a new semidefinite relaxation that applies to ReLU networks with any number of layers. We show that it produces meaningful robustness guarantees across a spectrum of networks that were trained against other objectives, something previous convex relaxations are not able to achieve.","Aditi Raghunathan*,Stanford University;Jacob Steinhardt,Stanford;Percy Liang,",aditir1994@gmail.com;jacob.steinhardt@gmail.com;pliang@cs.stanford.edu,"Applications/Privacy, Anonymity, and Security",Poster
6972,Removing Hidden Confounding by Experimental Grounding,"Observational data is being increasingly used as a means for making individual-level causal predictions and intervention recommendations. The foremost challenge of causal inference from observational data is hidden confounding, whose presence cannot be tested in data and can invalidate any causal conclusion. Experimental data does not stuffer from confounding but is usually limited in both scope and scale. We introduce a novel method of using limited experimental data to correct the hidden confounding in causal effect models trained on larger observational data, even if the observational data does not fully overlap with the experimental data. Our method makes strictly weaker assumptions than existing approaches, and we prove conditions under which our method yields a consistent estimator. We demonstrate our method's efficacy using real-world data from a large educational experiment. ","Nathan Kallus,Cornell Tech;Aahlad Manas Puli,NYU;Uri Shalit*,Technion",kallus@cornell.edu;apm470@nyu.edu;urishalit@technion.ac.il,Probabilistic Methods/Causal Inference*; Algorithms/Regression,Spotlight
6978,Topkapi: Parallel and Fast Sketches for Finding Top-K Frequent Elements,"Identifying the top-K frequent items is one of the most common and important operations in large data processing systems. As a result, several solutions have been proposed to solve this problem approximately. In this paper, we identify that in modern distributed settings with both multi-node as well as multi-core parallelism, existing algorithms, although theoretically sound, are suboptimal from the performance perspective. In particular, for identifying top-K frequent items, Count-Min Sketch (CMS) has fantastic update time but lack the important property of reducibility which is needed for exploiting available massive data parallelism. On the other end, popular Frequent algorithm (FA) leads to reducible summaries but the update costs are significant. In this paper, we present Topkapi, a fast and parallel algorithm for finding top-K frequent items, which gives the best of both worlds, i.e., it is reducible as well as efficient update time similar to CMS. Topkapi possesses strong theoretical guarantees and leads to significant performance gains due to increased parallelism, relative to past work. Topkapi also demonstrates the power of carefully tailored randomized algorithms accelerated over high-performance computing in obtaining disruptive speedups over distributed word counting benchmarks over the popular Spark frameworks.","Ankush Mandal*,Georgia Institute of Technology;He Jiang,Rice University;Anshumali Shrivastava,Rice University;Vivek Sarkar,Georgia Institute of Technology",ankush@gatech.edu;cary.jiang@rice.edu;anshumali@rice.edu;vsarkar@gatech.edu,"Applications/Web Applications and Internet Data*; Data, Competitions, Implementations, and Software/Benchmarks; Data, Competitions, Implementations",Poster
7985,Contrastive Learning from Pairwise Measurements,"Learning from pairwise measurements naturally arises from many applications, such as rank aggregation, ordinal embedding, and crowdsourcing. However, most existing models and algorithms are susceptible to potential model misspecification. In this paper, we study a semiparametric model where the pairwise measurements follow a natural exponential family distribution with an unknown base measure. Such a semiparametric model includes various popular parametric models, such as the Bradley-Terry-Luce model and the paired cardinal model, as special cases. To estimate this semiparametric model without specifying the base measure, we propose a data augmentation technique to create virtual examples, which enables us to define a contrastive estimator. In particular, we prove that such a contrastive estimator is invariant to model misspecification within the natural exponential family, and moreover, attains the optimal statistical rate of convergence up to a logarithmic factor. We provide numerical experiments to corroborate our theory. ","Yi Chen,Northwestern U;Zhuoran Yang,;Yuchen Xie,Northwestern University;Zhaoran Wang*,Northwestern U",yichen2016@u.northwestern.edu;zy6@princeton.edu;ycxie@u.northwestern.edu;zhaoranwang@gmail.com,"Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA)*; Algorithms/Ranking and Preference Learning",Poster
7992,Point process latent variable models of freely swimming larval zebrafish,"A fundamental goal of systems neuroscience is to understand how neural activity gives rise to rich natural behavior.  In order to achieve this goal, we must first build comprehensive models that offer quantitative descriptions of behavior.  We develop a new class of probabilistic models to tackle this challenge in the study of larval zebrafish, an important model organism for neuroscience.  Larval zebrafish locomote via sequences of punctate swim bouts---brief flicks of the tail---which are naturally modeled as a marked point process.  However, these sequences of swim bouts belie a set of discrete and continuous internal states, latent variables that are not captured by standard point process models.  We incorporate these variables as latent marks of a point process and explore various models for their dynamics.  To infer the latent variables and fit the parameters of this model, we develop an amortized variational inference algorithm that targets the collapsed posterior distribution, analytically marginalizing out the discrete latent variables.  With a dataset of over 120,000 swim bouts, we show that our models reveal interpretable discrete classes of swim bouts and continuous internal states like hunger that modulate their dynamics.  These models are a first step toward understanding the behavioral program of larval zebrafish and, ultimately, its neural underpinnings.","Anuj Sharma,Columbia University;Scott Linderman*,Columbia University;Robert Johnson,Harvard University;Florian Engert,Harvard University",as4529@columbia.edu;scott.linderman@columbia.edu;robertevanjohnson@fas.harvard.edu;florian@mcb.harvard.edu,Probabilistic Methods/Latent Variable Models*; Applications/Computational Biology and Bioinformatics; Neuroscience and Cognitive Science/Neur,Spotlight
7994,Computationally and Statistically Efficient Learning of Bayes Nets Using Path Queries,"Causal discovery from empirical data is a fundamental problem in many scientific domains. Observational data allows for identifiability only up to Markov equivalence class. In this paper we first propose a polynomial time algorithm for learning the exact correctly-oriented structure of the transitive reduction of any causal Bayesian networks with high probability, by using interventional path queries. Each path query takes as input an origin node and a target node, and answers whether there is a directed path from the origin to the target. This is done by intervening the origin node and observing samples from the target node. 
We theoretically  show the logarithmic sample complexity for the size of interventional data per path query, for continuous and discrete networks. 
We then show how to learn the transitive edges using also logarithmic sample complexity (albeit in time exponential in the maximum number of parents for discrete networks), which allows us to learn the full network. We further extend our work by reducing the number of interventional path queries for learning rooted trees. We also provide an analysis of imperfect interventions.","Kevin Bello*,Purdue University;Jean Honorio,Purdue",kbellome@purdue.edu;jhonorio@purdue.edu,Theory/Learning Theory*; Probabilistic Methods/Graphical Models,Poster
7997,Sparse PCA from Sparse Linear Regression,"Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR) are two problems that have a wide range of applications and have attracted a tremendous amount of attention in the last two decades as canonical examples of statistical problems in high dimension. A variety of algorithms have been proposed for both SPCA and SLR, but their literature has been disjoint for the most part. We have a fairly good understanding of conditions and regimes under which these algorithms succeed. But is there be a deeper connection between computational structure of SPCA and SLR? In this paper we show how to efficiently transform a blackbox solver for SLR into an algorithm for SPCA. Assuming the SLR solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, we show that the SPCA algorithm derived from it achieves state of the art performance, matching guarantees for testing and for support recovery under the single spiked covariance model as obtained by the current best polynomial-time algorithms. Our reduction not only highlights the inherent similarity between the two problems, but also, from a practical standpoint, it allows one to obtain a collection of algorithms for SPCA directly from known algorithms for SLR. Experiments on simulated data show that these algorithms perform well.","Guy Bresler,MIT;Sung Min Park,MIT;Madalina Persu*,Two Sigma Investments, MIT",guy@mit.edu;sp765@mit.edu;mpersu@mit.edu,"Algorithms/Components Analysis (e.g., CCA, ICA, LDA, PCA)",Poster
8002,Sequential Data Classification for Resource-constrained Devices,"We study the problem of fast and efficient classification of
sequential data (such as time-series) on tiny devices, which is
critical for various IoT related applications like audio keyword
detection or gesture detection. Deploying sequential data
classification modules on tiny devices is challenging as predictions
over sliding windows of data need to be invoked continuously at a high
frequency. Each of these predictors themselves are expensive as they
evaluate large models over long windows of data. In this paper, we
address this challenge by exploiting the following two observations
about classification tasks arising in typical IoT related
applications: (a) the ""signature"" of a particular class (e.g. an audio
keyword) typically occupies a small fraction of the overall data, and
(b) class signatures tend to discernible early-on in the data. 
We propose a method that exploits these observations by using a
multiple instance learning formulation along with an early prediction
technique to learn a model that can achieve better accuracy compared
to baseline models, while reducing the computation by a large
fraction. For instance, on an audio keyword detection benchmark
 our model improves standard LSTM model's accuracy
by up to 1.5\% while decreasing the computation cost by more than
60\%. This enables us to deploy such models for continuous real-time
prediction on a small device such as Raspberry Pi0, a task that the
baseline LSTM could not achieve. Finally, we also provide an analysis
of our multiple instance learning algorithm in a simple setting and
show that the proposed algorithm can efficiently converge to the
global optima, one of the first such result in this domain.","Don Dennis,Microsoft Research;Chirag Pabbaraju,Microsoft Research;Harsha Vardhan Simhadri,Microsoft Research India;Prateek Jain*,Microsoft Research",t-dodenn@microsoft.com;chiragramdas@gmail.com;harshasi@microsoft.com;prajain@microsoft.com,Deep Learning/Efficient Inference Methods*; Deep Learning/Recurrent Networks; Optimization/Non-Convex Optimization,Poster
8009,Transfer of Deep Reactive Policies for MDP Planning,"Domain-independent probabilistic planners input an MDP description in a factored representation language such as PPDDL or RDDL, and exploit the specifics of the representation for faster planning. Traditional algorithms operate on each problem instance independently, and good methods for transferring experience from policies of other instances of a domain to a new instance do not exist.  Recently, researchers have begun exploring the use of deep reactive policies, trained via deep reinforcement learning (RL), for MDP planning domains. One advantage of deep reactive policies is that they are more amenable to transfer learning.  

In this paper, we present the first domain-independent transfer algorithm for MDP planning domains expressed in an RDDL representation. Our architecture exploits the symbolic state configuration and transition function of the domain (available via RDDL) to learn a shared embedding space for states and state-action pairs for all problem instances of a domain. We then learn an RL agent in the embedding space, making a near zero-shot transfer possible, i.e., without much training on the new instance, and without using the domain simulator at all. Experiments on three different benchmark domains underscore the value of our transfer algorithm. Compared against planning from scratch, and a state-of-the-art RL transfer algorithm, our transfer solution has significantly superior learning curves.","Aniket Bajpai,Indian Institute of Technology;Sankalp Garg,Indian Institute of Technology Delhi;Mausam *,IIT Dehli",quantum.computing96@gmail.com;sankalp2621998@gmail.com;mausam@cse.iitd.ac.in,Reinforcement Learning and Planning,Poster
8034,The Price of Fair PCA: One Extra dimension," In this paper, we investigate the possibility that standard dimensionality reduction techniques might inadvertently produce data representations which do not maintain similar fidelity for two different populations. We show on several real-world datasets, PCA has higher reconstruction error on population $A$ than $B$ (for example, women versus men or lower versus higher-educated individuals). This can happen even when the dataset has similar number of samples from $A$ and $B$. This motivates our study of dimensionality reduction techniques with similar fidelity for $A$ as $B$. We give a polynomial-time algorithm for finding a projection which is nearly-optimal with respect to this measure, and evaluate it on several datasets.","Samira Samadi,Georgia Tech;Uthaipon Tantipongpipat,Georgia Tech;Jamie Morgenstern*,Georgia Tech;Mohit Singh,Georgia Tech;Santosh Vempala,Georgia Tech",s.samadi@gmail.com;tao@gatech.edu;jamiemor@cis.upenn.edu;mohit.singh@isye.gatech.edu;vempala@cc.gatech.edu,"Applications/Fairness, Accountability, and Transparency",Poster
8035,GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking,"Model compression is essential for serving large deep neural nets on devices with limited resources or applications that require real-time responses. For advanced NLP problems, a neural language model usually consists of recurrent layers (e.g., using LSTM cells), an embedding matrix for representing input tokens, and a softmax layer for generating output tokens. For problems with a very large vocabulary size, the embedding and the softmax matrices can account for more than half of the model size. For instance, the bigLSTM model achieves state-of-the-art performance on the One-Billion-Word (OBW) dataset with around 800k vocabulary, and its word embedding and softmax matrices use more than 6GBytes space, and are responsible for over 90\% of the model parameters. In this paper, we propose GroupReduce, a novel compression method for neural language models, based on vocabulary-partition (block) based low-rank matrix approximation and the inherent frequency distribution of tokens (the power-law distribution of words). We start by grouping words into $c$ blocks based on their frequency, and then refine the clustering iteratively by constructing weighted low-rank approximation for each block, where the weights are based the frequencies of the words in the block. The experimental results show our method can significantly outperform traditional compression methods such as low-rank approximation and pruning. On the OBW dataset, our method achieved 6.6x compression rate for the embedding and softmax matrices, and when combined with quantization, our method can achieve 26x compression rate without losing prediction accuracy.","Patrick Chen*,UC Davis;Si Si,Google Research;Yang Li,Google Research;Ciprian Chelba,google research;Cho-Jui Hsieh,UCLA, Google Research",phpchen@ucdavis.edu;sisidaisy@google.com;liyang@google.com;ciprianchelba@google.com;chojui.hsieh@gmail.com,Applications/Natural Language Processing,Poster
